
[[first_exploration]]
== First Exploration

////Write an introductory paragraph that specifically plants a first seed about the conceptual way of viewing big data.  Then, write a paragraph that puts this chapter in context for the reader, introduces it ("...in this chapter we'll show you how to start with a question and arrive at an answer without coding a big, hairy, monolithic program...")  Orient your reader to big data and the goals for lassoing it.  Doing this will hook your reader and prep their mind for the chapter's main thrust, its juicy bits.  Finally, say a word or two about big data before getting into Hadoop, for context (like "...big data is to Hadoop what x is to y...") Do these things before you jump so quickly into Hadoop. Amy////

Hadoop is a remarkably powerful tool for processing data, giving us at long last mastery over massive-scale distributed computing. More than likely, that's how you came to be reading this paragraph.

What you might not yet know is that Hadoop's power comes from _embracing_, not conquering, the constraints of distributed computing. This exposes a core simplicity that makes programming it exceptionally fun.

Hadoop's bargain is this. You must give up fine-grained control over how data is read and sent over the network. Instead, you write a series of short, constrained transformations, a sort of programming Haiku:

    Data flutters by
    Elephants make sturdy piles
    Insight shuffles forth

For any such program, Hadoop's diligent elephants intelligently schedule the tasks across single or dozens or thousands of machines. They attend to logging, retry and error handling; distribute your data to the workers that process it; handle memory allocation, partitioning and network routing; and attend to myriad other details that otherwise stand between you and insight. Putting these constraints on how you ask your _question_ releases constraints that traditional database technology placed on your _data_. Unlocking access to data that is huge, unruly, organic, highly-dimensional and deeply connected unlocks answers to a new deeper set of questions about the large-scale behavior of humanity and our universe. <remark>too much?? pk4</remark>

=== Data and Locality

//// I'm concerned that even the keenest of readers will find it a challenge to parse the "regional flavor" idea from the concept of "locality." (Maybe I'm confirming your own concern about this?)  I do realize you may choose another term for "locality" at some point, yet I think locality, while not quick to digest, is actually best.)  For this section, do you possibly have another example handy, one that isn't geographical, to use here?  If not, I suggest making a clearer distinction between region and place versus locality.  Making a clearer distinction will enable your reader to more quickly grasp and retain the important "locality" concept, apart from your regional flavor example. Amy//// 

There's no better example of data that is huge, unruly, organic, highly-dimensional and deeply connected than Wikipedia. Six million articles having XXX million associated properties and connected by XXX million links are viewed by XXX million people each year (TODO: add numbers). The full data -- articles, properties, links and aggregated pageview statistics -- is free for anyone to access it. (See the <<overview_of_datasets>> for how.)

The Wikipedia community have attach the latitude and longitude to more than a million articles: not just populated places like Austin, TX, but landmarks like Texas Memorial Stadium (where the Texas Longhorns football team plays), Snow's BBQ (proclaimed "The Best Texas BBQ in the World") and the TACC (Texas Advanced Computer Center, the largest academic supercomputer to date).

Since the birth of Artificial Intelligence we've wished we could quantify organic concepts like the "regional flavor" of a place -- wished we could help a computer understand that Austinites are passionate about Barbeque, Football and Technology -- and now we can, by say combining and analyzing the text of every article each city's page either links to or is geographically near.

"That's fine for the robots," says the skeptic, "but I can just phone my cousin Bubba and ask him what people in Austin like. And though I have no friend in Timbuktu, I could learn what's unique about it from the Timbuktu article and all those it links to, using my mouse or my favorite relational database." True, true. This question has what we'll call "easy locality"footnote:[Please discard any geographic context of the word "local": for the rest of the book it will always mean "held in the same computer location"]: the pieces of context we need (the linked-to articles) are a simple mouse click or database JOIN away. But if we turn the question sideways that stops being true. ////You can help the reader grasp the concept more reaily; I recommend revising to: "This question has what we call "easy locality," essentially, "held in the same computer location" (nothing to do with geography). Amy//// 

Instead of the places, let's look at the words. Barbeque is popular all through Texas and the Southeastern US, and as you'll soon be able to prove, the term "Barbeque" is overrepresented in articles from that region. You and cousin Bubba would be able to brainstorm a few more terms with strong place affinity, like "beach" (the coasts) or "wine" (France, Napa Valley), and you would guess that terms like "hat" or "couch" will not. But there's certainly no simple way you could do so comprehensively or quantifiably. That's because this question has no easy locality: we'll have to dismantle and reassemble in stages the entire dataset to answer it. This understanding of 'locality' is the most important concept in the book, so let's dive in and start to grok it. We'll just look at the step-by-step transformations of the data for now, and leave the actual code for <<geographic_words,a later chapter>>.

=== Where is Barbecue?

So here's our first exploration:

    For every word in the English language,
    which of them have a strong geographic flavor,
    and what are the places they attach to?

This may not be a practical question (though I hope you agree it's a fascinating one), but it is a template for a wealth of practical questions. It's a _geospatial analysis_ showing how patterns of term usage, such as ////list a couple quick examples of usage////, vary over space; the same approach can instead uncover signs of an epidemic from disease reports, or common browsing behavior among visitors to a website. It's a _linguistic analysis_ attaching estimated location to each term; the same approach term can instead quantify document authorship for legal discovery, letting you prove the CEO did authorize his nogoodnik stepson to destroy that orphanage. It's a _statistical analysis_ requiring us to summarize and remove noise from a massive pile of term counts; we'll use those methods ////unclear on which methods you're referring to?  Amy////in almost every exploration we do. It isn't itself a _time-series analysis_, but you'd use this data to form a baseline to detect trending topics on a social network or the anomalous presence of drug-trade related terms on a communication channel.

//// Consider defining the italicized terms, above, such as geospatial analysis, linguistic analysis, etc., inline (for example, "It's a linguistic analysis, the study of language, attaching estimated location to each term...") Amy////

//// Provide brief examples of how these methods might be useful, examples to support the above; offer questions that could be posed for each.  For example, for every symptom how it correlates to the epidemic and what zip codes the symptoms are attached to. Amy////

[[baldridge_bbq_wine]]
.Not the actual output, but gives you the picture; TODO insert actual results
image::images/baldridge-bbq_wine_beach_mountain-480.jpg[Location affinity for Beach, Mountain, BBQ and Wine]

=== Summarize every page on Wikipedia

First, we will summarize each article by preparing its "word bag" -- a simple count of the words on its wikipedia page. From the raw <<wp_lexington_article,article>> text:

[[wp_lexington_article]]
._Wikipedia article on "Lexington, Texas"_
______
Lexington is a town in Lee County, Texas, United States. ... Snow's BBQ, which Texas Monthly called "the best barbecue in Texas" and The New Yorker named "the best Texas BBQ in the world" is located in Lexington.
______

we get the <<wp_lexington_wordbag,following wordbag>>:

[[wp_lexington_wordbag]]
._Wordbag for "Lexington, Texas"_
------
Lexington,_Texas {("texas",4)("lexington",2),("best",2),("bbq",2),("barbecue",1), ...}
------

You can do this to each article separately, in any order, and with no reference to any other article. That's important! Among other things, it lets us parallelize the process across as many machines as we care to afford. We'll call this type of step a "transform": it's independent, non-order-dependent, and isolated.

==== Bin by Location

The article geolocations are kept in a different data file:

[[wp_coords]]
._Article coordinates_
------
Lexington,_Texas -97.01 30.41 023130130
------

We don't actually need the precise latitude and longitude, because rather than treating article as a point, we want to aggregate by area. Instead, we'll lay a set of grid lines down covering the entire world and assign each article to the grid cell it sits on. That funny-looking number in the fourth column is a 'quadkey' footnote:[you will learn all about quadkeys in the <<quadkey,"Geographic Data">> chapter], a very cleverly-chosen label for the grid cell containing this article's location. 

To annotate each wordbag with its grid cell location, we can do a 'join' of the two files on the wikipedia ID (the first column). Picture for a moment a tennis meetup, where you'd like to randomly assign the attendees to mixed-doubles (one man and one woman) teams. You can do this by giving each person a team number when they arrive (one pool of numbers for the men, an identical but separate pool of numbers for the women). Have everyone stand in numerical order on the court -- men on one side, women on the other -- and walk forward to meet in the middle; people with the same team number will naturally arrive at the same place and form a single team. That is effectively how Hadoop joins the two files: it puts them both in order by page ID, making records with the same page ID arrive at the same locality, and then outputs the combined record:

[[wp_lexington_wordbag_and_coords]]
._Wordbag with coordinates_
------
Lexington,_Texas -97.01 30.41 023130130 {("texas",4)("lexington",2),("best",2),("bbq",2),("barbecue",1), ...}
------

[[quadkey_central_texas]]
.Grid Tiles for Central Texas
image::images/Quadtree-google_maps_screenshot.png[Grid tiles for Central Texas]

==== Gridcell statistics

We have wordbag records labeled by quadkey for each article, but we want combined wordbags for each grid cell. So we'll <<wp_grouped_wordbags,group the wordbags by quadkey>>:

[[wp_grouped_wordbags]]
------
023130130 {(Lexington,_Texas,(("many", X),...,("texas",X),...,("town",X)...("longhorns",X),...("bbq",X),...)),(Texas_Memorial_Stadium,((...)),...),...}
------

them turn the individual word bags into a <<wp_combined_wordbags,combined word bag>>:

[[wp_combined_wordbags]]
------
023130130 {(("many", X),...,("texas",X),...,("town",X)...("longhorns",X),...("bbq",X),...}
------

==== A pause, to think

Let's look at the fundamental pattern that we're using. Our steps:

. transform each article individually into its wordbag
// . convert each article's precise point into the coarse-grained tile it sits on
. augment the wordbags with their geo coordinates by joining on page ID
. organize the wordbags into groups having the same grid cell;
. form a single combined wordbag for each grid cell.

//// Consider adding some text here that guides the reader with regard to the findings they might expect to result.  For example, "...if you were to use the example of finding symptoms that intersect with illness as part of an epidemic, you would have done x, y, and z..."  This will bring the activity to life and help readers appreciate how it applies to thier own data at hand.  Amy////

It's a sequence of _transforms_ (operations on each record in isolation: steps 1 and 4) and _pivots_ -- operations that combine records, whether from different tables (the join in step 2) or the same dataset (the group in step 3).

In doing so, we've turned articles that have a geolocation into coarse-grained regions that have implied frequencies for words. The particular frequencies arise from this combination of forces:

* _signal_: Terms that describe aspects of the human condition specific to each region, like "longhorns" or "barbecue", and direct references to place names, such as "Austin" or "Texas"
* _background_: The natural frequency of each term -- "second" is used more often than "syzygy" -- slanted by its frequency in geo-locatable texts (the word "town" occurs far more frequently than its natural rate, simply because towns are geolocatable).
* _noise_: Deviations introduced by the fact that we have a limited sample of text to draw inferences from.

Our next task -- the sprint home -- is to use a few more transforms and pivots to separate the signal from the background and, as far as possible, from the noise.

==== Pulling signal from noise

To isolate the signal, we'll pull out a trick called <<pmi,"Pointwise Mutual Information" (PMI)>>. Though it may sound like an insurance holding company, in fact PMI is a simple approach to isolate the noise and background. It compares the following:

* the rate the term 'barbecue' is used
* the rate that terms are used on grid cell 023130130
* the rate the term 'barbecue' is used on grid cell 023130130

Just as above, we can transform and pivot to get those figures:

* group the data by term; count occurrences
* group the data by tile; count occurrences
* group the data by term and tile; count occurrences
* count total occurrences
* combine those counts into rates, and form the PMI scores.

Rather than step through each operation, I'll wave my hands and pull its output from the oven:

------
023130130 {(("texas",X),...,("longhorns",X),...("bbq",X),...,...}
------

As expected, in <<baldridge_bbq_wine>> you see BBQ loom large over Texas and the Southern US; Wine, over the Napa Valleyfootnote:[This is a simplified version of work by Jason Baldrige, Ben Wing (TODO: rest of authors), who go farther and show how to geolocate texts _based purely on their content_. An article mentioning barbecue and Willie Nelson would be placed near Austin, TX; one mentioning startups and trolleys in San Francisco. See: Baldridge et al (TODO: reference)].

==== Takeaway #1: Start with a Question

We accomplished an elaborate data exploration, yet at no point did we do anything complex. Instead of writing a big hairy monolithic program, we wrote a series of simple scripts that either _transformed_ or _pivoted_ the data.

As you'll see later, the scripts are readable and short (none exceed a few dozen lines of code). They run easily against sample data on your desktop, with no Hadoop cluster in sight; and they will then run, unchanged, against the whole of Wikipedia on dozens or hundreds of machines in a Hadoop cluster.
////This sounds hard to believe.  Consider saying more here, as it comes off as a bit over-simplified.  Amy////

That's the approach we'll follow through this book: develop simple, maintainable transform/pivot scripts by iterating quickly and always keeping the data visible; then confidently transition those scripts to production as the search for a question becomes the rote production of an answer.

The challenge, then, isn't to learn to "program" Hadoop -- it's to learn how to think at scale, to choose a workable series of chess moves connecting the data you have to the insight you need. In the first part of the book, after briefly becoming familiar with the basic framework, we'll proceed through a series of examples to help you identify the key locality and thus the transformation each step calls for. In the second part of that book, we'll apply this to a range of interesting problems and so build up a set of reusable tools for asking deep questions in actual practice.

// ==== Takeaway #2: Locality
// 
// 
// Insight comes from data in context: places in the context of associated topics, or topics in the // context of associated locations. When your data is far too large to fit on a single machine,
// 
// In the second case, putting every word in context of all associated locations requires that 


=== Geographical Flavor

1. article -> wordbag  
2. join on page data to get geolocation
3. use pagelinks to get larger pool of implied geolocations
4. turn geolocations into quadtile keys
5. aggregate topics by quadtile
6. take summary statistics aggregated over term and quadkey
7. combine those statistics to identify terms that occur more frequently than the base rate would predict
8. explore and validate the results                              
9. filter to find strongly-flavored words, and other reductions of the data for visualization


==== Exemplars and Touchstones

There are three touchstones to hit in every data exploration:

* Confirm the things you know:
* Confirm or refute the things you suspect.
* Uncover at least one thing you never suspected.

Things we know: First, common words should show no geographic flavor. 
Geographic features -- "beach", "mountain", etc -- should be intensely localised.
* compared to other color words, there will be a larger regional variation for the terms "white" and "black" (as they describe ra
You don't have to stop exploring when you find a new mystery, but no data exploration is complete until you uncover at least one.

We will jointly discover two things
taking as a whole the terms that have a strong geographic flavor, we should largely see cultural terms (foods, sports, etc)
Next, we'll choose some _exemplars_: familiar records to trace through "Barbeque" should cover ;

* https://github.com/Ganglion/varaha/blob/master/src/main/java/varaha/text/TokenizeText.java




------
stream do |article|
  words = Wukong::TextUtils.tokenize(article.text, remove_stopwords: true)
  words.group_by(&:to_s).map{|word, occurs|
    yield [article.id, word, occurs.count]
  end
end
------

Reading it as prose the script says "for each article: break it into a list of words; group all occurrences of each word and count them; then output the article id, word and count."



.Snippet from the Wikipedia article on "Barbecue"
[quote, wikipedia, http://en.wikipedia.org/wiki/Barbeque]
____
Each Southern locale has its own particular variety of barbecue, particularly concerning the sauce. North Carolina sauces vary by region; eastern North Carolina uses a vinegar-based sauce, the center of the state enjoys Lexington-style barbecue which uses a combination of ketchup and vinegar as their base, and western North Carolina uses a heavier ketchup base. Lexington boasts of being "The Barbecue Capital of the World" and it has more than one BBQ restaurant per 1,000 residents. In much of the world outside of the American South, barbecue has a close association with Texas. Many barbecue restaurants outside the United States claim to serve "Texas barbecue", regardless of the style they actually serve. Texas barbecue is often assumed to be primarily beef. This assumption, along with the inclusive term "Texas barbecue", is an oversimplification. Texas has four main styles, all with different flavors, different cooking methods, different ingredients, and different cultural origins. In the June 2008 issue of Texas Monthly Magazine Snow's BBQ in Lexington was rated as the best BBQ in the state of Texas. This ranking was reinforced when New Yorker Magazine also claimed that Snow's BBQ was "The Best Texas BBQ in the World".


=== Pointwise Mutual Information

[[pmi]]

Pointwise Mutual Information sounds like an Insurance holding company, but is in fact a simple way // to expose signal from background.

Let's pick up the example from <<first_exploration>>

* rate the word 'barbecue' is used
* rate that words are used on grid cell 023130130
* rate the word 'barbecue' is used on grid cell 023130130

	pmi(x; y) := log[ p(x, y) / (p(x)*p(y))

	<math>
	\operatorname{pmi}(x;y) \equiv \log\frac{p(x,y)}{p(x)p(y)} = \log\frac{p(x|y)}{p(x)} = // \log\frac{p(y|x)}{p(y)}.
	</math>

==== Smoothing the counts ====

The count of each word is an imperfect estimate of the probability of seeing that word in the context of the given topic. Consider for instance the words that would have shown up if the article were 50% longer, or the cases where an author chose one synonym out of many equivalents. This is particularly significant considering words with zero count.

We want to treat "missing" terms as having occurred some number of times, and adjust the probabilities of all the observed terms.

.Minimally Invasive
[NOTE]
===============================
It's essential to use "minimally invasive" methods to address confounding factors.

What we're trying to do is expose a pattern that we believe is robust: that it will shine through any occlusions in the data. Occasionally, as here, we need to directly remove some confounding factor. The naive practitioner thinks, "I will use a powerful algorithm! That's good, because powerful is better than not powerful!" No -- simple and clear is better than powerful.

Suppose you were instead telling a story set in space - somehow or another, you must address the complication of faster-than-light travel. Star Wars does this early and well: its choices ("Ships can jump to faraway points in space, but not from too close to a planet and only after calculations taking several seconds; it happens instantaneously, causing nearby stars to appear as nifty blue tracks") are made clear in a few deft lines of dialog.

A ham-handed sci-fi author instead brings in complicated machinery requiring a complicated explanation resulting in complicated dialogue. There are two obvious problems: first, the added detail makes the story less clear. It's literally not rocket science: concentrate on heros and the triumph over darkness, not on rocket engines. Second, writing that dialog is wasted work. If it's enough to just have the Wookiee hit the computer with a large wrench, do that.

But it's essential to appreciate that this also _introduces extra confounding factors_. Rather than a nifty special effect and a few lines shouted by a space cowboy at his hairy sidekick, your junkheap space freighter now needs an astrophysicist, a whiteboard and a reason to have the one use the other. The story isn't just muddier, it's flawed.

We're trying to tell a story ("words have regional flavor"), but the plot requires a few essential clarifications ("low-frequency terms are imperfectly estimated").  If these patterns are robust, complicated machinery is detrimental. It confuses the audience, and is more work for you; it can also bring more pattern to the data than is actually there, perverting your results.

The only time you should bring in something complicated or novel is when it's a _central_ element of your story. In that case, it's worth spending multiple scenes in which Jedi masters show and tell the mechanics and limitations of The Force.
===============================

There are two reasonable strategies: be lazy; or consult a sensible mathematician.

To be lazy, add a 'pseudocount' to each term: pretend you saw it an extra small number of times For the common pseudocount choice of 0.5, you would treat absent terms as having been seen 0.5 times, terms observed once as having been seen 1.5 times, and so forth.  Calclulate probabilities using the adjusted count divided by the sum of all adjusted counts (so that they sum to 1). It's not well-justified mathematically, but is easy to code.

Consult a mathematician: for something that is mathematically justifiable, yet still simple enough to be minimally invasive, she will recommend "Good-Turing" smoothing.

In this approach, we expand the dataset to include both the pool of counter for terms we saw, and an "absent" pool of fractional counts, to be shared by all the terms we _didn't_ see. Good-Turing says to count the terms that occurred once, and guess that an equal quantity of things _would_ have occurred once, but didn't. This is handwavy, but minimally invasive; we oughtn't say too much about the things we definitionally can't say much about. 

We then make the following adjustments:

* Set the total _count_ of words in the absent pool equal to the number of terms that occur once. There are of course tons of terms in this pool; we'll give each some small fractional share of an appearance.
* Specifically, treat each absent term as occupying the same share of the absent pool as it does in the whole corpus (minus this doc). So, if "banana" does not appear in the document, but occurs at (TODO: value) ppm across all docs, we'll treat it as occupying the same fraction of the absent pool (with slight correction for the absence of this doc).
* Finally, estimate the probability for each present term as its count divided by the total count in the present and absent pools.

// 	def ct_doc(doc)
//     	  ct_wds_for_doc(doc).sum{|wd, ct| ct }
// 	end
// 
// 	def fr_doc_wd(doc, wd)
// 	  ct_doc_wd(doc, wd)  / ct_doc(doc)
// 	end
// 
// 	# estimate the total frequency of all absent words
// 	# as the total frequency of words appearing exactly once
// 	p_allabsent_for_doc(doc)
// 	  ct_once = ct_wds_for_doc(doc).select{|wd, ct| ct == 1 }
// 	  ct_once / ct_doc(doc)
// 	end
// 
// 	# global frequency of term among terms _not_ in document
// 	def fr_wd_notdoc(wd, doc)
//   	  # contribution of this doc to the all-doc totals
// 	  sumfreq_doc = fr_wds_doc(doc).sum{|wd, _| fr_wd_all(wd) }
// 	  # global frequency with correction
// 	  fr_wd(wd) / (1 - sumfreq_doc)
// 	end
// 
// 	def p_wd_for_doc(doc, wd)
// 	  pabs = p_allabsent_for_doc(doc)
// 	  if absent
// 	    # frequency share of the absent pool, times the corrected global frequency of the term
// 	    result =    pabs  * fr_wd_notdoc(wd, doc)
// 	  else
// 	    # frequency share of the present pool, times the observed frequency of the term
// 	    result = (1-pabs) * fr_wd_doc(doc, wd)
// 	  end
// 	end
// 



