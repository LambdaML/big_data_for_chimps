[[text_data]]
== Text Data

=== Geographic Flavor ===

There's no better example of data that is huge, unruly, organic, highly-dimensional and deeply connected than Wikipedia. Six million articles having XXX million associated properties and connected by XXX million links are viewed by XXX million people each year (TODO: add numbers). The full data -- articles, properties, links and aggregated pageview statistics -- is free for anyone to access it. (See the <<overview_of_datasets>> for how.)

The Wikipedia community have attach the latitude and longitude to more than a million articles: not just populated places like Austin, TX, but landmarks like the University of Texas' Memorial Stadium, Snow's BBQ ("The Best Texas BBQ in the World") and the TACC (Texas Advanced Computer Center, home of the world's largest academic supercomputer). This lets us put not just each article, but the cloud of data around it, in geographical context.

What happens if we apply this context to not just the article, but those articles' words? Barbeque is popular all through Texas and the Southeastern US -- is the term "Barbeque" overrepresented in articles from that region? We can brainstorm a few more terms with strong place affinity, like "beach" (the coasts) or "wine" (France, Napa Valley), and ones without, like "hat" or "couch". Hadoop, combined with the Wikipedia dataset, will let us _rigorously_ identify words with this kind of geographic flavor, along with the locales they have affinity for.

At a high level, what we'll do is this:

* Divide the world into grid cells, and group all the words in wikipedia onto their article's grid cell
* Determine the overall frequency of each word in the wikipedia corpus
* Identify prominent (unusually frequent) words on each grid cell
* Identify words that are prominent on a large number of grid cells -- these have strong geographic "flavor"


=== Match Wikipedia Article Text with Article Geolocation

Let's start by assembling the data we need. The wikipedia dataset has three different tables for each article: the metadata for each page (page id, title, size, last update time, and so); the full text of each article (a very large field); and the article geolocations. Below are snippets from the articles table and of the geolocations table:

[[wp_lexington_article]]
._Wikipedia article record for "Lexington, Texas"_
------
Lexington,_Texas  Lexington is a town in Lee County, Texas, United States. ... Snow's BBQ, which Texas Monthly called "the best barbecue in Texas" and The New Yorker named "the best Texas BBQ in the world" is located in Lexington.
------


[[wp_coords]]
._Article coordinates_
------
Lexington,_Texas -97.01 30.41 023130130
------

Since we want to place the words in each article in geographic context, our first step is to reunite each article with its geolocation.

----
article_text = LOAD ('...');
article_geolocations = LOAD('...');
articles = JOIN article_geolocations BY page_id, article_text BY page_id;
articles = FOREACH articles GENERATE article_text::page_id, QUADKEY(lng, lat) as quadcell, text;
----

The quadkey field you see is a label for the grid cell containing an article's location; you'll learn all about them in the <<quadkey,"Geographic Data">> chapter, but for the moment just trust us that it's a clever way to divide up the world map for big data computation. Here's the result:

[[wp_lexington_wordbag_and_coords]]
._Wordbag with coordinates_
------
Lexington,_Texas 023130130 Lexington is a town in Lee County, Texas ...
------

=== Decompose Wikipedia Articles

Next, we will summarize each article by preparing its "word bag" -- a simple count of the terms on its wikipedia page. We've written a Pig UDF (User-Defined Function) to do so:

----
REGISTER path/to/udf ...;
wordbags = FOREACH articles {
    wds = WORDBAG(text)
      AS tuple(tot_usages:long, num_terms:long, num_onces:long,
         wordbag:bag{tuple(article_term_usages,term)});
    GENERATE page_id, quadcell, wds.tot_usages, wds.num_terms, wds.num_onces, wds.wordbag;
};
term_article_freqs = FOREACH wordbags GENERATE
    page_id, quadcell, tot_usages, num_terms, FLATTEN(wordbag);
STORE term_article_freqs INTO '/data/work/geo_flavor/term_article_freqs;
----

Here's the <<wp_lexington_wordbag,output>>:

[[wp_lexington_wordbag]]
._Wordbag for "Lexington, Texas"_
------
Lexington,_Texas 023130130 TODO:tot_usages TODO:terms TODO:NUMONCES {(4,"texas")(2,"lexington"),(2,"best"),(2,"bbq"),(1,"barbecue"), ...}
------

And the output after the flatten:

[[wp_lexington_wordbag]]
._"term_cell_freqs" result_
------
Lexington,_Texas 023130130 tot_usages num_terms 4   "texas"
Lexington,_Texas 023130130 tot_usages num_terms 2   "lexington"
Lexington,_Texas 023130130 tot_usages num_terms 2   "best"
Lexington,_Texas 023130130 tot_usages num_terms 2   "bbq"
Lexington,_Texas 023130130 tot_usages num_terms 1   "barbecue"
------


=== Term Statistics by Grid Cell

----
taf_g = GROUP term_article_freqs BY quadcell, term;
cell_freqs = FOREACH taf_g GENERATE
      group.quadcell AS quadcell,
      group.term AS term,
      SUM(term_article_freqs.article_term_usages) AS cell_term_usages;
cf_g = GROUP cell_freqs BY quadcell;
term_cell_freqs = FOREACH cf_g GENERATE
    group AS quadcell,
    SUM(cell_term_usages) AS cell_usages
    FLATTEN(cell_term_usages, term);
----

._"cell_freqs" result_
----
023130130 7 "bbq"
023130130 20 "texas"
----

._"cf_g" result_
----
023130130 {(7,"bbq"),(20,"texas"),...}
----

._"term_cell_freqs" result_
----
023130130 95 7 "bbq"
023130130 95 20 "texas"
----

=== Term Statistics

We will be defining the prominence of a term on a grid cell by comparing its local frequency to the overall frequency of the term. The occurrence frequency of the term "the" is XX parts per million (ppm), while that of "barbeque"'s is XX ppm. However, on the quadcell surrounding Lexington, Texas, "the" occurs at XX ppm and "barbeque" at XX ppm -- a significantly elevated rate.

Let's now prepare those global statistics.

----
all_terms = GROUP term_article_freqs BY term;
term_info_1 = FOREACH all_terms GENERATE
    group AS term,
    COUNT_STAR(term_article_freqs) AS num_articles,
    SUM(article_term_usages) AS term_usages;
global_term_info_g = GROUP term_info BY ALL;
global_term_info = FOREACH global_term_info_g GENERATE
    COUNT_STAR(term_info) AS num_terms,
    SUM(term_usages) AS global_usages;
STORE global_term_info INTO '/data/work/geo_flavor/global_term_info';
----

(The actual code is somewhat different from what you see here -- we'll explain below)

... (TODO describe term_info)


=== Pattern: Re-injecting global totals

We also extract two global statistics: the number of distinct terms, and the number of distinct usages. This brings up one of the more annoying things about Hadoop programming. The global_term_info result is two lousy values, needed to turn the global _counts_ for each term into the global _frequency_ for each term. But a pig script just orchestrates the top-level motion of data: there's no intrinsic way to bring the result of a step into the declaration of following steps. The proper recourse is to split the script into two parts, and run it within a workflow tool like Rake, Drake or Oozie. The workflow layer can fish those values out of the HDFS and inject them as runtime parameters into the next stage of the script.

We prefer to cheat. We instead ran a version of the script that found the global count of terms and usages, then copy/pasted their values as static parameters at the top of the script. This also lets us calculate the ppm frequency of each term and the other term statistics in a single pass. To ensure our time-traveling shenanigans remain valid, we add an `ASSERT` statement which compares the memoized values to the actual totals.

----
DEFINE memoized_num_terms XXX;
DEFINE memoized_global_usages XXX;
all_terms = GROUP term_cell_freqs BY term;
term_info_1 = FOREACH all_terms GENERATE
    group AS term,
    COUNT_STAR(term_cell_freqs) AS num_articles,
    SUM(article_term_usages) AS term_usages,
    1000000 * SUM(article_term_usages)/memoized_global_usages AS term_ppm:double
    ;
-- Validate the global term statistics
global_term_info_g = GROUP term_info BY ALL;
global_term_info = FOREACH global_term_info_g GENERATE
   COUNT_STAR(term_info) AS num_terms,
   SUM(term_usages) AS global_usages;
STORE global_term_info INTO '/data/work/geo_flavor/global_term_info';
ASSERT(global_term_info.num_terms = memoized_num_terms);
ASSERT(global_term_info.global_usages = memoized_global_usages);
----

(TODO: just realized the way we've done this finds global term stats on only geolocated articles. To find them on all articles will complicate the script: we have to do a left join and then filter, or we'd have to do wordbags first then join on geolocations.)


