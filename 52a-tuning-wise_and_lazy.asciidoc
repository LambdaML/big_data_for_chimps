=== Hadoop Tuning for the wise and lazy ===

There are enough knobs and twiddles on a hadoop installation to fully stock the cockpit of a 747. Many of them interact surprisingly, and many settings improve some types of jobs while impeding others. This chapter will help you determin

* Baseline constraints of system components: CPU, disk, memory, network
* Baseline constraints of Atomic operations: stream, join, sort, filter
* Baseline constraints of each stage: setup, read, mapper, spill/combine, midflight, shuffle, reducer, write, replicate, commit.

Hadoop is designed to put its limiting resource at full utilization.

=== Baseline Performance ===

best-case scenario:

* all-local mapper tasks
* mapper throughput at baseline rate
* low mapper setup overhead
* one mapper spill per record
* low variance in mapper finish time
* shuffle is largely complete when last merge segments come in
* reducer throughput at baseline rate
* low replication overhead


=== Performance constraints: job stages ===

Raw ingredients:

* _scripts_:
  - faker	-- generates apache weblog records _deterministically_. Ensure the timestamp will require no complicated parsing to use in Pig.
  - swallow-m	-- mapper emits 0.01% (one out of every 10_000) records, no reduce
  - most-m	-- mapper emits 99.99% of records, no reduce
  - swallow-r	-- mapper emits all records, reducer emits  0.01% of records
  - most-r	-- mapper emits all records, reducer emits 99.99% of records

  - url-group	-- project only URL and timestamp fields; group on URL; emit URL and bag of timestamps that URL was visited
   
* _files_:
  - oneline	-- 512 files, each with only its index
  - fakered	-- faker.rb-generated 64-GB dataset as 1 64-GB file, 8 8-GB, 64 1-GB, 512 128-MB files. Re-running faker script will recreate fakered dataset.

* _setups_:
  - max-shuffle	-- workers with only reduce-slots, having max-sized shuffle buffers, no shuffle flush (i.e as close as we can get to zero shuffle)
  - baseline    -- large output block size, replication factor 1

* setup
  - zeros	-- mapper-only	-- swallow
  - oneline	-- mapred	-- identity
* read
  - fakered-128	-- mapper-only	-- emit nothing
* mapper
  - fakered-128	-- mapper-only	-- split fields, regexp, but don't emit
  - fakered-128	-- mapper-only	-- split fields, regexp, emit
  - oneline	-- mapper-only	-- faker
* spill/combine
  - fakered-128	-- mapred	-- identity 
  - oneline	-- mapred	-- faker
* midflight: 
  - xx		-- free-shuffle	-- swallow
* shuffle; with various sizes of data per reducer
  - fakered	-- lo-skew	-- swallow
  - fakered	-- hi-skew	-- swallow
* reducer
  - fakered	-- mapred	-- identity	-- identity	-- replication factor 1
  - oneline	-- mapred	-- identity	-- faker	-- replication factor 1
  - fakered	-- mapred	-- identity	-- split fields, regexp, but don't emit	-- replication factor 1
  - fakered	-- mapred	-- identity	-- split fields, regexp, emit	-- replication factor 1
* write
  - oneline	-- mapred	-- identity	-- faker
* replicate
  - oneline	-- mapred	-- identity	-- faker	-- replication factor 1
  - oneline	-- mapred	-- identity	-- faker	-- replication factor 2
  - oneline	-- mapred	-- identity	-- faker	-- replication factor 3
  - oneline	-- mapred	-- identity	-- faker	-- replication factor 5
* commit
  - oneline	-- mapred	-- identity	-- identity
  - oneline	-- mapred	-- identity	-- swallow

==== Variation ====

* non-local map tasks
* EBS volumes
* slowstart
* more reducers than slots
* S3 vs EBS vs HBase vs Elasticsearch vs ephemeral HDFS

=== Performance constraints: by operation ===

**mapper-only performance**

disk-cpu-disk only

* FOREACH only
* FILTER on a numeric column only
* MATCH only
* decompose region into tiles

**midflight**


==== Active vs Passive Benchmarks ====

When tuning, you should engage in _active benchmarking_. Passive benchmarking would be to start a large job run, time it on the wall clock (plus some other global measures) and call that a number. Active benchmarking means that while that job is running you watch the fine-grained metrics (following the <<use_method>>) -- validate that the limiting resource is what you believe it to be, and understand how the parameters you are varying drive tradeoffs among other resources.

* What are the maximum practical capabilities of my system, and are they reasonable?
* How do I determine a job's primary constraint, and whether it's worthwhile to optimize it?
* If I must to optimize a job, what setting adjustments are relevant, and what are the tradeoffs those adjustments?

Coarsely speaking, jobs are constrained by one of these four capabilities:

* RAM: Available memory per node,
* Disk IO: Disk throughput,
* Network IO: Network throughput, and
* CPU: Computational throughput.

Your job is to

* **Recognize when your job significantly underperforms** the practical expected throughput, and if so, whether you should worry about it. If your job's throughput on a small cluster is within a factor of two of a job that does nothing, it's not worth tuning. If that job runs nightly and costs $1000 per run, it is.
* **Identify the limiting capability**.
* **Ensure there's enough RAM**. If there isn't, you can adjust your the memory per machine, the number of machines, or your algorithm design.
* **Not get in Hadoop's way**. There are a few easily-remedied things to watch for that will significantly hamper throughput by causing unneccesary disk writes or network traffic.
* **When reasonable, adjust the RAM/IO/CPU tradeoffs**. For example, with plenty of RAM and not too much data, increasing the size of certain buffers can greatly reduce the number of disk writes: you've traded RAM for Disk IO.

=== Tune Your Cluster to your Job ===

If you are running Hadoop in an elastic environment, life gets easy: you can tune your cluster to the job, not the other way around.

* Choose the number of mappers and reducers
  - To make best use of your CPUs, you want the number of running tasks to be at least `cores-1`; as long as there's enough ram, go as high as mappers = `cores * 3/4` and reducers = `cores * 1/2`.  For a cluster purpose-built to run jobs with minimal reduce tasks, run as many mappers as cores.
  - The total heap allocated to the datanode, tasktracker, mappers and reducers should be less than but close to the size of RAM on the machine.
  - The mappers should get at least twice as much total ram as your typical mapper output size (which is to say, at least twice as much ram as your HDFS block size).
  - The more memory on your reducers the better. If at all possible, size your cluster to at least half as much RAM as your reduce input data size.

* Get the job working locally on a reduced dataset
  - for a wukong job, you don't even need hadoop; use `cat` and pipes.
* Profile its run time on a small cluster

For data that will be read much more often than it's written, 

* Produce output files of 1-4 GB with a block size of 128MB
  - if there's an obvious join key, do a total sort. This lets you do a merge join later.

=== Happy Mappers ===

==== A Happy Mapper is **well-fed**, **finishes with its friends**, **uses local data**, **doesn't have extra spills**, and has a **justifiable data rate**

==== A Happy Mapper is Well-fed

* Map tasks should take longer to run than to start. If mappers finish in less than a minute or two, and you have control over how the input data is allocated, try to feed each more data. In general, 128MB is sufficient; we set our HDFS block size to that value.

==== A Happy Mapper finishes with its friends ====

Assuming well-fed mappers, you would like every mapper to finish at roughly the same time. The reduce cannot start until all mappers have finished. Why would different mappers take different amounts of time?

* large variation in file size
* large variation in load -- for example, if the distribution of reducers is uneven, the machines with multiple reducers will run more slowly in general
* on a large cluster, long-running map tasks will expose which machines are slowest.

==== A Happy Mapper is Busy ====

Assuming mappers are well fed and prompt, you would like to have nearly every mapper running a job.


* Assuming every mapper is well fed and every mapper is running a job, 


Pig can use the combine splits setting to make this intelligently faster. Watch out for weirdness with newer versions of pig and older versions of HBase.

If you're reading from S3, dial up the min split size as large as 1-2 GB (but not 

==== A Mapper with no Reducer is Typically Quite Happy

Any time you can turn a job with a reduce phase into a job without one (for instance, by using a HashMap replication join), do so. Any reduce phase involves shipping the full midstream data size over the network.

==== Match the reducer heap size to the data it processes ====
  
===== A Happy Reducer is **well-balanced**, has **few merge passes**, has **good RAM/data ratio**, and a **justifiable data rate** =====

* **well-balanced**: 


All of the below use our data-science friendly configuration parameters.
It also only concerns jobs worth thinking about -- more than a few dozen gigabytes.

* **What's my map input size?**
  - the min split size, file size and block size set the size of the map input.
  - a 128MB block size is a nice compromise between wasted space and map efficiency, and is the typical map input size.
  - you'd like your map tasks to take at least one minute, but not be the dominant time of the job. If all your map slots are full it's OK if they take longer.

* It's usually straightforward to estimate the pessimistic-case output size. For cluster defaults, let's use a 25% overhead -- 160 MB output size.
* 15% (`io.sort.record.percent`) of the buffer is taken by record-keeping, so the 160MB should fit in 190 MB (at 15%), 170 MB (at 5%).

The maximum number of records collected before the collection thread will spill is r * x * q * 2^16

if your reduce task itself doesn't need ram (eg for wukong jobs), set this to more like 0.7.

You'd like the "File bytes read" / "File bytes written" to be nil, and the spilled records close to zero. You *don't* want to see spilled records >> reduce input records -- this means the reducers had to do multiple layers of merge sort.

an m1.large:
  - 3 map tasks 300 MB raw input, 340 MB raw output (150 MB compressed), in 2 min
    - 1 GB in, 1 GB out (450 MB compressed)
  - 2 reduce tasks 700 MB in, 1.7 GB out, 50% spill
    - 1.5GB in, 3.5 GB out, 4 mins.

an m2.2xlarge:
  - 5 map tasks, each 460 MB raw input, 566 MB raw output (260 MB compressed) 1.5 min
    - 2.3 GB in, 2.8 GB out (1.3 GB compressed) -> 2 GB / m2.2xl*min

  - overall 50 GB in, 53 GB out, 12.5 min * 6 m2.2xl = $1.12
  - for 1 TB, ~ 30 m2.2xl 50 min


=== Happy Reducers ===


<<reducer_size>>
==== Merge Sort Input Buffers ====


In pre-2.0 Hadoop (the version most commonly found at time of writing in 2012), there's a hard limit of 2 GB in the buffers used for merge sorting of mapper outputs footnote[it's even worse than that, actually; see `mapred.job.shuffle.input.buffer.percent` in the tuning-for-the-foolish chapter.]. You want to make good use of those buffers, but 

=== Hadoop Tuning for the foolish and brave ===

=== Measuring your system: theoretical limits ===

What we need here is a ready-reckoner for calculating the real costs of processing. We'll measure two primary metrics:

* throughput, in `GB/min`.
* machine cost in `$/TB` -- equal to `(number of nodes) * (cost per node hour) / (60 * throughput)`. This figure accounts for tradeoffs such as spinning up twice as many nodes versus using nodes with twice as much RAM. To be concrete, we'll use the 2012 Amazon AWS node pricing; later in this chapter we'll show how to make a comparable estimate for physical hardware.

If your cluster has a fixed capacity, throughput has a fixed proportion to cost and to engineer time. For an on-demand cluster, you should 

_note: I may go with min/TB, to have them be directly comparable. Throughput is typically rendered as quantity/time, so min/TB will seem weird to some. However, min/TB varies directly with $/TB, and is slightly easier to use for a rough calculation in your head._

* Measure disk throughput by using the `cp` (copy) command to copy a large file from one disk to another on the same machine, compressed and uncompressed.
* Measure network throughput by using `nc` (netcat) and  `scp` (ssh copy) to copy a large file across the network, compressed and uncompressed.
* Do some increasingly expensive computations to see where CPU begins to dominate IO. 
* Get a rough understanding of how much RAM you should reserve for the operating system's caches and buffers, and other overhead -- it's more than you think.

=== Measuring your system: imaginary limits ===

* http://code.google.com/p/bonnie-64/[Bonnie] for disk; http://www.textuality.com/bonnie/advice.html[advice], https://blogs.oracle.com/roch/entry/decoding_bonnie[more advice]
* http://www.coker.com.au/bonnie/[Bonnie++]  for disk 
* http://www.phoronix-test-suite.com/?k=downloads[Phoronix] for a broad-based test

Test these with a file size equal to your HDFS block size.

=== Measuring your system: practical limits ===

* Understand the practical maximum throughput baseline performance against the fundamental limits of the system


* If your runtime departs significantly from the practical maximum throughput

Tuning your cluster to your job makes life simple
* If you are hitting a hard constraint (typically, not enough RAM)

=== Physics of Tuning constants


There are some things that should grow square-root-ishly as the size of the cluster -- handler counts, some buffer sizes, and others. 

Let's think about the datanode handler count. Suppose you double the size of your cluster -- double the datanodes and double the tasktrackers. Now the cluster has twice as many customers for datanodes (2x the peer traffic from datanodes and 2x the tasktrackers requesting data), but the cluster also has twice as many datanodes to service those customers. So the average number of customers per datanode has not changed.  However, the number of workers that might gang up on one datanode simultaneously has increased; roughly speaking, this kind of variance increases as the square root, so it would be reasonable to increase that handler count by 1.4 (the square root of 2). Any time you have a setting that a) is sized to accommodate the peak number of inbound activity, and b) the count of producers and consumers grows in tandem, you're thinking about a square root.

That is, however, from intra-cluster traffic. By contrast, flume connections are long-lived, and so you should account for them as some portion of the datanode handler count -- each agent will be connected to one datanode at a time (as directed by the namenode for that particular block at th). Doubling the number of flume writers should double that portion; doubling the number of datanodes should halve that portion.

=== Pig settings ===

see `-Dpig.exec.nocombiner=true` if using combiners badly. (You'll want to use this for a rollup job).

=== Tuning pt 2 ===

* Lots of files:
  - Namenode and 2NN heap size
* Lots of data:
  - Datanode heap size.
* Lots of map tasks per job:
  - Jobtracker heap size
  - tasktracker.http.threads
  - mapred.reduce.parallel.copies

=== coupling constants ===

Tuning and coupling constants the example GC says look at what it constraints is and look at the natural time scale of the system for instance you can turn on data into time using throughput so to think about the palm case of the reducer there's trade-off between Emery just fine bio for network
