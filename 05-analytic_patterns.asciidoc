TODO-qem: review patterns, confirm discussion is cogent. Want to make sure each example makes sense on its own, that it fits, and also ensure narrative flow throughout the chapter.
"Lots of data into less data"

TODO: find biz applications for the fancy ones
TODO: find SQL and Hive equivalents for the core ops; the SQL should be valid for both MySQL and Postgres
TODO-qem: better to show a fuller example using an operation we haven't mentioned yet? (Eg listing team pairs: do the group-and-count when we talk about listing pairs)? Or postpone and call ahead to it?

== Pipeline Operations

All of the following operations require no reduce phase of their own. Standing alone, they lead to map-only jobs. When combined with the grouping / sorting operations you're about to meet, they become additional pipeline stages of the mapper or reducer (depending on whether they come before or after).

=== Somewhere

* Strings That Include Quotes or Special Characters

=== FOREACH to transform records individually

* modify the contents of records individually with FOREACH
  - ba, slg, from rate stats
* Binning records (See Statistics Chapter for more)
* coalesce
* ternary
* String Relative Ordering
  - Generate pairs of teams, use ternary to choose lexicographic firstmost


=== Blowing up data

* String/Collection decomposition Decomposing or Combining Strings
  - generating chars: str.split("(?!^)")
* Ungrouping operations (FOREACH..FLATTEN) expand records
  - call ahead to Section on Tidy data by FLATTENing an inline record
* See time series chapter: Discrete interval sampling (convert value-over-range to date-value)
* See text chapter: Wordbag, Flatten
* See statistics chapter: generating data
* See statistics chapter: Transpose data

=== eliminating records or fields

* Filter: 
  - players after 1900
  - Testing String Equality: players for Red Sox
  - Substring or Regular Expressions: players named Q.* OR .*lip.* OR Die.*
  - Select at-bats with three straight caught-looking strikes (the most ignominious of outcomes)
  - isNull, isEmpty, vs Boolean test
  - Caution on Floating Point comparisons
* Select Fixed Number of Arbitrary Records (LIMIT)
  - note: Limit doesn't stop reading so only supply a few filenames
  - no "OFFSET" like there is in SQL.
* Note: To select the top K items from a table or from groups of records is covered below
* Select only the fields you need ("projection") using a FOREACH
  - project just the core stats -- Specifying Which Columns to Display and Giving Names to Output Columns
* Sample: see statistics chapter
* Ssee below: JOINs are often used to filter items in one table using fields from another table

=== Splitting into pieces

* Split using filter: Bill James' black ink metric?
    * Write into named files: game logs by team. Warn about files count.
    * Combine small files: (find the worst offender and repair it)
    * case statement?
* splitting into uniform chunks
  - records: use RANK then group on rank mod record size
  - byte size: use HDFS block size?
  - fraction: approximate -- use sort and N reducers
* Files named for key using Piggybank multistorage
* Files Named for explicit filter: Pitchers vs Non-pitchers; hofPlayers, All-stars, all qualified
  - note that it does not short-circuit and their is no "else" clause
  - call ahead to the transpose part of the summarizinator in statistics chapter
* Combine tables with UNION

For sort note a udf to unique (distinct) won't work because keys can be split


== Structural Operations

=== Aggregation for summary statistics

* Group and agg: 
    * career stats
    * HR Stats by year
* Summarizing with MIN(), MAX(), SUM(), AVG(), STDDEV, COUNT(), count star, count distinct, byte size, character size, line / word count
* Count vs COUNTSTAR 
   - number of missing values using countstar-count
* Fancy `FOREACH` lets you  operate on bags
  - batting average, slg and OPS for career
* GROUP BY ALL
  - explain algebraic aggregators make this ok (but disaster if not algebraic)
  - season-by-season trends
* Note: HAVING not needed, just use a filter after the group by.
* Re-injecting global totals
* Histogram
  - histogram of home runs per season (doesn't need bin)
  - histogram of career games
  - categorical bins for non-categorical data
* Cube and rollup
  - stats by team, division and league

=== Putting tables in context with JOIN and friends

* Join is a Group and Flatten
* Direct Join: Extend Records with Uniquely Matching Records from Another Table
  - hang full names off records from master file
* Many-to-many join: teams to stadiums; players to teams
  - parks: team seasons and count; distinct teams and count
* Sparse join for matching: geo names for stadiums
* Sparse join for filtering: all-star table (hall of fame table?)
* Self-join
* Distinct: players with a unique first name (once again we urge you: crawl through your data. Big data is a collection of stories; the power of its unusual effectiveness mode comes from the comprehensiveness of those stories. even if you aren't into baseball this celebration of the diversity of our human race and the exuberance of identity should fill you with wonder.)
* bag left outer join from DataFu
* Left outer join on three tables: http://datafu.incubator.apache.org/docs/datafu/guide/more-tips-and-tricks.html
* Sparse joins for filtering
    * HashMap (replicated) join
    * bloom filter join
* (add note) Joins on null values are dropped even when both are null. Filter nulls.
* Range query
    * using cross
    * using prefix and UDFs
* Semi-join
* Bitmap index
* Self-join for successive row differences
* Combining Rows in One Table with Rows in Another
* Finding Rows in One Table That Match Rows in Another
* Finding Rows with No Match in Another Table
* Section 12-10 Using a Join to Fill in Holes in a List
* Enumerating a Many-to-Many Relationship
* Comparing a Table to Itself
* Eliminating Duplicates from a Query Result:
    * and from a Self-Join Result Section
    * Eliminating Duplicates from a Table
* Getting the duplicated values -- group by, then emit bags with more than one size

=== Set Operations

* Union (make sure to note it doesn't dedupe and doesn't order)
* Intersect
* Distinct
* Difference (in a but not in b)
* Equality (use symmetric difference)
* Symmetric difference: in A or B but not in A intersect B -- do this with aggregation: count 0 or 1 and only keep 1
* http://datafu.incubator.apache.org/docs/datafu/guide/set-operations.html
* http://www.cs.tufts.edu/comp/150CPA/notes/Advanced_Pig.pdf

* Using DISTINCT to Eliminate Duplicates
* Eliminating rows that have a duplicated value (ie you're not comparing the whole thing) 
* Finding Values Associated with Minimum and Maximum Values
* Selecting Only Groups with Certain Characteristics
* Determining Whether Values are Unique

=== Structural Group Operations (ie non aggregating)

* Group flatten regroup
    * OPS+ -- group on season, normalize, reflatten
    * player's highest OPS+: season, normalize, flatten, group on player, top
* Group Elements From Multiple Tables On A Common Attribute (COGROUP)
* GROUP/COGROUP To Restructure Tables
* Self join of table on its next row (eg timeseries at regular sample)
* Working with NULL Values: Negating a Condition on a Column That Contains NULL Values Section; Writing Comparisons Involving NULL in Programs; Mapping NULL Values to Other Values
* Cogroup and aggregate (vs SQL Cookbook 3.10)
* Using DISTINCT to Eliminate Duplicates
* Finding Values Associated with Minimum and Maximum Values
* Selecting Only Groups with Certain Characteristics
* Determining Whether Values are Unique
* Finding Rows Containing Per-Group Minimum or Maximum Values
* Computing Team Standings
* Producing Master-Detail Lists and Summaries
* Find Overlapping Rows
* Find Gaps in Time-Series..
* Find Missing Rows in Series / Count all Values
* Normalize Denormalized 
* Denormalize Normalized 
* Transpose Numeric Data
* Calculating Differences Between Successive Rows
* Finding Cumulative Sums and Running Averages
* Section 13.3. Per-Group Descriptive Statistics Section 
* Counting Missing Values

=== Sorting and Ordering

* Operations on the order of records: Sorting, Shuffling, Ranking and Numbering
  - ORDER: multiple fields
  - (how do NULLs sort?)
  - RANK: Dense, not dense
  - ASC / DESC 
  - in SQL you can omit the sort expression from the table; fields must be there in Pig
* Note
* Top K: 
    * whole table: most hr in a season
    * most hr season-by-season

* Top K Records within a table using ORDER..LIMIT
    * Top K Within a Group using GROUP...FOREACH GENERATE TOP
  - middle K (LIMIT..OFFSET)
* Number records with a serial or unique index
* Running total http://en.wikipedia.org/wiki/Prefix_sum
* prefix sum value; by combining list ranking, prefix sums, and Euler tours, many important problems on trees may be solved by efficient parallel algorithms.[3]
* Shuffle a set of records
    * See notes on random numbers.
    * Don't use the pig ORDER operation for this (two passes) (can you count on the built-in sort?)
* Sorting a Result Set
* Selecting Records from the Beginning or End of a Result Set
* Pulling a Section from the Middle of a Result Set
* Calculating LIMIT Values from Expressions
* What to Do When LIMIT Requires the "Wrong" Sort Order
* Sorting with Order by; Sorting and NULL Values; Controlling Case Sensitivity of String Sorts
* Sorting Subsets of a Table;
* Displaying One Set of Values While Sorting by Another
* Controlling Summary Display Order
* Finding Smallest or Largest Summary Values
* Randomizing a Set of Rows
* Assigning Ranks
* Counting and Identifying Duplicates

=== Graph Operatioms

* Neighborhood extraction
* Graph statistics: degree, clustering coefficient
* symmetrize a graph
* Triangles
* Eulerian Walk
* Connected components, Union find
* Graph matching
* Minimum spanning tree
* Pagerank
* label propagation
* k-means clustering
* Layout / Lgl
* List all children of AAA

=== Time Series Operations

* Interval coalesce: given a set of intervals, what is the smallest set of intervals that covers all of them?
    * for each team, what is the smallest number of stints (continuous player for team) needed so that every player was a teammate of one of them for that team? http://www.dba-oracle.com/t_sql_patterns_interval_coalesce.htm
* Turn player-seasons into stints (like the sessionize operation I think)
* Sessionize
  - sessionize web logs
  - Continuous game streak

=== Statistics

* Data Generation
* Make Reproducible Random Data - Varying Distribution
* Calculating Linear Regressions or Correlation Coefficients

* Calculate the summary statistics
  - Transpose (datafu) and flatten
  - group on attribute
  - calculate statistics
  - unionize
* Sniff through the data: extrema, mountweazels, exemplars
* Make a histogram
  - by scale and mod
  - by log scale and mod
  - by lookup table
  - by Z-score
  - equal-width
* Plot it: time series, trellis plot
  
* Summarizing with COUNT(), count star, count distinct, MIN(), MAX(), SUM(), AVG(), byte size, character size, line / word count
* Number of Distinct elements (Cardinality)
  - count distinct
  - hyperloglog
* Sum, sumsq, Entropy, Standard Deviation, variance, moments (eg GINI)
  - Correlation /covariance: what rate stats go with game time temp?
* Streaming moments (see Alon, Matias, and Szegedy)
* Histogram
  - quantiles
  - Median (approx, exact)
* Heavy hitters -- Count-Min sketch
* Running averages
* note: see below for Graph summaries



=== Advanced Patterns

* True if NONE Match: Find all rows in TABLE1 where there are no rows in TABLE2 that have a T2C value equal to the current T1A value in the TABLE1 table:
* True if ten match: Find all rows in TABLE1 where there are exactly ten rows in TABLE2 that have a T2B value equal to the current T1A value in the TABLE1 table
* Entity-Attribute-Value: bad idea in SQL
* Vertical and horizontal partitioning
* Serial ids -- natural ids
* Composite keys, foreign keys
* Small record with large blob (eg video file and metadata)
* Using float data type when you should use fixed point
* Group by has functionally dependent value (ie we know all elements of bag have same value for group)

* Pivot
* Histogram
* Skyline query (elements not dominated)
    * eliminate all players with no claim to be the best ever: their full set of core stats are less than some other player's full set of core stats. Related to convex hull http://www.cs.umd.edu/class/spring2005/cmsc828s/slides/skyline.pdf
    * like the hipmunk "agony" ranking
    * http://projekter.aau.dk/projekter/files/77335632/Scientific_Article.pdf - do this with quad keys - http://www.vldb.org/pvldb/vol6/p2002-shim.pdf
* Relational division
    * for each job listing (table of name, qualification pairs), find applicants who have all job qualifications (table is listing if, qualification pairs)
    * an applicant who is not qualified has one (listing, qual) pair missing
    * or use counting?
* Outer union
* Complex constraint
* Nested intervals
* Transitive closure
* Hierarchical total
* Small result set from a few tables with specific criteria applied to those tables
* Small result set based on criteria applied to tables other than the data source tables
* Small result set based on the intersection of several broad criteria
* Small result set from one table, determined by broad selection criteria applied to two or more additional tables 
* Large result set
* Result set obtained by self-joining on one table
* Result set obtained on the basis of aggregate function(s)
* Result set obtained by simple searching or by range searching on dates
* Result set predicated on the absence of other data


* Chapter 1 - Counting in SQL
    * List of patterns 
    * Introduction to SQL Counting 
    * Counting Ordered Rows 
    * Conditional Summation with CASE Operator 
    * Indicator and Step Functions 
    * A Case for the CASE Operator 
    * Summarizing by more than one Relation 
    * Interval Coalesce
* Chapter 2 - Integer Generators in SQL
    * Integers Relation 
    * Recursive With 
    * Big Table 
    * Table Function 
    * Cube 
    * Hierarchical Query 
    * String Decomposition 
    * Enumerating Pairs 
    * Enumerating Sets of Integers 
    * Discrete Interval Sampling
* Chapter 3 - Exotic Operators in SQL
    * Introduction to SQL exotic operators 
    * List Aggregate 
    * Product 
    * Factorial 
    * Interpolation 
    * Pivot 
    * Symmetric Difference 
    * Histograms in SQL 
    * Equal-Width Histogram 
    * Equal-Height Histogram 
    * Logarithmic Buckets 
    * Skyline Query 
    * Relational Division 
    * Outer Union
* Chapter 4 - SQL Constraints
    * Function Based Constraints 
    * Symmetric Functions 
    * Materialized View Constraints 
    * Disjoint Sets 
    * Disjoint Intervals 
    * Temporal Foreign Key Constraint 
    * Cardinality Constraint 
* Chapter 5 - Trees in SQL
    * Materialized Path 
    * Nested Sets 
    * Interval Halving 
    * From Binary to N-ary Trees 
    * Matrix Encoding 
    * Parent and Children Query 
    * Nested Intervals 
    * Descendants Query 
    * Ancestor Criteria 
    * Ancestors Query 
    * Converting Matrix to Path 
    * Inserting Nodes 
    * Relocating Tree Branches 
    * Ordering 
    * Exotic Labeling Schemas 
    * Dietz Encoding 
    * Pre-order – Depth Encoding 
    * Reversed Nesting 
    * Ordered Partitions
* Chapter 6 - Graphs in SQL
    * Schema Design 
    * Tree Constraint 
    * Transitive Closure 
    * Recursive SQL 
    * Connect By 
    * Incremental Evaluation 
    * Hierarchical Weighted Total 
    * Generating Baskets 
    * Comparing Hierarchies



Credits

* Art of SQL
* SQL patterns
* Baseball hacks
* MySQL patterns
* SQL Design Patterns http://www.rampant-books.com/book_0601_sql_coding_styles.htm http://www.nocoug.org/download/2006-11/sql_patterns.ppt
* DB2 cookbook
* Patterns for improving runtime: http://www.idi.ntnu.no/~noervaag/papers/VLDBJ2013_MapReduceSurvey.pdf

Instead of counting with the count( ) function, we can, at the
same time as we compute the total count, add 1 if amount_diff is not 0, and 0 otherwise.

==== combining into fewer files

==== Sampling

We'll cover sampling in the chapter on statistics, but so you know what to look for there:

* Random sampling using the traditional pseudo-random number generators (which can be dangerous; we'll tell you how to do it right) (use input filename as seed)
* Consistent sampling returns a fraction of records by _key_: if a record with the key "chimpanzee" is selected into the sample, all records with that key are selected into the sample.
* (with/without replacement; weighted)
* Reservoir sampling selects a given number of records. A uniform reservoir sample with count 100, say, would return 100 records, each with the same chance of being selected, regardless of the size of the dataset.
* Subuniverse sampling selects a set of records and all associated records with it -- useful when you want to be able to joins on the sampled data, or to select a dense subgraph of a network. (TECH: is "dense subgraph" right?)
* Stratified sampling: sampling from groups/bins/strata/whatever - http://en.wikipedia.org/wiki/Stratified_sampling
* Sampling into multiple groups eg for bootstrapping
* Note that pig sample is mathematically lame (see Datafu for why)
* Note that pig sample is nice about eliminating records while loading (find out if Datafu does too)
* Warning I may have written lies about reservoir sampling make sure to review
* Spatial Sampling
* Also: generating distributions (use the random.org data set and generate a column for each dist using it)
* Expand the random.org by taking each r.o number as seed

=== SQL-to-Pig-to-Hive

* SELECT..WHERE
* SELECT...LIMit
* GROUP BY...HAVING
* SELECT WHERE... ORDER BY
* SELECT WHERE... SORT BY (just use reducer sort) ~~ (does reducer in Pig guarantee this?)
* SELECT … DISTRIBUTE BY … SORT BY ...
* SELECT ... CLUSTER BY (equiv of distribute by X sort by X)
* Indexing tips
* CASE...when...then
* Block Sampling / Input pruning
* SELECT country_name, indicator_name, `2011` AS trade_2011 FROM wdi WHERE (indicator_name = 'Trade (% of GDP)' OR indicator_name = 'Broad money (% of GDP)') AND `2011` IS NOT NULL CLUSTER BY indicator_name;

SELECT columns or computations FROM table WHERE condition GROUP BY columns HAVING condition ORDER BY column  [ASC | DESC] LIMIT offset,count;

==== A Foolish Optimization --> probably a sidebar, late in the chapter

Make this be more generally "don't use the O(N) algorithm that works locally" -- fisher-yates and top-k-via-heap being two examples

=== Pig Functions act on fields --> chapter 4
 
* Add all the DataFu operations: http://datafu.incubator.apache.org/docs/datafu/guide/bag-operations.html and coalesce http://datafu.incubator.apache.org/docs/datafu/guide/more-tips-and-tricks.html and maybe others

==== Ready Reckoner: How fast should your Pig fly? --> not sure what this is

ah. yes this should move to the first tuning chapter. The idea is to have you run through a set of pig scripts with datasets of defined size, measuring the throughput of the core operations. The result is a ready reckoner that lets you estimate how long your job _should_ take (and how many map-reduce stages it will use).

==== Pig Gotchas --> probably a sidebar, halfway through the chapter

[[analytic_patterns]]

=== FILTER: eliminate records using given criteria

The `FILTER` operation select a subset of records. This example selects all wikipedia articles that contain the word 'Hadoop':

----
articles = LOAD './data/wp/articles.tsv' AS (page_id: long, namespace: int, wikipedia_id: chararray, revision_id: long, timestamp: long, title: chararray, redirect: chararray, text: chararray);
hadoop_articles = FILTER articles BY text matches '.*Hadoop.*';
STORE hadoop_articles INTO './data/tmp/hadoop_articles.tsv';
----

Filter as early as possible -- and in all other ways reduce the number of records you're working with. (This may sound obvious, but in the next chapter (TODO ref) we'll highlight many non-obvious expressions of this rule).

It's common to want to extract a _uniform_ sample -- one where every record has an equivalent chance of being selected. Pig's `SAMPLE` operation does so by generating a random number to select records. This brings an annoying side effect: the output of your job is different on every run. A better way to extract a uniform sample is the "consistent hash digest" -- we'll describe it, and much more about sampling, in the Statistics chapter (TODO  ref).



=== LIMIT selects only a few records

The `LIMIT` operator selects only a given number of records.
In general, you have no guarantees about which records it will select. Changing the number of mappers or reducers, small changes in the data, and so forth can change which records are selected. However, using the `ORDER` operator before a `LIMIT` _does_ guarantee you will get the top `k` records -- not only that, it applies a clever optimization (reservoir sampling, see TODO ref) that sharply limits the amount of data sent to the reducers.
If you truly don't care which records to select, just use one input file (`some_data/part-00000`, not all of `some_data`).

TODO: map-reduce example





=== FOREACH: modify the contents of records individually

We can now properly introduce you to the first interesting Pig command. A `FOREACH` makes simple transformations to each record.

For example, baseball fans use a few rough-but-useful player statistics to compare players' offensive performance: batting average, slugging average, and offensive percentage. This script calculates just those statistics, along with the player's name, id and number of games played.

----
player_seasons = LOAD `player_seasons` AS (...);
qual_player_seasons = FILTER player_years BY plapp > what it should be;
player_season_stats = FOREACH qual_player_seasons GENERATE
   player_id, name, games,
   hits/ab AS batting_avg,
   whatever AS slugging_avg,
   whatever AS offensive_avg,
   whatever+whatever AS ops
   ;
STORE player_season_stats INTO '/tmp/baseball/player_season_stats';
----

This example digests the players table; selects only players who have more than a qualified number of plate appearances; and generates the stats we're interested in
(If you're not a baseball fan, just take our word that "these four fields are particularly interesting")

A `FOREACH` won't cause a new Hadoop job stage: it's chained onto the end of the preceding operation (and when it's on its own, like this one, there's just a single a mapper-only job). A FOREACH always produces exactly the same count of output records as input records.

Within the GENERATE portion of a FOREACH, you can apply arithmetic expressions (as shown); project fields (rearrange and eliminate fields); apply the FLATTEN operator (see below); and apply Pig functions to fields. Let's look at Pig's functions.

TODO: map reduce demonstration



=== Select only the fields you need ("projection") using a FOREACH

TODO-qem: leave this here, or merge back in to FOREACH section
TODO: projection (using only a subset of fields) -- either here, or with limit/filter/sample






=== Group and Flatten

The fundamental Map/Reduce operation is to group a set of records and operate on that group. In fact, it’s a one-liner in Pig:

----
BINS = Group WP_pageviews by (date, hour)
DESCRIBE BINS
(TODO: SHOW OUTPUT)
----

The result is always a tuple whose first field is named “Group” -- holding the individual group keys in order. The next field has the full input record with all its keys, even the group key. Here’s a Wukong script that illustrates what is going on:

----
(TODO: Wukong script)
----

You can group more than one dataset at the same time. In weather data, there is one table listing the location and other essentials of each weather station and a set of tables listing, for each hour, the weather at each station. Here’s one way to combine them into a new table, giving the explicit latitude and longitude of every observation:

----
G1=GROUP WSTNS BY (ID1, ID2), WOBS BY (ID1, ID2);
G2=FLATTEN G1…
G3=FOR EACH G2 …
----

This is equivalent to the following Wukong job:

----
(TODO: Wukong job)
----

(TODO: replace with an example where you would use a pure code group).



=== Pig matches records in datasets using JOIN

TODO: a JOIN is used for: direct foreign key join; matching records on a criterion, possibly sparsely; set intersection.

For the examples in this chapter and often throughout the book, we will use the Retrosheet.org compendium of baseball data. We will briefly describe tables as we use them, but for a full explanation of its structure see the "Overview of Datasets" appendix (TODO:  REF).

The core operation you will use to put records from one table into context with data from another table is the JOIN. A common application of the JOIN is to reunite data that has been normalized -- that is to say, where the database tables are organized to eliminate any redundancy. For example, each Retrosheet game log lists the ballpark in which it was played but, of course, it does not repeat the full information about that park within every record. Later in the book, (TODO:  REF) we will want to label each game with its geo-coordinates so we can augment each with official weather data measurements.

To join the game_logs table with the parks table, extracting the game time and park geocoordinates, run the following Pig command:

----
gls_with_parks_j = JOIN
   parks     BY (park_id),
   game_logs BY (park_id);
explain gls_with_parks_j;
gls_with_parks = FOREACH gls_with_parks_j GENERATE
 (game_id, gamelogs.park_id, game_time, park_lng, statium_lat);
explain gls_with_parks;
(TODO output of explain command)
----

The output schema of the new `gls_with_parks` table has all the fields from the `parks` table first (because it's first in the join statement), stapled to all the fields from the `game_logs` table. We only want some of the fields, so immediately following the JOIN is a FOREACH to extract what we're interested in. Note there are now two 'park_id' columns, one from each dataset, so in the subsequent FOREACH, we need to dereference the column name with the table from which it came. (TODO: check that Pig does push the projection of fields up above the JOIN). If you run the script, 'examples/geo/baseball_weather/geolocate_games.pig' you will see that its output has example as many records as there are 'game_logs' because there is exactly one entry in the 'parks' table for each park.

In the general case, though, a JOIN can be many to many. Suppose we wanted to build a table listing all the home ballparks for every player over their career. The 'player_seasons' table has a row for each year and team over their career. If a player changed teams mid year, there will be two rows for that player. The 'park_years' table, meanwhile, has rows by season for every team and year it was used as a home stadium. Some ballparks have served as home for multiple teams within a season and in other cases (construction or special circumstances), teams have had multiple home ballparks within a season.

The Pig script (TODO: write script) includes the following JOIN:

----
JOIN
player_park_years=JOIN
 parks(year,team_ID),
 players(year,team_ID);
explain_player_park_year;
----

First notice that the JOIN expression has multiple columns in this case separated by commas; you can actually enter complex expressions here -- almost all (but not all) the things you do within a FOREACH. If you examine the output file (TODO: name of output file), you will notice it has appreciably more lines than the input 'player' file. For example (TODO: find an example of a player with multiple teams having multiple parks), in year x player x played for the x and the y and y played in stadiums p and q. The one line in the 'players' table has turned into three lines in the 'players_parks_years' table.

The examples we have given so far are joining on hard IDs within closely-related datasets, so every row was guaranteed to have a match. It is frequently the case, however, you will join tables having records in one or both tables that will fail to find a match. The 'parks_info' datasets from Retrosheet only lists the city name of each ballpark, not its location. In this case we found a separate human-curated list of ballpark geolocations, but geolocating records -- that is, using a human-readable location name such as "Austin, Texas" to find its nominal geocoordinates (-97.7,30.2) -- is a common task; it is also far more difficult than it has any right to be, but a useful first step is match the location names directly against a gazette of populated place names such as the open source Geonames dataset.

Run the script (TODO: name of script) that includes the following JOIN:

----
park_places = JOIN
 parks BY (location) LEFT OUTER,
 places BY (concatenate(city, ", ", state);
DESCRIBE park_places;
----

In this example, there will be some parks that have no direct match to location names and, of course, there will be many, many places that do not match a park. The first two JOINs we did were "inner" JOINs -- the output contains only rows that found a match. In this case, we want to keep all the parks, even if no places matched but we do not want to keep any places that lack a park. Since all rows from the left (first most dataset) will be retained, this is called a "left outer" JOIN. If, instead, we were trying to annotate all places with such parks as could be matched -- producing exactly one output row per place -- we would use a "right outer" JOIN instead. If we wanted to do the latter but (somewhat inefficiently) flag parks that failed to find a match, you would use a "full outer" JOIN. (Full JOINs are pretty rare.)

TODO: discuss use of left join for set intersection.

In a Pig JOIN it is important to order the tables by size -- putting the smallest table first and the largest table last. (You'll learn why in the "Map/Reduce Patterns" (TODO:  REF) chapter.) So while a right join is not terribly common in traditional SQL, it's quite valuable in Pig. If you look back at the previous examples, you will see we took care to always put the smaller table first. For small tables or tables of similar size, it is not a big deal -- but in some cases, it can have a huge impact, so get in the habit of always following this best practice.

----
NOTE
A Pig join is outwardly similar to the join portion of a SQL SELECT statement, but notice that  although you can place simple expressions in the join expression, you can make no further manipulations to the data whatsoever in that statement. Pig's design philosophy is that each statement corresponds to a specific data transformation, making it very easy to reason about how the script will run; this makes the typical Pig script more long-winded than corresponding SQL statements but clearer for both human and robot to understand.
----


==== Join Practicalities

The output of the Join job has one line for each discrete combination of A and B. As you will notice in our Wukong version of the Join, the job receives all the A records for a given key in order, strictly followed by all the B records for that key in order. We have to accumulate all the A records in memory so we know what rows to emit for each B record. All the A records have to be held in memory at the same time, while all the B records simply flutter by; this means that if you have two datasets of wildly different sizes or distribution, it is worth ensuring the Reducer receives the smaller group first. In Wukong, you do this by giving it an earlier-occurring field group label; in Pig, always put the table with the largest number of records per key last in the statement.





=== Group Elements From Multiple Tables On A Common Attribute (COGROUP)

The fundamental structural operation in Map/Reduce is the COGROUP:  assembling records from multiple tables into groups based on a common field; this is a one-liner in Pig, using, you guessed it, the COGROUP operation. This script returns, for every world map grid cell, all UFO sightings and all airport locations within that grid cell footnote:[We've used the `quadkey` function to map geocoordinates into grid cells; you'll learn about in the Geodata Chapter (REF)]:

----
sightings = LOAD('/data/gold/geo/ufo_sightings/us_ufo_sightings.tsv') AS (...);
airports     = LOAD('/data/gold/geo/airflights/us_airports.tsv') AS (...);
cell_sightings_airports = COGROUP
   sightings by quadkey(lng, lat),
   airports  by quadkey(lng, lat);
STORE cell_sightings_locations INTO '...';
----

In the equivalent Map/Reduce algorithm, you label each record by both the indicated key and a number based on its spot in the COGROUP statement (here, records from sightings would be labeled 0 and records from airports would be labeled 1). Have Hadoop then PARTITION and GROUP on the COGROUP key with a secondary sort on the table index. Here is how the previous Pig script would be done in Wukong:

----
mapper(partition_keys: 1, sort_keys: 2) do
 recordize_by_filename(/sightings/ => Wu::Geo::UfoSighting, /airport/ => Wu::Geo::Airport)
 TABLE_INDEXES = { Wu::Geo::UfoSighting => 0, Wu::Geo::Airport => 1 }
 def process(record)
   table_index = TABLE_INDEXES[record.class] or raise("Don't know how to handle records of type '{record.class}'")
   yield( [Wu::Geo.quadkey(record.lng, record.lat), table_index, record.to_wire] )
 end
end

reducer do
 def recordize(quadkey, table_index, jsonized_record) ; ...; end
 def start(key, *)
   @group_key = key ;
   @groups = [ [], [] ]
 end
 def accumulate(quadkey, table_index, record)
   @groups[table_index.to_i] << record
 end
 def finalize
   yield(@group_key, *groups)
 end
end
----

The Mapper loads each record as an object (using the file name to recognize which class to use) and then emits the quadkey, the table index (0 for sightings, 1 for airports) and the original record's fields. Declaring partition keys 1, sort keys 2 insures all records with the same quadkey are grouped together on the same Reducer and all records with the same table index arrive together. The body of the Reducer makes temporary note of the GROUP key, then accumulates each record into an array based on its type.

The result of the COGROUP statement always has the GROUP key as the first field. Next comes the set of elements from the table named first in the COGROUP statement -- in Pig, this is a bag of tuples, in Wukong, an array of objects. After that comes the set of elements from the next table in the GROUP BY statement and so on.

While a standalone COGROUP like this is occasionally interesting, it is also the basis for many other common patterns, as you'll see over the next chapters.

// ==== Regexp matching in Pig

// === Grouping operations (JOIN, GROUP, COGROUP, CUBE, DISTINCT, CROSS) place records into context with each other.

=== Complex `FOREACH`

Let's continue our example of finding the list of home ballparks for each player over their career.

----
parks = LOAD '.../parks.tsv' AS (...);
team_seasons = LOAD '.../team_seasons.tsv' AS (...)
park_seasons = JOIN parks BY park_id, team_seasons BY park_id;
park_seasons = FOREACH park_seasons GENERATE
   team_seasons.team_id, team_seasons.year, parks.park_id, parks.name AS park_name;

player_seasons = LOAD '.../player_seasons.tsv' AS (...);
player_seasons = FOREACH player_seasons GENERATE
   player_id, name AS player_name, year, team_id;
player_season_parks = JOIN
   parks           BY (year, team_id),
   player_seasons BY (year, team_id);
player_season_parks = FOREACH player_season_parks GENERATE player_id, player_name, parks::year AS year, parks::team_id AS team_id, parks::park_id AS park_id;

player_all_parks = GROUP player_season_parks BY (player_id);
describe player_all_parks;
Player_parks = FOREACH player_all_parks {
   player = FirstFromBag(players);
   home_parks = DISTINCT(parks.park_id);
   GENERATE group AS player_id,
       FLATTEN(player.name),
       MIN(players.year) AS beg_year, MAX(players.year) AS end_year,
       home_parks; -- TODO ensure this is still tuple-ized
}
----

Whoa! There are a few new tricks here. This alternative `{` curly braces form of `FOREACH` lets you describe its transformations in smaller pieces, rather than smushing everything into the single `GENERATE` clause. New identifiers within the curly braces (such as `player`) only have meaning within those braces, but they do inform the schema.

We would like our output to have one row per player, whose fields have these different flavors:

* Aggregated fields (`beg_year`, `end_year`) come from functions that turn a bag into a simple type (`MIN`, `MAX`).
* The `player_id` is pulled from the `group` field, whose value applies uniformly to the the whole group by definition. Note that it's also in each tuple of the bagged `player_park_seasons`, but then you'd have to turn many repeated values into the one you want...
* ... which we have to do for uniform fields (like `name`) that are not part of the group key, but are the same for all elements of the bag. The awareness that those values are uniform comes from our understanding of the data -- Pig doesn't know that the name will always be the same. The FirstFromBag (TODO fix name) function from the Datafu package grabs just first one of those values
* Inline bag fields (`home_parks`), which continue to have multiple values.

We've applied the `DISTINCT` operation so that each home park for a player appears only once. `DISTINCT` is one of a few operations that can act as a top-level table operation, and can also act on bags within a foreach -- we'll pick this up again in the next chapter (TODO ref). For most people, the biggest barrier to mastery of Pig is to understand how the name and type of each field changes through restructuring operations, so let's walk through the schema evolution.

We `JOIN`ed player seasons and team seasons on `(year, team_id)`. The resulting schema has those fields twice. To select the name, we use two colons (the disambiguate operator): `players::year`.

After the `GROUP BY` operation, the schema is `group:int, player_season_parks:bag{tuple(player_id, player_name, year, team_id, park_id, park_name)}`. The schema of the new `group` field matches that of the `BY` clause: since `park_id` has type chararray, so does the group field. (If we had supplied multiple fields to the `BY` clause, the `group` field would have been of type `tuple`). The second field, `player_season_parks`, is a bag of size-6 tuples. Be clear about what the names mean here: grouping on the `player_season_parks` _table_ (whose schema has six fields) produced the `player_parks` table. The second field of the `player_parks` table is a tuple of size six (the six fields in the corresponding table) named `player_season_parks` (the name of the corresponding table).

So within the `FOREACH`, the expression `player_season_parks.park_id` is _also_ a bag of tuples (remember, bags only hold tuples!), now size-1 tuples holding only the park_id. That schema is preserved through the `DISTINCT` operation, so `home_parks` is also a bag of size-1 tuples.

NOTE: Some late night under deadline, Pig will supply you with the absolutely baffling error message "scalar has more than one row in the output". You've gotten confused and used the tuple element operation (`players.year`) when you should have used the disambiguation operator (`players::year`). The dot is used to reference a tuple element, a common task following a `GROUP`. The double-colon is used to clarify which specific field is intended, common following a join of tables sharing a field name.

----
   team_park_seasons = LOAD '/tmp/team_parks.tsv' AS (
       team_id:chararray,
       park_years: bag{tuple(year:int, park_id:chararray)},
       park_ids_lookup: map[chararray]
       );
   team_parks = FOREACH team_park_seasons { distinct_park_ids = DISTINCT park_years.park_id; GENERATE team_id, FLATTEN(distinct_park_ids) AS park_id; }
   DUMP team_parks;
----

=== Ungrouping operations (FOREACH..FLATTEN) expand records

So far, we've seen using a group to aggregate records and (in the form of `JOIN’) to match records between tables.
Another frequent pattern is restructuring data (possibly performing aggregation at the same time). We used this several times in the first exploration (TODO ref): we regrouped wordbags (labelled with quadkey) for quadtiles containing composite wordbags; then regrouping on the words themselves to find their geographic distribution.

The baseball data is closer at hand, though, so l

----
team_player_years = GROUP player_years BY (team,year);
FOREACH team_player_years GENERATE
   FLATTEN(player_years.player_id), group.team, group.year, player_years.player_id;
----

In this case, since we grouped on two fields, `group` is a tuple; earlier, when we grouped on just the `player_id` field, `group` was just the simple value.

The contextify / reflatten pattern can be applied even within one table. This script will find the career list of teammates for each player -- all other players with a team and year in common footnote:[yes, this will have some false positives for players who were traded mid-year. A nice exercise would be to rewrite the above script using the game log data, now defining teammate to mean "all other players they took the field with over their career".].

----
GROUP player_years BY (team,year);
FOREACH
   cross all players, flatten each playerA/playerB pair AS (player_a
FILTER coplayers BY (player_a != player_b);
GROUP by playerA
FOREACH {
   DISTINCT player B
}
----

Here's another

The result of the cross operation will include pairing each player with themselves, but since we don't consider a player to be their own teammate we must eliminate player pairs of the form `(Aaronha, Aaronha)`. We did this with a FILTER immediate before the second GROUP (the best practice of removing data before a restructure), but a defensible alternative would be to `SUBTRACT` playerA from the bag right after the `DISTINCT` operation.

=== Sorting (ORDER BY, RANK) places all records in total order

To put all records in a table in order, it's not sufficient to use the sorting that each reducer applies to its input. If you sorted names from a phonebook, file `part-00000` will have names that start with A, then B, up to Z; `part-00001` will also have names from A-Z; and so on. The collection has a _partial_ order, but we want the 'total order' that Pig's `ORDER BY` operation provides. In a total sort, each record in `part-00000` is in order and precedes every records in `part-00001`; records in `part-00001` are in order and precede every record in `part-00002`; and so forth. From our earlier example to prepare topline batting statistics for players, let's sort the players in descending order by the "OPS" stat (slugging average plus offensive percent, the simplest reasonable estimator of a player's offensive contribution).

----
player_seasons = LOAD `player_seasons` AS (...);
qual_player_seasons = FILTER player_years BY plapp > what it should be;
player_season_stats = FOREACH qual_player_seasons GENERATE
   player_id, name, games,
   hits/ab AS batting_avg,
   whatever AS slugging_avg,
   whatever AS offensive_pct
   ;
player_season_stats_ordered = ORDER player_season_stats BY (slugging_avg + offensive_pct) DESC;
STORE player_season_stats INTO '/tmp/baseball/player_season_stats';
----

This script will run _two_ Hadoop jobs. One pass is a light mapper-only job to sample the sort key, necessary for Pig to balance the amount of data each reducer receives (we'll learn more about this in the next chapter (TODO ref). The next pass is the map/reduce job that actually sorts the data: output file `part-r-00000` has the earliest-ordered records, followed by `part-r-00001`, and so forth.

NOTE: The custom partitioner of an `ORDER` statement subtly breaks the reducer contract: it may send records having the same key to different reducers. This will cause them to be in different output (`part-xxxxx`) files, so make sure anything using the sorted data doesn't assume keys uniquely correspond to files.



== Core Analytic Patterns

TODO: parts of this have been uncarefully split into 05-first_exploration, so the plot won't make sense in some places.

Now that you've met the fundamental analytic operations -- in both their map/reduce and table-operation form -- it's time to put them to work in an actual data exploration.

This chapter will equip you to think tactically, to think in terms of the changes you would like to make to the data. Each section introduces a repeatedly-useful data transformation pattern, demonstrated in Pig (and, where we'd like to reinforce the record-by-record action, in Wukong as well).

// The real goal is to learn to think strategically, to be able to look at the data you have and recognize the steps required to produce the data you want. You do not do this, however, by thinking about how to coordinate the fundamental operations you have just learned directly into your solution any more than a general thinks about coordinating the actions of every individual soldier while preparing a battle plan.

=== Pattern: Atom-only Records

All of the fields in the table we've just produced are atomic -- strings, numbers and such, rather than bags or tuples -- what database wonks call "First Normal Form". There is a lot of denormalization (each article's quadcell and total term count are repeated for every term in the article), but the simplicity of each record's schema has a lot of advantages.

Think of this atom-only form as the neutral fighting stance for your tables. From here we're ready to put each record into context of other records with the same term, same geolocation, same frequency; we can even reassemble the wordbag by grouping on the page_id. The exploration will proceed from here by reassembling these records into various context groups, operating on those groups, and then expanding the result back into atom-only form.

=== Pattern: Blowing Up records: Wordbag, Flatten

TODO: flatten

TODO: wordbag


=== GROUP/COGROUP To Restructure Tables

This next pattern is one of the more difficult to picture but also one of the most important to master. Once you can confidently recognize and apply this pattern, you can consider yourself a black belt in the martial art of Map/Reduce.

(TODO: describe this pattern)

=== Pattern: Extend Records with Uniquely Matching Records from Another Table

Using a join as we just did -- to extend the records in one table with the fields from one matching record in another -- is a very common pattern. Datasets are commonly stored as tables in 'normalized' form -- that is, having tables structured to minimize redundancy and dependency. The global hourly weather dataset has one table giving the metadata for every weather station: identifiers, geocoordinates, elevation, country and so on. The giant tables listing the hourly observations from each weather station are normalized to not repeat the station metadata on each line, only the weather station id. However, later in the book (REF) we'll do geographic analysis of the weather data -- and one of the first tasks will be to denormalize the geocoordinates of each weather station with its observations, letting us group nearby observations.

Another reason to split data across tables is 'vertical partitioning': storing fields that are very large or seldom used in context within different tables. That's the case with the Wikipedia article tables -- the geolocation information is only relevant for geodata analysis; the article text is both large and not always relevant.

=== Pattern: Summarizing Groups

Pretty much every data exploration you perform will involve summarizing datasets using statistical aggregations -- counts, averages and so forth. You have already seen an example of this when we helped the reindeer count UFO visit frequency by month and later in the book, we will devote a whole chapter to statistical summaries and aggregation.

=== Pattern: Re-injecting global totals

We also extract two global statistics: the number of distinct terms, and the number of distinct usages. This brings up one of the more annoying things about Hadoop programming. The global_term_info result is two lousy values, needed to turn the global _counts_ for each term into the global _frequency_ for each term. But a pig script just orchestrates the top-level motion of data: there's no intrinsic way to bring the result of a step into the declaration of following steps. The proper recourse is to split the script into two parts, and run it within a workflow tool like Rake, Drake or Oozie. The workflow layer can fish those values out of the HDFS and inject them as runtime parameters into the next stage of the script.

We prefer to cheat. We instead ran a version of the script that found the global count of terms and usages, then copy/pasted their values as static parameters at the top of the script. This also lets us calculate the ppm frequency of each term and the other term statistics in a single pass. To ensure our time-traveling shenanigans remain valid, we add an `ASSERT` statement which compares the memoized values to the actual totals.

==== Select a Fixed Number of Arbitrary Records (LIMIT)

The Pig LIMIT operation arbitrarily selects, at most, the specified number of records from a table.

----
(TODO: example)
----

(TODO: Is there a non-Reduce way to do this?)

In the simplest Map/Reduce equivalent, Mappers emit each record unchanged until they hit the specified limit (or reach the end of their input). Those output records are sent to a single Reducer, which itself emits each record unchanged until it has hit the specified limit and does nothing on all subsequent records.

(TODO: Do we want to talk about a non-single Reducer approach?)

A Combiner is helpful here in the predominant case where the specified limit is small, as it will eliminate excess records before they are sent to the Reducer and at each merge/sort pass.

==== Top K Records (ORDER..LIMIT)

The naive way to extract the top K elements from a table is simply to do an ORDER and then a LIMIT. For example, the following script will identify the top 100 URLs from the waxy.org weblog dataset.

----
logs=LOAD '/data/gold/waxy/whatever.log' AS (...) USING APACHE LOG READER;
logs=FOREACH logs GENERATE url;
url_logs = GROUP logs BY url;
URL_COUNTS=FOREACH url_logs GENERATE
    COUNT_STAR(url_logs) AS views,
    group AS url;
url_counts_o = ORDER url_counts BY views PARALLEL 1;
top_url_counts = LIMIT url_counts_o 100;
STORE top_url_counts INTO '/data/out/weblogs/top_url_counts';
----

There are two useful optimizations to make when K (the number of records you will keep) is much less than N (the number of records in the table). The first one, which Pig does for you, is to only retain the top K records at each Mapper; this is a great demonstration of where a Combiner is useful:  After each intermediate merge/sort on the Map side and the Reduce side, the Combiner discards all but the top K records.

==== Top K Within a Group

There is a situation where the heap-based top K algorithm is appropriate:  finding the top K elements for a group. Pig's 'top' function accepts a bag and returns a bag with its top K elements. Here is an example that uses the World Cup dataset to find the top 10 URLs for each day of the tournament:

----
visits = load ('worldcup');
visits = FOREACH visits generate day, url;
visits by day = GROUP visits by day;
top visits by day = FOREACH visits url = GROUP visits by url;
   generate GROUP as day, top (visits, top visit URLs, COUNT_STAR (visit urls), 100;
store top visits by url into 'top visits by url';
----


== TODO: need to sort out where following sections go

==== A Foolish Optimization

TODO: make this a sidebar?

We will tell you about another "optimization," mostly because we want to illustrate how a naive performance estimation based on theory can lead you astray in practice. In principle, sorting a large table in place takes 'O(N log N)' time. In a single compute node context, you can actually find the top K elements in 'O(N log K)' time -- a big savings since K is much smaller than N. What you do is maintain a heap structure; for every element past the Kth, if it is larger than the smallest element in the heap, remove the smallest member of the heap and add the element to the heap. While it is true that 'O(N log K)' beats 'O(N log N)', this reasoning is flawed in two ways. First, you are not working in a single-node context; Hadoop is going to perform that sort anyway. Second, the fixed costs of I/O almost always dominate the cost of compute (FOOTNOTE:  Unless you are unjustifiably fiddling with a heap in your Mapper.)

The 'O(log N)' portion of Hadoop's log sort shows up in two ways:  The N memory sort that precedes a spill is 'O(N log N)' in compute time but less expensive than the cost of spilling the data. The true 'O(N log N)' cost comes in the reducer: 'O(log N)' merge passes, each of cost 'O(N)'. footnote:[If initial spills have M records, each merge pass combines B spills into one file, and we can skip the last merge pass, the total time is `N (log_B(N/M)-1).` [TODO: double check this]. But K is small, so there should not be multiple merge passes; the actual runtime is 'O(N)' in disk bandwidth. Avoid subtle before-the-facts reasoning about performance; run your job, count the number of merge passes, weigh your salary against the costs of the computers you are running on, and only then decide if it is worth optimizing.


=== Pig Functions act on fields

TODO-qem: decide whether to leave this inline, or sidebar? (rule of thumb for sidebar: "is it an aside/distraction from the main text?")

Pig offers a sparse but essential set of built-in functions -- the Pig cheatsheet (TODO ref) at the end of the book gives a full list. The whole middle of the book is devoted to examples of Pig and map/reduce programs in practice (and in particular a chapter on Statistics), so we'll just list the highlights here:

* *Math functions* for all the things you'd expect to see on a good calculator: `LOG`/`LOG10`/`EXP`, `RANDOM`, `ROUND`/`FLOOR`/`CEIL`, `ABS`, trigonometric functions, and so forth.
* *String comparison*:
 - `matches` tests a value against a regular expression:
 - Compare strings directly using `==`. `EqualsIgnoreCase` does a case-insensitive match, while `STARTSWITH`/`ENDSWITH` test whether one string is a prefix or suffix of the other.
 - `SIZE` returns the number of characters in a `chararray`, and the number of bytes in a `bytearray`. Be reminded that characters often occupy more than one byte: the string 'Motörhead' has nine characters, but because of its umlaut-ed 'ö' the string occupies ten bytes. You can use `SIZE` on other types, too; but as mentioned, use `COUNT_STAR` and not `SIZE` to find the number of elements in a bag.
 - `INDEXOF` finds the character position of a substring within a `chararray` // `LAST_INDEX_OF`
* *Transform strings*:
 - `CONCAT` concatenates all its inputs into a new string
 - `LOWER` converts a string to lowercase characters; `UPPER` to all uppercase // `LCFIRST`, `UCFIRST`
 - `TRIM` strips leading and trailing whitespace // `LTRIM`, `RTRIM`
 - `REPLACE(string, 'regexp', 'replacement')` substitutes the replacement string wherever the given regular expression matches, as implemented by `java.string.replaceAll`. If there are no matches, the input string is passed through unchanged.
 - `REGEX_EXTRACT(string, regexp, index)` applies the given regular expression and returns the contents of the indicated matched group. If the regular expression does not match, it returns NULL. The `REGEX_EXTRACT_ALL` function is similar, but returns a tuple of the matched groups.
 - `STRSPLIT` splits a string at each match of the given regular expression
 - `SUBSTRING` selects a portion of a string based on position
* *Datetime Functions*, such as `CurrentTime`, `ToUnixTime`, `SecondsBetween` (duration between two given datetimes)
* *Aggregate functions* that act on bags:
 - `AVG`, `MAX`, `MIN`, `SUM`
 - `COUNT_STAR` reports the number of elements in a bag, including nulls; `COUNT` reports the number of non-null elements. `IsEmpty` tests that a bag has elements. Don't use the quite-similar-sounding `SIZE` function on bags: it's much less efficient.
 - `SUBTRACT(bag_a, bag_b)` returns a new bag with all the tuples that are in the first but not in the second, and `DIFF(bag_a, bag_b)` returns a new bag with all tuples that are in either but not in both. These are rarely used, as the bags must be of modest size -- in general us an inner JOIN as described below.
 - `TOP(num, column_index, bag)` selects the top `num` of elements from each tuple in the given bag, as ordered by `column_index`. This uses a clever algorithm that doesn't require an expensive total sort of the data -- you'll learn about it in the Statistics chapter (TODO ref)
* *Conversion Functions* to perform higher-level type casting: `TOTUPLE`, `TOBAG`, `TOMAP`






====  More

TODO-qem: once I've sorted out the patterns in this chapter, consider moving this section to Chapter 4, "Fundamental Data Operations"

There are a few more Operators we will use later in the book:
Cube, which produces aggregations at multiple levels within a Group;
Rank, which is sugar on top of Order to produce a number, total-ordered set of records;
Split, to separate a dataset into multiple pieces; and
Union, to produce a new dataset to have all the records from its input datasets.

That’s really about it. Pig is an extremely sparse language. By having very few Operators and very uniform syntax (FOOTNOTE:  Something SQL users but non-enthusiasts like your authors appreciate), the language makes it easy for the robots to optimize the dataflow and for humans to predict and reason about its performance.

We won’t spend any more time introducing Pig, the language, as its usage will be fairly clear in context as you meet it later in the book. The online Pig manual at (TODO: REF) is quite good and for a deeper exploration, consult (TODO: Add name of best Pig book here).



==== Ready Reckoner: How fast should your Pig fly?

TODO: describe for each Pig command what jobs should result.

TODO-flip: (from qem) not sure what this section is supposed to mean?


==== Pig Gotchas

That one error where you use the dot or the colon when you should use the other. 
TODO-flip: this is covered elsewhere in this chapter, as a note under "Complex FOREACH"; we should move that here


Where to look to see that Pig is telling you have either nulls, bad fields, numbers larger than your type will hold or a misaligned schema.
