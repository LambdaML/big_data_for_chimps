== Hadoop Internals: Just Enough for Now

TODO: write Intro

TODO: Move the part about tiny files to here
TODO: See if other parts of the Hadoop tuning or internals chapters should move here or v/v

 3a: Hadoop Internals and Performance: Just Enough for Now
. the HDFS at moderate detail
. ??? job orchestration/lifecycle at light detail
  - how a job is born

The harsh realities of the laws of physics and economics prevent traditional data analysis solutions such as relational databases, supercomputing and so forth from economically scaling to arbitrary-sized data.  By comparison, Hadoop's Map/Reduce paradigm does not provide complex operations, modification of existing records, fine-grain control over how the data is distributed or anything else beyond the ability to write programs that adhere to a single, tightly-constrained template.  If Hadoop were a publishing medium, it would be one that refused essays, novels, sonnets and all other literary forms beyond the haiku:

 	data flutters by
     	elephants make sturdy piles
   	context yields insight

Our Map/Reduce haiku illustrates Hadoop's template:

1. The Mapper portion of your script processes records, attaching a label to each.
2. Hadoop assembles those records into context groups according to their label.
3. The Reducer portion of your script processes those context groups and writes them to a data store or external system.


What is remarkable is that from this single primitive, we can construct the familiar relational operations (such as `GROUP`s and `ROLLUP`s) of traditional databases, many machine-learning algorithms, matrix and graph transformations and the rest of the advanced data analytics toolkit.  In the next two chapters, we will demonstrate high-level relational operations and illustrate the Map/Reduce patterns they express.  In order to understand the performance and reasoning behind those patterns, let's first understand the motion of data within a Map/Reduce job.




=== The HDFS: Highly Durable Storage Optimized for Analytics ===

The HDFS, as we hope you’ve guessed, holds the same role within Hadoop that Nanette and her team of elephants do within C&E Corp.  It ensures that your data is always available for use, never lost or degraded and organized to support efficient Map/Reduce jobs.  Files are stored on the HDFS as blocks of limited size (128 MB is a common choice).  Each block belongs to exactly one file; a file larger than the block size is stored in multiple blocks.  The blocks are stored in cooked form as regular files on one of the Datanode’s regular volumes.  (Hadoop’s decision to use regular files rather than attempting lower-level access to the disk, as many traditional databases do, helps make it remarkably portable, promotes reliability and plays to the strengths of the operating system’s finely-tuned access mechanisms.)

The HDFS typically stores multiple replicas of each block (three is the universal default, although you can adjust it per file), distributed across the cluster.  Blocks within the same file may or may not share a Datanode but replicas never do (or they would not be replicas, would they?).  The obvious reason for this replication is availability and durability -- you can depend on finding a live Datanode for any block and you can depend that, if a Datanode goes down, a fresh replica can be readily produced.

JT and Nanette’s workflow illustrates the second benefit of replication:  being able to “move the compute to the data, not [expensively] moving the data to the compute.”  Multiple replicas give the Job Tracker enough options that it can dependably assign most tasks to be “Mapper-local.”

A special node called the _Namenode_ is responsible for distributing those blocks of data across the cluster.  Like Nanette, the Namenode holds no data, only a sort of file allocation table (FAT), tracking for every file the checksum responsible Datanodes and other essential characteristics of each of its blocks.  The Namenode depends on the Datanodes to report in regularly. Every three seconds, it sends a heartbeat -- a lightweight notification saying, basically, "I'm still here!". On a longer timescale, each Datanode prepares a listing of the replicas it sees on disk along with a full checksum of each replica's contents. Having the Datanode contact the Namenode is a good safeguard that it is operating regularly and with good connectivity. Conversely, the Namenode uses the heartbeat response as its opportunity to issue commands dening a struggling Datanode.

If, at any point, the Namenode finds a Datanode has not sent a heartbeat for several minutes, or if a block report shows missing or corrupted files, it will commission new copies of the affected blocks by issuing replication commands to other Datanodes as they heartbeat in.

A final prominent role the Namenode serves is to act as the public face of the HDFS.  The ‘put’ and ‘get’ commands you just ran were Java programs that made network calls to the Namenode.  There are API methods for the rest of the file system commands you would expect for use by that or any other low-level native client.  You can also access its web interface, typically by visiting port 50070 (`http://hostname.of.namenode:50070`), which gives you the crude but effective ability to view its capacity, operational status and, for the very patient, inspect the contents of the HDFS.

Sitting behind the scenes is the often-misunderstood secondary Namenode; this is not, as its name implies and as you might hope, a hot standby for the Namenode.  Unless you are using the “HA namenode” feature provided in later versions of Hadoop, if your Namenode goes down, your HDFS has gone down.  All the secondary Namenode does is perform some essential internal bookkeeping.  Apart from ensuring that it, like your Namenode, is _always_ running happily and healthily, you do not need to know anything more about the second Namenode for now.

One last essential to note about the HDFS is that its contents are immutable.  On a regular file system, every time you hit “save,” the application modifies the file in place -- on Hadoop, no such thing is permitted.  This is driven by the necessities of distributed computing at high scale but it is also the right thing to do.  Data analysis should proceed by chaining reproducible syntheses of new beliefs from input data.  If the actions you are applying change, so should the output.  This casual consumption of hard drive resources can seem disturbing to those used to working within the constraints of a single machine, but the economics of data storage are clear; it costs $0.10 per GB per month at current commodity prices, or one-tenth that for archival storage, and at least $50 an hour for the analysts who will use it.

Possibly the biggest rookie mistake made by those new to Big Data is a tendency to economize on the amount of data they store; we will try to help you break that habit.  You should be far more concerned with the amount of data you send over the network or to your CPU than with the amount of data you store and most of all, with the amount of time you spend deriving insight rather than acting on it.  Checkpoint often, denormalize when reasonable and preserve the full provenance of your results.
=== JT and Nanette at Work

JT and Nanette work wonderfully together -- JT rambunctiously barking orders, Nanette peacefully gardening her card catalog -- and subtly improve the efficiency of their team in a variety of ways. We'll look closely at their bag of tricks later in the book (TODO ref) but here are two. The most striking thing any visitor to the worksite will notice is how _calm_ everything is. One reason for this is Nanette's filing scheme, which designates each book passage to be stored by multiple elephants. Nanette quietly advises JT of each passage's location, allowing him to almost always assign his chimpanzees a passage held by the librarian in their cubicle. In turn, when an elephant receives a freshly-translated scroll, she makes two photocopies and dispatches them to two other cubicles. The hallways contain a stately parade of pygmy elephants, each carrying an efficient load; the only traffic consists of photocopied scrolls to store and the occasional non-cubicle-local assignment.

The other source of calm is on the part of their clients, who know that when Nanette's on the job, their archives are safe -- the words of Shakespeare will retain their eternal form footnote:[When Nanette is not on the job, it's a total meltdown -- a story for much later in the book. But you'd be wise to always take extremely good care of the Nanettes in your life.] To ensure that no passage is never lost, the librarians on Nanette's team send regular reports on the scrolls they maintain. If ever an elephant doesn't report in (whether it stepped out for an hour or left permanently), Nanette identifies the scrolls designated for that elephant and commissions the various librarians who hold other replicas of that scroll to make and dispatch fresh copies. Each scroll also bears a check of authenticity validating that photocopying, transferring its contents or even mouldering on the shelf has caused no loss of fidelity. Her librarians regularly recalculate those checks and include them in their reports, so if even a single letter on a scroll has been altered, Nanette can commission a new replica at once.


=== SIDEBAR: What's Fast At High Scale

image::images/02-Throughput-and-Cost-for-Compute-Primitives-aka-Numbers-Every-Programmer-Should-Know.png[Throughput and Cost for Compute Primitives -- the "Numbers Every Programmer Should Know"]

image::images/02-Cost-to-Host-and-Serve-1TB.png[Cost to Host and Serve One Billion 1kB Records (1 TB)]

The table at the right (REF) summarizes the 2013 values for Peter Norvig's http://norvig.com/21-days.html#answers["Numbers Every Programmer Should Know."]   -- the length of time for each computation primitive on modern hardware.  We've listed the figures several different ways: as latency (time to execute); as the number of 500-byte records that could be processed in an hour (TODO: day), if that operation were the performance bottleneck of your process; and as an amount of money to process one billion records of 500-byte each on commodity hardware.  Big Data requires high volume, high throughput computing, so our principle bound is the speed at which data can be read from and stored to disk.  What is remarkable is that with the current state of technology, most of the other operations are slammed to one limit or the other:  either bountifully unconstraining or devastatingly slow.  That lets us write down the following "rules for performance at scale:"

* High throughput programs cannot run faster than x (TODO:  Insert number)
* Data can be streamed to and from disk at x GB per hour (x records per hour, y records per hour, z dollars per billion records) (TODO:  insert numbers)
* High throughput programs cannot run faster than that but not run an order of magnitude slower.
* Data streams over the network at the same rate as disk.
* Memory access is infinitely fast.
* CPU is fast enough to not worry about except in the obvious cases where it is not.
* Random access (seeking to individual records) on disk is unacceptably slow.
* Network requests for data (anything involving a round trip) is infinitely slow.
* Disk capacity is free.
* CPU and network transfer costs are cheap.
* Memory is expensive and cruelly finite.  For most tasks, available memory is either all of your concern or none of your concern.

Now that you know how Hadoop moves data around, you can use these rules to explain its remarkable scalability.

1. Mapper streams data from disk and spills it back to disk; cannot go faster than that.
2. In between, your code processes the data
3. If your unwinding proteins or multiplying matrices are otherwise CPU or memory bound, Hadoop at least will not get in your way; the typical Hadoop job can process records as fast as they are streamed.
4. Spilled records are sent over the network and spilled back to disk; again, cannot go faster than that.


That leaves the big cost of most Hadoop jobs: the midstream merge-sort. Spilled blocks are merged in several passes (at the Reducer and sometimes at the Mapper) as follows. Hadoop begins streaming data from each of the spills in parallel.  Under the covers, what this means is that the OS is handing off the contents of each spill as blocks of memory in sequence.  It is able to bring all its cleverness to bear, scheduling disk access to keep the streams continually fed as rapidly as each is consumed.

Hadoop's actions are fairly straightforward.  Since the spills are each individually sorted, at every moment the next (lowest ordered) record to emit is guaranteed to be the next unread record from one of its streams.  It continues in this way, eventually merging each of its inputs into an unbroken output stream to disk.  At no point does the Hadoop framework require a significant number of seeks on disk or requests over the network; the memory requirements (the number of parallel streams times the buffer size per stream) are manageable; and the CPU burden is effectively nil, so the merge/sort as well runs at the speed of streaming to disk.

=== Hadoop Output phase may be more expensive than you think

As your Reducers emit records, they are streamed directly to the job output, typically the HDFS or S3.  Since this occurs in parallel with reading and processing the data, the primary spill to the Datanode typically carries minimal added overhead.  However, the data is simultaneously being replicated as well, which can extend your job's runtime by more than you might think.

TODO-qem -- a) does this section belong in "06a jsut enough performance for now" or here? b) I think this is a really good point to hit so want it to be really clear; apply extra criticism here.

Let's consider how data flows in a job intended to remove duplicate records: for example, processing 100 GB of data with one-percent duplicates, and writing output with replication factor three. As you'll see when we describe the 'distinct' patterns in Chapter 5 (REF), the Reducer input is about the same size as the mapper input. Using what you now know, Hadoop moves roughly the following amount of data, largely in parallel:

* 100 GB of Mapper input read from disk;
* 100 GB spilled back to disk;
* 100 GB of Reducer input sent and received over the network;
* 100 GB of Reducer input spilled to disk
* some amount of data merge/sorted to disk if your cluster size requires multiple passes;
* 100 GB of Reducer output written to disk by the local Datanode;
* 200 GB of replicated output sent over the network, received over the network and written to disk by the Datanode.

If your Datanode is backed by remote volumes (common in some virtual environments footnote:[This may sound outrageous to traditional IT folk, but the advantages of elasticity are extremely powerful -- we'll outline the case for virtualized Hadoop in Chapter (REF)]), you'll additionally incur

* 300 GB sent over the network to the remote file store

As you can see, unless your cluster is undersized (producing significant merge/sort overhead), the cost of replicating the data rivals the cost of the rest of the job.  The default replication factor is 3 for two very good reasons:  it helps guarantee the permanence of your data and it allows the Job tracker to efficiently allocate Mapper-local tasks.  But in certain cases -- intermediate checkpoint data, scratch data or where backed by a remote file system with its own durability guarantee -- an expert who appreciates the risk can choose to reduce the replication factor to 2 or 1.



=== Humans are important, robots are cheap. ===

To store 10 Billion records with an average size of 1 kB -- that's 10 TB -- it costs

* $200,000 /month to store it all on ram       ($1315/mo for 150 68.4 GB machines)
* $ 20,000 /month to have it 10% backed by ram ($1315/mo for  15 68.4 GB machines)
* $  1,000 /month to store it on disk (EBS volumes)

Compare witih

* $  1,600 /month salary of a part-time intern
* $  5,500 /month salary of a full-time junior engineer 
* $ 10,000 /month salary of a full-time senior engineer 

For a 10-hour working day, 

* $ 270 /day  for a 30-machine cluster having a total of 1TB ram, 120 cores
* $ 650 /day  for that same cluster if it runs for the full 24-hour day
* $  64 /day  for a 10-machine cluster having a total of 150 GB ram, 40 cores
* $ 180 /day  for an intern         to operate it
* $ 300 /day  for a junior engineer to operate it
* $ 600 /day  for a senior engineer to operate it


Suppose you have a job that runs daily, taking 3 hours on a 10-machine cluster of 15 GB machines. That job costs you $600/month.

If you tasked a junior engineer to spend three days optimizing that job, with a 10-machine cluster running the whole time, it would cost you about $1100. If she made the job run three times faster -- so it ran in 1 hour instead of 3 -- the job would now cost about $200. However, it will take three months to reach break-even.

As a rule of thumb, 

    Size your cluster so that it is either almost-always-idle or healthily exceeds the opportunity cost to the humans working on it.

Takeaways:

* Engineers are more expensive than compute. 
* Use elastic clusters for your data science team
