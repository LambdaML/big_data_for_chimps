== Analytic Patterns: Joining Tables

In database terminology, a _join_ combines the rows of two or more tables based on some matching information, known as a _key_.  For example, you could join a table of names and a table of mailing addresses, so long as both tables had a common field for the user ID.  You could also join a table of prices to a table of items, given an item ID column in both tables.  Joins are useful because they permit people to _normalize_ data -- that is to say, eliminate redundant content between multiple tables -- yet still bring several tables' content to a single view on-the-fly.

Joins are pedestrian fare in relational databases.  Far less so for Hadoop, since MapReduce wasn't really created with joins in mind, and you have to go through acrobatics to make it work.
footnote:[Hence why you may see Hadoop joins on data scientist tech interviews.]
Pig's `JOIN` operator provides the syntactical ease of a SQL query.  While Pig will shield you from hand-writing joins in MapReduce, it's still all MapReduce behind the scenes, so your joins are still subject to certain performance considerations.  This section will dig into the basics of Pig joins, then explain how to avoid certain mishaps.

=== Matching Records Between Tables (Inner Join)

An _inner join_ drops records that don't have matching keys in both tables.  This means that the result of an inner join may have fewer rows than either of the original tables.

==== Joining Records in a Table with Directly Matching Records from Another Table (Direct Inner Join)

There is a stereotypical picture in baseball of a "slugger": a big fat man who comes to plate challenging your notion of what an athlete looks like, and challenging the pitcher to prevent him from knocking the ball for multiple bases (or at least far enough away to lumber up to first base). To examine the correspondence from body type to ability to hit for power (i.e. high SLG), we will need to join the `people` table (listing height and weight) with their hitting stats.

[source,sql]
.Preparing the Relations to Join (ch_06/fat_hits.pig)
------
fatness = FOREACH people GENERATE
    player_id, name_first, name_last,
    height_in, weight_lb;

slugging_stats = FOREACH (FILTER bat_careers BY (PA > 1000))
    GENERATE 
        player_id, 
        SLG;
------

The syntax of the join statement itself shouldn't be much of a surprise:

[source,sql]
.An Inner Join (ch_06/fat_hits.pig)
------
slugging_fatness_join = JOIN
    fatness        BY player_id,
    slugging_stats BY player_id;

just_20 = LIMIT slugging_fatness_join 20; DUMP @;

DESCRIBE just_20

/*
{
    fatness::player_id: chararray,
    fatness::name_first: chararray,
    fatness::name_last: chararray,
    fatness::height_in: int,
    fatness::weight_lb: int,
    slugging_stats::player_id: chararray,
    slugging_stats::SLG: double
}
*/
------

===== Disambiguating Field Names With `::`

As a consequence of flattening records from the fatness table next to records from the slugging_stats table, the two tables each contribute a field named `player_id`. Although _we_ privately know that both fields have the same value, Pig is right to insist on an unambiguous reference. The schema helpfully prefixes the field names with a slug, separated by `::`, to make it unambiguous.

You'll need to run a `FOREACH` across the joined data, specifying the qualified names of the fields you want to keep. One thing to keep in mind is that it is easy to get confused as to whether you should reference a field via `x::y` or `x.y`. Try to remember: `x::y` is used to disambiguate joined records, and `x.y` is used to reference values in bags when calling aggregate functions.

===== Body Type vs Slugging Average

So having done the join, we finish by preparing the output:

------
bmis = FOREACH (JOIN fatness BY player_id, slugging_stats BY player_id) {

    BMI = 703.0*weight_lb/(double)(height_in*height_in);

    GENERATE 
        fatness::player_id, 
        name_first, 
        name_last,
        SLG, 
        height_in, 
        weight_lb, 
        BMI;
};
------

We added a field for BMI (Body Mass Index), a simple measure of body type. It is found by diving a person's weight by their height squared, and, since we're stuck with english units, multiplying by 703 to convert to metric.

Though BMI can't distinguish between 180 pounds of muscle and 180 pounds of flab, it reasonably controls for weight-due-to-tallness vs weight-due-to-bulkiness: beanpole Randy Johnson (6'10"/2.1m, 225lb/102kg) and pocket rocket Tim Raines (5'8"/1.7m, 160lb/73kb) both have a low BMI of 23; Babe Ruth (who in his later days was 6'2"/1.88m 260lb/118kb) and Cecil Fielder (of whom Bill James wrote "...his reported weight of 261 leaves unanswered the question of what he might weigh if he put his other foot on the scale") both have high BMIs well above 30 footnote:[The dataset we're using unfortunately only records players' weights at the start of their career, so you will see different values listed for Mr. Fielder and Mr. Ruth.]

=== How a Join Works

So that you can effectively reason about the behavior of a JOIN, it's important that you have the following two-and-a-half ways to think about its operation: (a) as the equivalent of a COGROUP-and-FLATTEN; and (b) as the underlying map-reduce job it produces.

==== A Join is a COGROUP+FLATTEN

Applying the `COGROUP` operation with a `FLATTEN` in place of the JOIN gives us the equivalent command:

------
-- Original JOIN
slugging_fatness_join = JOIN fatness BY player_id, slugging_stats BY player_id;

-- Equivalent COGROUP/FLATTEN
slugging_fatness_join = FOREACH 
	(COGROUP fatness BY player_id, slugging_stats BY player_id)
	GENERATE 
		FLATTEN(fatness), 
		FLATTEN(slugging_stats);
		
DESCRIBE slugging_fatness_join;

slugging_fatness_join: {
	fatness::player_id: chararray,
	fatness::name_first: chararray,
	fatness::name_last: chararray,
	fatness::height_in: int,
	fatness::weight_lb: int,
	slugging_stats::player_id: chararray,
	slugging_stats::SLG: double
}
------

In this sense, a `JOIN` is just a convenience - shorthand for a `COGROUP`/`FLATTEN`.

==== A Join is a Map/Reduce Job with a secondary sort on the Table Name

The way to perform a join in map-reduce is similarly a particular application of the `COGROUP` we stepped through above. Even still, we'll walk through it mostly on its own. The mapper receives its set of input splits either from the bat_careers table or from the people table and makes the appropriate transformations. Just as above, the mapper knows which file it is receiving via either framework metadata or environment variable in Hadoop Streaming. The records it emits follow the `COGROUP` pattern: the join fields, anointed as the partition fields; then the index labeling the origin file, anointed as the secondary sort fields; then the remainder of the fields. So far this is just a transform (`FOREACH`) inlined into a `COGROUP`.

------
mapper do
  self.processes_models
  config.partition_fields 1 # player_id
  config.sort_fields      2 # player_id, origin_key
RECORD_ORIGINS = [
  /bat_careers/ => ['A', Baseball::BatCareer],
  /players/     => ['B', Baseball::Player],
]
def set_record_origin!
  RECORD_ORIGINS.each do |origin_name_re, (origin_index, record_klass)|
    if config[:input_file]
      [@origin_key, @record_klass] = [origin_index, record_klass]
      return
    end
  end
  # no match, fail
  raise RuntimeError, "The input file name #{config[:input_file]} must match one of #{RECORD_ORIGINS.keys} so we can recognize how to handle it."
end
def start(*) set_record_origin! ; end
def recordize(vals) @record_klass.receive(vals)
def process(record)
  case record
  when CareerStats
    yield [rec.player_id, @origin_idx, rec.slg]
  when Player
    yield [rec.player_id, @origin_key, rec.height_in, rec.weight_lb]
  else raise "Impossible record type #{rec.class}"
  end
end
end
------

For each key, the reducer spools all the records matching that key from the initial table into an array in memory. The lastmost-named table, however, does not need to be accumulated. As the framework streams in each record in the current group, you simply compose it with each record in the accumulated array

.Join in Wukong: Reducer Part.
------
reducer do
  def gather_records(group, origin_key)
    records = []
    group.each do |*vals|
      if vals[1] != origin_key # We hit start of next table's keys
        group.shift(vals)      # put it back before Mom notices
        break                  # and stop gathering records
      end
      records << vals
    end
    return records
  end

  BMI_ENGLISH_TO_METRIC = 0.453592 / (0.0254 * 0.254)
  def bmi(ht, wt)
    BMI_ENGLISH_TO_METRIC * wt / (ht * ht)
  end

  def process_group(group)
    # remainder are slugging stats
    group.each do |player_id, _, slg|
      players = gather_records(group, 'A')
      players.each do |player_id, _, height_in, weight_lb|
        #   The result of the JOIN in Pig would be all the fields, keys and not, in order by origin table
	# after_the_join = [
	#   player_id, slg,                  # fields from 'A'
	#   player_id, height_in, weight_lb  # fields from 'B'
	# ]

        #   But Pig then pipelines the post-join FOREACH into the reducer, and so do we:
        yield [player_id, slg, height_in, weight_lb, bmi(height_in, weight_lb)]
      end
    end
  end
end
------

// TODO-qem should I show the version that has just the naked join-like output ie. the 'after_the_join' variable? And if so, do I show it as well or instead? --> tough to picture it in my head, but I vote to show both, such that people can see the flow.

The output of the Join job will have one record for each discrete combination of A and B. As you will notice in our Wukong version of the Join, the secondary sort ensures that for each key the reducer receives all the records for table A strictly followed by all records for table B. We gather all the A records in to an array, then on each B record emit the A records stapled to the B records. All the A records have to be held in memory at the same time, while all the B records simply flutter by; this means that if you have two datasets of wildly different sizes or distribution, it is worth ensuring the Reducer receives the smaller group first. In map/reduce, the table with the largest number of records per key should be assigned the last-occurring field group label; in Pig, that table should be named last in the `JOIN` statement.

------
stats_and_fatness = FOREACH (JOIN fatness BY player_id, stats BY player_id)
  GENERATE fatness::player_id..BMI, stats::n_seasons..OPS;
------

// The output of the Join job has one line for each discrete combination of A and B. As you will notice in our Wukong version of the Join, the job receives all the A records for a given key in order, strictly followed by all the B records for that key in order. We have to accumulate all the A records in memory so we know what rows to emit for each B record. All the A records have to be held in memory at the same time, while all the B records simply flutter by; this means that if you have two datasets of wildly different sizes or distribution, it is worth ensuring the Reducer receives the smaller group first. In Wukong, you do this by giving it an earlier-occurring field group label; in Pig, always put the table with the largest number of records per key last in the statement.


// 
// TODO: a JOIN is used for: direct foreign key join; matching records on a criterion, possibly sparsely; set intersection.
// 
// The core operation you will use to put records from one table into context with data from another table is the JOIN. A common application of the JOIN is to reunite data that has been normalized -- that is to say, where the database tables are organized to eliminate any redundancy. For example, each Retrosheet game log lists the ballpark in which it was played but, of course, it does not repeat the full information about that park within every record. Later in the book, (TODO:  REF) we will want to label each game with its geo-coordinates so we can augment each with official weather data measurements.
// 
// To join the game_logs table with the parks table, extracting the game time and park geocoordinates, run the following Pig command:
// 
// ------
// gls_with_parks_j = JOIN
//    parks     BY (park_id),
//    game_logs BY (park_id);
// explain gls_with_parks_j;
// gls_with_parks = FOREACH gls_with_parks_j GENERATE
//  (game_id, gamelogs.park_id, game_time, park_lng, statium_lat);
// explain gls_with_parks;
// (TODO output of explain command)
// ------
// 
// The output schema of the new `gls_with_parks` table has all the fields from the `parks` table first (because it's first in the join statement), stapled to all the fields from the `game_logs` table. We only want some of the fields, so immediately following the JOIN is a FOREACH to extract what we're interested in. Note there are now two 'park_id' columns, one from each dataset, so in the subsequent FOREACH, we need to dereference the column name with the table from which it came. (TODO: check that Pig does push the projection of fields up above the JOIN). If you run the script, 'examples/geo/baseball_weather/geolocate_games.pig' you will see that its output has example as many records as there are 'game_logs' because there is exactly one entry in the 'parks' table for each park.
// 
// In the general case, though, a JOIN can be many to many. Suppose we wanted to build a table listing all the home ballparks for every player over their career. The 'player_seasons' table has a row for each year and team over their career. If a player changed teams mid year, there will be two rows for that player. The 'park_years' table, meanwhile, has rows by season for every team and year it was used as a home stadium. Some ballparks have served as home for multiple teams within a season and in other cases (construction or special circumstances), teams have had multiple home ballparks within a season.
// 
// The Pig script (TODO: write script) includes the following JOIN:
// 
// ------
// JOIN
// player_park_years=JOIN
//  parks(year,team_ID),
//  players(year,team_ID);
// explain_player_park_year;
// ------
// 
// First notice that the JOIN expression has multiple columns in this case separated by commas; you can actually enter complex expressions here -- almost all (but not all) the things you do within a FOREACH. If you examine the output file (TODO: name of output file), you will notice it has appreciably more lines than the input 'player' file. For example (TODO: find an example of a player with multiple teams having multiple parks), in year x player x played for the x and the y and y played in stadiums p and q. The one line in the 'players' table has turned into three lines in the 'players_parks_years' table.
// 
// The examples we have given so far are joining on hard IDs within closely-related datasets, so every row was guaranteed to have a match. It is frequently the case, however, you will join tables having records in one or both tables that will fail to find a match. The 'parks_info' datasets from Retrosheet only lists the city name of each ballpark, not its location. In this case we found a separate human-curated list of ballpark geolocations, but geolocating records -- that is, using a human-readable location name such as "Austin, Texas" to find its nominal geocoordinates (-97.7,30.2) -- is a common task; it is also far more difficult than it has any right to be, but a useful first step is match the location names directly against a gazette of populated place names such as the open source Geonames dataset.
// 
// Run the script (TODO: name of script) that includes the following JOIN:
// 
// ------
// park_places = JOIN
//  parks BY (location) LEFT OUTER,
//  places BY (concatenate(city, ", ", state);
// DESCRIBE park_places;
// ------
// 
// In this example, there will be some parks that have no direct match to location names and, of course, there will be many, many places that do not match a park. The first two JOINs we did were "inner" JOINs -- the output contains only rows that found a match. In this case, we want to keep all the parks, even if no places matched but we do not want to keep any places that lack a park. Since all rows from the left (first most dataset) will be retained, this is called a "left outer" JOIN. If, instead, we were trying to annotate all places with such parks as could be matched -- producing exactly one output row per place -- we would use a "right outer" JOIN instead. If we wanted to do the latter but (somewhat inefficiently) flag parks that failed to find a match, you would use a "full outer" JOIN. (Full JOINs are pretty rare.)
// 
// TODO: discuss use of left join for set intersection.
// 
// In a Pig JOIN it is important to order the tables by size -- putting the smallest table first and the largest table last. (You'll learn why in the "Map/Reduce Patterns" (TODO:  REF) chapter.) So while a right join is not terribly common in traditional SQL, it's quite valuable in Pig. If you look back at the previous examples, you will see we took care to always put the smaller table first. For small tables or tables of similar size, it is not a big deal -- but in some cases, it can have a huge impact, so get in the habit of always following this best practice.
// 
// ------
// NOTE
// A Pig join is outwardly similar to the join portion of a SQL SELECT statement, but notice that  although you can place simple expressions in the join expression, you can make no further manipulations to the data whatsoever in that statement. Pig's design philosophy is that each statement corresponds to a specific data transformation, making it very easy to reason about how the script will run; this makes the typical Pig script more long-winded than corresponding SQL statements but clearer for both human and robot to understand.
// ------
// 
// ==== Reassemble a Vertically Partitioned Table
// 
// Another reason to split data across tables is 'vertical partitioning': storing fields that are very large or seldom used in context within different tables. That's the case with the Wikipedia article tables -- the geolocation information is only relevant for geodata analysis; the article text is both large and not always relevant.
// 
// 
// 
// Every stadium a player has played in. (We're going to cheat on the detail of
// multiple stints and credit every player with all stadiums visited by the team
// of his first stint in a season
// 
// ------
//   -- there are only a few many-to-many cases, so the 89583 seasons in batting
//   -- table expands to only 91904 player-park-years. But it's a cross product, so
//   -- beware.
// SELECT COUNT(*) FROM batting bat WHERE bat.stint = 1;
// SELECT bat.player_id, bat.team_id, bat.year_id, pty.park_id
//   FROM       batting bat
//   INNER JOIN park_team_years pty
//     ON bat.year_id = pty.year_id AND bat.team_id = pty.team_id
//   WHERE bat.stint = 1
//   ORDER BY player_id
//   ;
// ------
// 
// What if you only want the distinct player-team-years?
// You might naively do a join and then a group by,
// or a join and then distinct. Don't do that.
// 
// ------
//   -- DON'T DO THE (pig equivalent) OF THIS to find the distinct teams, years and parks;
//   -- it's an extra reduce.
// SELECT bat.player_id, bat.nameCommon,
//     GROUP_CONCAT(DISTINCT pty.park_id) AS park_ids, COUNT(DISTINCT pty.park_id) AS n_parks,
//     GROUP_CONCAT(DISTINCT bat.team_id) AS team_ids,
//     MIN(bat.year_id) AS begYear, MAX(bat.year_id) AS endYear
//   FROM       bat_war bat
//   INNER JOIN park_team_years pty
//     ON bat.year_id = pty.year_id AND bat.team_id = pty.team_id
//   WHERE bat.stint = 1 AND player_id IS NOT NULL
//   GROUP BY player_id
//   HAVING begYear > 1900
//   ORDER BY n_parks DESC, player_id ASC
//   ;
// 
//   Join bat_yr on (team_id, year_id), pty by (team_id, year_id);
//   FOREACH @ GENERATE bat_years::player_id, park_id;
//   Group by player_id
//   Distinct parks
// 
//   Cogroup baty by (team_id, year_id), pty by (team_id, year_id);
//    distinct park_id,
// ------
// 
// So now we disclose the most important thing that SQL experts need to break
// their brains of:
// 
// In SQL, the JOIN is supreme.
// In Pig, the GROUP is supreme
// 
// A JOIN is, for the most part, just sugar around a COGROUP-and-FLATTEN.
// Very often you'll find the simplest path is through COGROUP not JOIN.
// 
// In this case, if you start by thinking of the group, you'll see you can eliminate a whole reduce.
// 
// (show pig, including a DISTINCT in the fancy-style FOREACH)
// 
// ==== Join Practicalities
// 
// (add note) Joins on null values are dropped even when both are null. Filter nulls. (I can't come up with a good example of this)
// (add note) in contrast, all elements with null in a group _will_ be grouped as null. This can be dangerous when large number of nulls: all go to same reducer


===== Pattern in Use

* _Exercise_ -- Explore the correspondence of weight, height and BMI to SLG using a medium-data tool such as R, Pandas or Excel. Spoiler alert: the stereotypes of the big fat slugger is quite true.

==== Handling Nulls and Non-matches in Joins and Groups

It's important to understand how Null keys are handled in Join and Group operations. Briefly:

* In map-reduce, Nulls are respected as keys:
* In a single-table Pig `GROUP`, Nulls are also respected as keys.
* In a multi-table `COGROUP`, Nulls are respected as keys, _but not grouped together_
* In a `JOIN` operation, rows with Nulls _do not take place in the join_ at all, but are _processed anyway_
* If you have a lot of Null keys, watch out: it is somewhere between costly and foolish.

When we say 'null key', we mean that if the group or join key is a scalar expression, that it has a null result; and if the key is a tuple, that all elements of the tuple are null. So

* these are null keys: `Null`, `(Null,Null,Null)`, `("hi",Null,"howareyou")` (even one non-null field)
* these are not: `""` (empty string), `0` (zero); An empty bag `{}` and a bag with a tuple holding null `{()}` are both not-null, but a bag cannot be used as a join or group key.

In the base Hadoop infrastructure, there's not much to understand: a key is a key, and Hadoop doesn't treat nulls specially in any way. Anything different is up to your program, and Pig does in fact supply something different.

A single-table `GROUP` statement does treat Nulls as keys. It's pretty easy to come up with a table having many Null values for the key you're grouping on; and if you do, all of them will be sent to the same reducer. If you actually need those keys, well, whaddayagonnado: sounds like one of the reducers will have to endure a bad day at work. But if you don't need the groups having Null keys, get rid of them as early as possible.

A `COGROUP` statement with multiple tables also treats Nulls as keys (so get rid of them if unwanted). But take note! Multi-table groups treat _each table's Nulls as distinct_. That is, if table A had 4 records with null keys, and table B had 2 records with null keys, `COGROUP A by key, B by key` would produce

* a row whose three fields are the null key; a bag holding the four associated records from A, and an empty bag; and
* a row whose three fields are the null key; an empty bag; and a bag holding the two associated records from B.

What do you do if you want null keys treated like any other tuple? Add an indicator field saying whether the value is null, and coalesce the actual key to non-null value. So instead of `JOIN aa BY has_nulls, bb BY has_nulls`, write

[source,sql]
.Join on NULL Fields
------
JOIN
  aa BY ( (has_nulls IS NULL ? 'x' : 'Y'), (has_nulls IS NULL ? -999 : has_nulls) ),
  bb BY ( (has_nulls IS NULL ? 'x' : 'Y'), (has_nulls IS NULL ? -999 : has_nulls) );
------

Even if there are records whose value is -999, they will have `'Y'` for the indicator, while the null-keyed records will have `'x'`, and so they will not meet up. (For your sanity, if it's possible to choose a replacement value that can't occur in the data set do so). The file `j-important_notes_about_joins.pig` in the sample code repo has a bunch more demonstrations of edge cases in groups and joins.


===== Pattern in Use: Inner Join

* _Where You'll Use It_  -- Any time you need to match records among tables. Re-attaching metadata about a record to the record. Combining incidences of defective products with the manufacturing devices that made them. 
* _Standard Snippet_	 -- `JOIN aa BY key, bb BY key;`
* _Hello, SQL Users_     -- The only join that Hadoop admits is the "equi-join" -- equality of values. Much more on this to follow.
* _Important to Know_
  - List the tables in the statement from smallest to largest (largest table last)
  - You can do a multi-way join; see the documentation
  - The key does not appear in the output
  - `::` is for disambiguation, `.` is for projecting tuples in a bag. `JOIN` doesn't create new bags, so `::` is probably what you want.
* _Output Count_	 -- For each key that matches, the number of pairings among keys. This can be anywhere from much smaller to explosively bigger.
* _Records_		 -- Schema of the result is the schema from each table stapled end-to-end. Values are unchanged from their input.
* _Data Flow_		 -- Pipelinable: it's composed onto the end of the preceding map or reduce, and if it stands alone becomes a map-only job.
* _See Also_
  - DataFu's bag left outer join;
  - Left outer join on three tables: http://datafu.incubator.apache.org/docs/datafu/guide/more-tips-and-tricks.html
  - Time-series chapter: Range query using cross
  - Time-series chapter: Range query using prefix and UDFs (the ip-to-geo example)
  - Time-series chapter: Self-join for successive row differences
  - Advanced Pig: Sparse joins for filtering, with a HashMap (replicated)
  - The internet, for information on Bitmap index or Bloom filter joins

=== Enumerating a Many-to-Many Relationship

In the previous examples there's been a direct pairing of each line in the main table with the unique line from the other table that decorates it. Therefore, there output had exactly the same number of rows as the larger input table. When there are multiple records per key, however, the the output will have one row for each _pairing_ of records from each table. A key with two records from the left table and 3 records from the right table yields six output records.

Using the `GROUP ALL` trick we learned last chapter, we can count the total records before and after a many-to-many JOIN:

[source,sql]
.Many-to-Many Join (ch_06/many_to_many.pig)
------
-- Count the number of bat_seasons records
total_bat_seasons = FOREACH (GROUP bat_seasons ALL) GENERATE 
    'bat_seasons' AS label,
    COUNT_STAR(bat_seasons) AS total;

-- Count the number of park_team_years
total_park_team_years = FOREACH (GROUP park_team_years ALL) GENERATE
    'park_team_years' AS label,
    COUNT_STAR(park_team_years) AS total;

-- Always trim the fields we don't need
player_team_years = FOREACH bat_seasons GENERATE year_id, team_id, player_id;
park_team_years   = FOREACH park_team_years GENERATE year_id, team_id, park_id;

player_stadia = FOREACH (JOIN
    player_team_years BY (year_id, team_id),
    park_team_years   BY (year_id, team_id)
    ) GENERATE
        player_team_years::year_id AS year_id, 
        player_team_years::team_id AS team_id,
        player_id,
        park_id;
total_player_stadia = FOREACH (GROUP player_stadia ALL) GENERATE
    'player_stadium' AS label,
    COUNT_STAR(player_stadia) AS total;

-- Finally, UNION our label/totals and dump them together
answer = UNION total_bat_seasons, total_park_team_years, total_player_stadia; DUMP @;
------

Which results in:

----
(park_team_years,2911)
(bat_seasons,77939)
(player_stadio,80565)
----

You'll see that the 77939 batting_seasons became 80565 home stadium-player pairings. The cross-product behavior didn't cause a big explosion in counts -- as opposed to our next example, which will generate much more data.

=== Joining a Table with Itself (self-join)

Joining a table with itself is very common when you are analyzing relationships of elements within the table (when analyzing graphs or working with datasets represented as attribute-value lists it becomes predominant.) Our example here will be to identify all teammates pairs: players listed as having played for the same team in the same year. The only annoying part about doing a self-join in Pig is that you can't, at least not directly. Pig won't let you list the same table in multiple slots of a JOIN statement, and also won't let you just write something like `"mytable_dup = mytable;"` to assign a new alias footnote:[If it didn't cause such a surprisingly hairy set of internal complications, it would have long ago been fixed]. Instead you have to use a FOREACH or somesuch to create a duplicate representative. If you don't have any other excuse, use a project-star expression: `p2 = FOREACH p1 GENERATE *;`. In this case, we already need to do a projection; we feel the most readable choice is to repeat the statement twice.

------
-- Pig disallows self-joins so this won't work:
wont_work = JOIN bat_seasons BY (team_id, year_id), bat_seasons BY (team_id, year_id);
"ERROR ... Pig does not accept same alias as input for JOIN operation : bat_seasons"
------

That's OK, we didn't want all those stupid fields anyway; we'll just make two copies and then join
the table copies to find all teammate pairs. We're going to say a player isn't their their own
teammate, and so we also reject the self-pairs.

------
p1 = FOREACH bat_seasons GENERATE player_id, team_id, year_id;
p2 = FOREACH bat_seasons GENERATE player_id, team_id, year_id;

teammate_pairs = FOREACH (JOIN
    p1 BY (team_id, year_id),
    p2 by (team_id, year_id)
  ) GENERATE
    p1::player_id AS pl1,
    p2::player_id AS pl2;
teammate_pairs = FILTER teammate_pairs BY NOT (pl1 == pl2);
------

As opposed to the slight many-to-many expansion of the previous section, there are on average ZZZ players per roster to be paired. The result set here is explosively larger: YYY pairings from the original XXX player seasons, an expansion of QQQ footnote:[See the example code for details]. Now you might have reasonably expected the expansion factor to be very close to the average number of players per team, thinking "QQQ average players per team, so QQQ times as many pairings as players." But a join creates as many rows as the product of the records in each tables' bag -- the square of the roster size in this case -- and the sum of the squares necessarily exceeds the direct sum.

The 78,000 player seasons we joined onto the team-parks-years table In
contrast, a similar `JOIN` expression turned 78,000 seasons into 2,292,658
player-player pairs, an expansion of nearly thirty times

(A simplification was made) footnote:[(or, what started as a footnote but should probably become a sidebar or section in the timeseries chapter -- QEM advice please) Our bat_seasons table ignores mid-season trades and only lists a single team the player played the most games for, so in infrequent cases this will identify some teammate pairs that didn't actually overlap. There's no simple option that lets you join on players' intervals of service on a team: joins must be based on testing key equality, and we would need an "overlaps" test. In the time-series chapter you'll meet tools for handling such cases, but it's a big jump in complexity for a small number of renegades. You'd be better off handling it by first listing every stint on a team for each player in a season, with separate fields for the year and for the start/end dates. Doing the self-join on the season (just as we have here) would then give you every _possible_ teammate pair, with some fraction of false pairings. Lastly, use a FILTER to reject the cases where they don't overlap. Any time you're looking at a situation where 5% of records are causing 150% of complexity, look to see whether this approach of "handle the regular case, then fix up the edge cases" can apply.]

// SELECT DISTINCT b1.player_id, b2.player_id
//   FROM bat_season b1, bat_season b2
//   WHERE b1.team_id = b2.team_id          -- same team
//     AND b1.year_id = b2.year_id          -- same season
//     AND b1.player_id != b2.player_id     -- reject self-teammates
//   GROUP BY b1.player_id
//   ;

=== Joining Records Without Discarding Non-Matches (Outer Join)

The Baseball Hall of Fame is meant to honor the very best in the game, and each year a very small number of players are added to its rolls. It's a significantly subjective indicator, which is its cardinal virtue and its cardinal flaw -- it represents the consensus judgement of experts, but colored to some small extent by emotion, nostalgia, and imperfect quantitative measures. But as you'll see over and over again, the best basis for decisions is the judgement of human experts backed by data-driven analysis. What we're assembling as we go along this tour of analytic patterns isn't a mathematical answer to who the highest performers are, it's a basis for centering discussion around the right mixture of objective measures based on evidence and human judgement where the data is imperfect.

So we'd like to augment the career stats table we assembled earlier with columns showing, for hall-of-famers, the year they were admitted, and a `Null` value for the rest. (This allows that column to also serve as a boolean indicator of whether the players were inducted). If you tried to use the JOIN operator in the form we have been, you'll find that it doesn't work. A plain JOIN operation keeps only rows that have a match in all tables, and so all of the non-hall-of-famers will be excluded from the result. (This differs from COGROUP, which retains rows even when some of its inputs lack a match for a key). The answer is to use an 'outer join'

------
career_stats = FOREACH (
  JOIN
    bat_careers BY player_id LEFT OUTER,
    batting_hof BY player_id) GENERATE
  bat_careers::player_id..bat_careers::OPS, allstars::year_id AS hof_year;
------

Since the batting_hof table has exactly one row per player, the output has exactly as many rows as the career stats table, and exactly as many non-null rows as the hall of fame table.

footnote:[Please note that the `batting_hof` table excludes players admitted to the Hall of Fame based on their pitching record. With the exception of Babe Ruth -- who would likely have made the Hall of Fame as a pitcher if he hadn't been the most dominant hitter of all time -- most pitchers have very poor offensive skills and so are relegated back with the rest of the crowd]

------
-- (sample data)
-- (Hank Aaron)... Year
------

In this example, there will be some parks that have no direct match to location names and, of course, there will be many, many places that do not match a park. The first two JOINs we did were "inner" JOINs -- the output contains only rows that found a match. In this case, we want to keep all the parks, even if no places matched but we do not want to keep any places that lack a park. Since all rows from the left (first most dataset) will be retained, this is called a "left outer" JOIN. If, instead, we were trying to annotate all places with such parks as could be matched -- producing exactly one output row per place -- we would use a "right outer" `JOIN` instead. If we wanted to do the latter but (somewhat inefficiently) flag parks that failed to find a match, you would use a "full outer" JOIN. (Full JOINs are pretty rare.)

In a Pig `JOIN` it is important to order the tables by size -- putting the smallest table first and the largest table last. (You'll learn why in the "Map/Reduce Patterns" (REF) chapter.) So while a right join is not terribly common in traditional SQL, it's quite valuable in Pig. If you look back at the previous examples, you will see we took care to always put the smaller table first. For small tables or tables of similar size, it is not a big deal -- but in some cases, it can have a huge impact, so get in the habit of always following this best practice.

NOTE: A Pig join is outwardly similar to the join portion of a SQL SELECT statement, but notice that  although you can place simple expressions in the join expression, you can make no further manipulations to the data whatsoever in that statement. Pig's design philosophy is that each statement corresponds to a specific data transformation, making it very easy to reason about how the script will run; this makes the typical Pig script more long-winded than corresponding SQL statements but clearer for both human and robot to understand.

===== Pattern in Use

* _Where You'll Use It_  -- Any time only some records have matches but you want to preserve the whole. All products from the manufacturing line paired with each incident report about a product (keeping products with no incident report). All customers that took a test drive matched with the past cars they bought from you (but not discarding the new customer records)
* _Standard Snippet_	 -- `FOREACH (JOIN aa BY key LEFT OUTER, bb BY key) GENERATE a::key..a::last_field,b::second_field...;`
* _Hello, SQL Users_     -- Right joins are much more common in Pig, because you want the table size to determine the order they're listed in
* _Important to Know_	 -- Records with NULL keys are dropped even in an outer join
* _Output Count_	 -- At least as many records as the `OUTER` table has, expanded by the number of ways to pair records from each table for a key. Like any join, output size can be explosively higher
* _Data Flow_		 -- Pipelinable: it's composed onto the end of the preceding map or reduce, and if it stands alone becomes a map-only job.


==== Joining Tables that do not have a Foreign-Key Relationship

All of the joins we've done so far have been on nice clean values designed in advance to match records among tables. In SQL parlance, the career_stats and batting_hof tables both had player_id as a primary key (a column of unique, non-null values tied to each record's identity). The team_id field in the bat_seasons and park_team_years tables points into the teams table as a foreign key: an indexable column whose only values are primary keys in another table, and which may have nulls or duplicates. But sometimes you must match records among tables that do not have a polished mapping of values. In that case, it can be useful to use an outer join as the first pass to unify what records you can before you bring out the brass knuckles or big guns for what remains.

Suppose we wanted to plot where each major-league player grew up -- perhaps as an answer in itself as a browsable map, or to allocate territories for talent scouts, or to see whether the quiet wide spaces of country living or the fast competition of growing up in the city better fosters the future career of a high performer. While the people table lists the city, state and country of birth for most players, we must geolocate those place names -- determine their longitude and latitude -- in order to plot or analyze them.

There are geolocation services on the web, but they are imperfect, rate-limited and costly for commercial use footnote:[Put another way, "Accurate, cheap, fast: choose any two]. Meanwhile the freely-available geonames database gives geo-coordinates and other information on more than seven million points of interest across the globe, so for informal work it can make a lot of sense to opportunistically decorate whatever records match and then decide what to do with the rest.

// TODO: rework this to instead attack the 'birth_ctry' and 'death_ctry' fields in the people table.
// On the other hand, that allows it to accurately describe Ed Porray's birthplace as "A Ship on Atlantic Ocean". The joys of community-generated data!

------
geolocated_somewhat = JOIN
  people BY (birth_city, birth_state, birth_country),
  places BY (city, admin_1, country_id)
------

In the important sense, this worked quite well: XXX% of records found a match.
(Question do we talk about the problems of multiple matches on name here, or do we quietly handle it?)

Experienced database hands might now suggest doing a join using some sort of fuzzy-match
or some sort of other fuzzy equality. However, in map-reduce the only kind of join you can do is an "equi-join" -- one that uses key equality to match records. Unless an operation is 'transitive' -- that is, unless `a joinsto b` and `b joinsto c` guarantees `a joinsto c`, a plain join won't work, which rules out approximate string matches; joins on range criteria (where keys are related through inequalities (x < y)); graph distance; geographic nearness; and edit distance. You also can't use a plain join on an 'OR' condition: "match stadiums and places if the placename and state are equal or the city and state are equal", "match records if the postal code from table A matches any of the component zip codes of place B". Much of the middle part of this book centers on what to do when there _is_ a clear way to group related records in context, but which is more complicated than key equality.

Exercise: are either city dwellers or country folk over-represented among major leaguers? Selecting only places with very high or very low population in the geonames table might serve as a sufficient measure of urban-ness; or you could use census data and the methods we cover in the geographic data analysis chapter to form a more nuanced indicator. The hard part will be to baseline the data for population: the question is how the urban vs rural proportion of ballplayers compares to the proportion of the general populace, but that distribution has changed dramatically over our period of interest. The US has seen a steady increase from a rural majority pre-1920 to a four-fifths majority of city dwellers today.

===== Pattern in Use

* _Where You'll Use It_  -- Any time you're geolocating records, sure, but the lessons here hold any time you're combining messy data with canonical records
* _Hello, SQL Users_     -- No fuzzy matches, no string distance, no inequalities. There's no built-in `SOUNDEX` UDF, but that would be legal as it produces a scalar value to test with equality
* _Important to Know_	 -- Watch out for an embarrassment of riches -- there are many towns named "Springfield".

==== Joining on an Integer Table to Fill Holes in a List

In some cases you want to ensure that there is an output row for each potential value of a key. For example, a histogram of career hits will show that Pete Rose (4256 hits) and Ty Cobb (4189 hits) have so many more hits than the third-most player (Hank Aaron, 3771 hits) there are gaps in the output bins.

To fill the gaps, generate a list of all the potential keys, then generate your (possibly hole-y) result table, and do a join of the keys list (LEFT OUTER) with results. In some cases, this requires one job to enumerate the keys and a separate job to calculate the results. For our purposes here, we can simply use the integer table. (We told you it was surprisingly useful!)

If we prepare a histogram of career hits, similar to the one above for seasons, you'll find that Pete Rose (4256 hits) and Ty Cobb (4189 hits) have so many more hits than the third-most player (Hank Aaron, 3771 hits) there are gaps in the output bins. To make it so that every bin has an entry, do an outer join on the integer table. (See, we told you the integers table was surprisingly useful.)

------
-- SQL Equivalent:
SET @H_binsize = 10;
SELECT bin, H, IFNULL(n_H,0)
  FROM      (SELECT @H_binsize * idx AS bin FROM numbers WHERE idx <= 430) nums
  LEFT JOIN (SELECT @H_binsize*CEIL(H/@H_binsize) AS H, COUNT(*) AS n_H
    FROM bat_career bat GROUP BY H) hist
  ON hist.H = nums.bin
  ORDER BY bin DESC
;
------

Regular old histogram of career hits, bin size 100

------
H_vals = FOREACH (GROUP bat_seasons BY player_id) GENERATE
  100*ROUND(SUM(bat_seasons.H)/100.0) AS bin;
H_hist_0 = FOREACH (GROUP H_vals BY bin) GENERATE
  group AS bin, COUNT_STAR(H_vals) AS ct;
------

Generate a list of all the bins we want to keep, then perform a LEFT `JOIN` of bins with histogram
counts. Missing rows will have a null `ct` value, which we can convert to zero.

------
H_bins = FOREACH (FILTER numbers_10k BY num0 <= 43) GENERATE 100*num0  AS bin;

H_hist = FOREACH (JOIN H_bins BY bin LEFT OUTER, H_hist_0 BY bin) GENERATE
  H_bins::bin,
  ct,                    -- leaves missing values as null
  (ct IS NULL ? 0 : ct)  -- converts missing values to zero
;
------


===== Pattern in Use

* _Where You'll Use It_  -- Whenever you know the values you want (whether they're integers, model numbers, dates, etc) and always want a corresponding row in the output table


=== Selecting Only Records That Lack a Match in Another Table (anti-join)

A common use of a `JOIN` is to perform an effective filter on a large number of values -- the big brother of the pattern in section (REF). In this case (known as an 'anti-join'), we don't want to keep the selection table around afterwards

------
-- Project just the fields we need
allstars_p  = FOREACH allstars GENERATE player_id, year_id;

-- An outer join of the two will leave both matches and non-matches.
scrub_seasons_jn = JOIN
  bat_seasons BY (player_id, year_id) LEFT OUTER,
  allstars_p  BY (player_id, year_id);

-- ...and the non-matches will have Nulls in all the allstars slots
scrub_seasons_jn_f = FILTER scrub_seasons_jn
  BY allstars_p::player_id IS NULL;
------

Once the matches have been eliminated, pick off the first table's fields. The double-colon in 'bat_seasons::' makes clear which table's field we mean. The fieldname-ellipsis 'bat_seasons::player_id..bat_seasons::RBI' selects all the fields in bat_seasons from player_id to RBI, which is to say all of them.

------
scrub_seasons_jn   = FOREACH scrub_seasons_jn_f
  GENERATE bat_seasons::player_id..bat_seasons::RBI;
------

// This is a good use of the fieldname-ellipsis syntax: to the reader it says "all fields of bat_seasons, the exact members of which are of no concern". (It would be even better if we could write `bat_seasons::*`, but that's not supported in Pig <= 0.12.0.) In a context where we did go on to care about the actual fields, that syntax becomes an unstated assumption about not just what fields exist at this stage, but what _order_ they occur in. We can try to justify why you wouldn't use it with a sad story: Suppose you wrote `bat_seasons::PA..bat_seasons::HR` to mean the counting stats (PA, AB, HBP, SH, BB, H, h1B, h2B, h3b, HR). In that case, an upstream rearrangement of the schema could cause fields to be added or removed in a way that would be hard to identify. Now, that failure scenario almost certainly won't happen, and if it did it probably wouldn't lead to real problems, and if there were they most likely wouldn't be that hard to track down. The true point is that it's lazy and unhelpful to the reader. If you mean "PA, AB, HBP, SH, BB, H, h1B, h2B, h3b, HR", then that's what you should say.

=== Selecting Only Records That Posess a Match in Another Table (semi-join)

A semi-join is the counterpart to  an anti-join: you want to find records that _do_ have a match in another table, but not keep the fields from that table around.

Let's use the same example -- player seasons where they made the all-star team -- but only look for seasons that _were_ all-stars.
You might think you could do this with a join:

------
-- Don't do this... produces duplicates!
bats_g    = JOIN allstar BY (player_id, year_id), bats BY (player_id, year_id);
badness   = FOREACH bats_g GENERATE bats::player_id .. bats::HR;
------

The result is wrong, and even a diligent spot-check will probably fail to notice. You see, from 1959-1962 there were multiple All-Star games (!), and so players who appeared in both have two rows in the All-Star table. In turn, each singular row in the `bat_season` table became two rows in the result for players in those years. We've broken the contract of leaving the original table unchanged.

This is the biggest thing people coming from a SQL background need to change about their thinking. In SQL, the JOIN rules all. In Pig, GROUP and COGROUP rule the land, and nearly every other structural operation is some piece of syntactic sugar on top of those. So when going gets rough with a JOIN, remember that it's just a convenience and ask yourself whether a COGROUP would work better. In this case it does:

------
-- Players with no entry in the allstars_p table have an empty allstars_p bag
allstar_seasons_cg = COGROUP
  bat_seasons BY (player_id, year_id),
  allstars_p  BY (player_id, year_id);
------

Now select all cogrouped rows where there was an all-star record, and project just the records from the original table.

------
-- One row in the batting table => One row in the result
allstar_seasons = FOREACH 
  (FILTER allstar_seasons_cg BY (COUNT_STAR(allstars_p) > 0L))
  GENERATE FLATTEN(bat_seasons);
------

The `JOIN` version was equivent to flattening both bags (`GENERATE FLATTEN(bat_seasons), FLATTEN(allstars_p)`) and then removing the fields we had just flattened. In the `COGROUP` version, neither the incorrect duplicate rows nor the unnecessary columns are created.


==== An Alternative to Anti-Join: use a COGROUP

As a lesson on the virtues of JOINs and COGROUPs, let's examine an alternate version of the anti-join introduced above (REF).

------
-- Players with no entry in the allstars_p table have an empty allstars_p bag
bats_ast_cg = COGROUP
  bat_seasons BY (player_id, year_id),
  allstars_p BY (player_id, year_id);
------

Select all cogrouped rows where there were no all-star records, and project the batting table fields.

------
scrub_seasons_cg = FOREACH
  (FILTER bats_ast_cg BY (COUNT_STAR(allstars_p) == 0L))
  GENERATE FLATTEN(bat_seasons);
------

// IMPROVEME: take another look to see whether the JOIN materializes more data than the COGROUP

There are three opportunities for optimization here. Though these tables are far to small to warrant optimization, it's a good teachable moment for when to (not) optimize.

* You'll notice that we projected off the extraneous fields from the allstars table before the map. Pig is sometimes smart enough to eliminate fields we   don't need early. There's two ways to see if it did so. The surest way is to consult the tree that EXPLAIN produces. If you make the program use `allstars` and not `allstars_p`, you'll see that the extra fields are   present. The other way is to look at how much data comes to the reducer with and without the projection. If there is less data using `allstars_p` than `allstars`, the explicit projection is required.

* The EXPLAIN output also shows that co-group version has a simpler map-reduce plan, raising the question of whether it's more performant.

* Usually we put the smaller table (allstars) on the right in a join or cogroup. However, although the allstars table is smaller, it has larger cardinality (barely): `(player_id, team_id)` is a primary key for the bat_seasons table. So the order is likely to be irrelevant.

But "more performant" or "possibly more performant" doesn't mean "use it instead".

Eliminating extra fields is almost always worth it, but the explicit projection means extra lines of code and it means an extra alias for the reader to understand. On the other hand, the explicit projection reassures the experienced reader that the projection is for-sure-no-doubt-about-it taking place. That's actually why we chose to be explicit here: we find that the more-complicated script gives the reader less to think about.

In contrast, any SQL user will immediately recognize the join formulation of this as an anti-join. Introducing a RIGHT OUTER join or choosing the cogroup version disrupts that familiarity. Choose the version you find most readable, and then find out if you care whether it's more performant; the simpler explain graph or the smaller left-hand join table _do not_ necessarily imply a faster dataflow. For this particular shape of data, even at much larger scale we'd be surprised to learn that either of the latter two optimizations mattered.


// // ==== Shooting Yourself in the Foot with `COGROUP`-and-`FLATTEN`
// //
// // Discuss general problem of cross product
// //
// // Discuss common error of flattening on two fields and not on one tuple
