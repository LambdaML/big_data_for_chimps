== Hadoop Tuning for the Brave and Foolish

*  A happy Mapper spends most of its time processing data..
*  Happy Mappers all finish at the same time.
*  A happy Mapper uses local data.
*  A happy Mapper does not have unnecessary spills.
*  A happy Mapper has enough Java memory heap to avoid costly garbage collection.
*  A happy Mapper has a justifiable data rate.
*  A happy Reducer finishes its merge/sort in reasonable time.
*  A happy Reducer has enough Java memory heap to avoid costly garbage collection.
*  Happy Hadoop processes of all kinds do not keep customers waiting in line.
*  Happy Hadoop programmers have all the logs they need.
*  A happy Hadoop cluster should never over tax its name nodes, job tracker, datanodes or task tracker.

=== Introduction

Throughout the book, we have emphasized using Hadoop from the outside.  In almost all cases, the benefits to performance of fine-grained configuration changes do not exceed the opportunity cost of your time and the impact on maintainability and production confidence.  It is now time to arm you with the full tool set necessary to optimize Hadoop clusters and your jobs.  

The overall strategy we recommend is as follows:

1.  First, most importantly and hopefully last: Optimize the programmer not the program.  Write simple jobs that are easy to read and easy to reason about. If your job's throughput per task slot compares reasonably to the baseline throughput of your cluster, there's nothing fruitful to optimize.  The previous chapter (REF) on Tuning for the Wise and Lazy prescribed a process for determining that baseline and identifying bottlenecks.  
2. The best way to address a costly bottleneck is to minimize the amount of data Hadoop moves around. Almost nothing that follows will have as large an impact on performance as, say, filtering nulls before a JOIN.  We have spent most of the book equipping you with techniques and best practices for making your job efficient from the outside.
3.  When it is time to get hard-core, you will next want to start all the way from the inside and work your way out; this chapter will take you from the fine-grained details of how Hadoop manages memory, disk and network resources to its outward effects on machine and cluster.  While we can break the system into largely decoupled parts, there are a significant number of complex interactions, so only engage this phase if you have time for careful trials.  
4.  Finally, there are cases where nothing but a comprehensive understanding of the system will do.  We have adapted an approach called the "USCE" method made popular for low-level systems analysts by Joyent's Brendan Gregg.  (FOOTNOTE:  Brendan's original method does not include connectivity, so is called the "USE" method; putting the "C" for connectivity as "USCE" lets us still pronounce it like the word "use.")  Start by enumerating every significant resource within the system -- internal memory buffers, disks, thread pools and a few dozen others.  Then, for each resource, characterize its utilization (how busy it is), saturation (demand versus capacity), connectivity (influence of interacting resources) and errors.  It is hard work but straightforward and leaves nowhere for problems to hide.

==== A happy Reducer finishes its merge/sort in reasonable time.

As you learned in the previous chapter, and have surely experienced many times, the critical bottleneck of most costly jobs is the group-sort phase that must complete before your Reducer can run.  We will assume you have done everything you can to minimize the amount of data coming into your Reducers and minimize skew that would unfairly burden some Reducers.  We also assume you cannot solve your problem by simply commissioning more machines.  Your poor little Reducers will have to handle the amount of data you are sending to it, so it is now our job to help them do so as efficiently as possible.  

Although the group-sort phase cannot complete until all Map tasks have succeeded, you will notice the Reduce tasks start partway through the Map phase, as set by the slow start (FIX) parameter; a value of 0.3 causes it to launch when the Map phase has processed 30-percent of its input data.  This lets the Reducers perform the group-sort in parallel with the Mappers for a significant improvement in pipeline efficiency.  Setting a slow start parameter (FIX) too high leaves you doing the same amount of work in the Reducers but unnecessarily delaying it past the Map phase completion.  Setting the value too low is unfair to your friend, colleagues.  Cases where the progress estimate is particularly unreliable may justify reducing this parameter to 0.  

If you look at the Reducer log files (TODO:  find path), you will start seeing lines that read (TODO: find log message for merge during the shuffle).  These should be happening throughout the time the Map phase is executing and with the amount of data you would expect for the parameters you have just set.  

A Reducer receives its portion of fully-sorted midstream data, in parallel, as each Mapper successfully completes.  We have been using the phrase "group-sort" to describe the full process by which the Reducer receives, sorts and groups its midstream data.  For internal reasons, Hadoop describes the group-sort as being two distinct phases:  the shuffle phase, where it receives Mapper output data and opportunistically group-sorts it and the sort phase, where it completes all remaining group-sort passes.  (TODO: make sure you're calling it a group-sort and not a merge/sort throughout this phase).  

As data streams in from the Map tasks, the Reducer sorts the set of incoming records into the shuffle input buffer, very similarly to what you are familiar with in the Mapper.  The size of this buffer is not only one of the most important Hadoop settings.  It is also, in the 1.0 branch, widely and dangerously misunderstood.  

As soon as the shuffle input buffer exceeds its threshold, the Reducer flushes its data to disk, resulting in a number of spilled merge chunks.  After some number of spills have occurred (TODO: what is that number?), a parallel thread within the Reducer will begin merge/sorting the chunks.  (TODO: keep this as merge/sorting, don't change to group-sorting in this paragraph)  For good reason, each merge/sort pass can combine only a limited number of chunks, so fully sorting large amounts of midstream data will require multiple passes.  

Let's sketch a picture of what a Reducer configured to spill 1.5 GB chunks and merge them 10 at a time might do during its shuffle and sort phases.  For a job with 20 GB of midstream data per Reducer, the first merge pass would initiate soon after 10 chunks, totalling 15 GB, had been spilled.  (TODO: what triggers the merge?)  As this proceeds, however, the final 5 GB of midstream data continue to roll in -- so when the first merge pass concludes, there are 51 segments on disk.  Since the number of chunks is less than the threshold of 10, the final merge/sort can begin.  Hadoop cleverly avoids writing the merged data back to disk as a single spill by feeding the merged data directly to the merge task.  That means that in the shuffle and sort phases, Hadoop has written 35 GB to disk (20 GB as initial spills and 15 GB again as merge/sort output), and read 35 GB from disk (15 as merge/sort input, and 20 as reducer input).

If this job, instead, sent 200 GB of data to each Reducer, we will simplistically assume the first pass created 13 merged chunks of 15 GB each and four small merged chunks for the remaining 5 GB for a total of 17 chunks.  (TODO: does it try to only merge files with similar sizes?)  This means it has to do one more merge pass on 10 of the files (TODO: or 8?), producing a second-pass merge chunk of 95 GB and seven remaining first-pass merge chunks of 15 GB.  In this case, the total amount of data written and read was 490 GB of data.  

With 20 GB of data, the first merge pass has a 50-GB head start on Map phase completion, so should not continue for terribly long after the final Mapper output is received.  The merge/sort overhead is 75-percent:  an excess 15 GB above the 20 it received.  In the 200 GB case, that overhead is almost 150-percent; most of the initial merge passes and all of the second-pass merge will extend past the completion of the Map phase.  If this cluster's baseline throughput is (TODO: find an actual baseline number), processing the extra 15 GB of data took about ?15 minutes? while processing 290 GB of data took ?5 hours?.  The additional merge activity is enormously costly.  

As you can see, the number of merge passes is governed by the size of the shuffle spills and the number of parallel merges.  The larger their product, the fewer merge passes, so all other things being equal, we would like to set them as high as possible.  

The I/O sort factor variable sets the maximum number of merge segments that can be combined.  For production clusters, it should be set higher than the default of 10 but you cannot increase it too much.  The number of parallel streams the OS can provide is limited to some modest multiple of the number of disks that can be read in parallel.  (TODO: find out what the limits are on this in better detail).  Setting this too low will choke parallelism and lead to excess merge/sort passes.  Further increase above the parallelism the OS can reasonably supply has no effect; setting it too high will incur increasing bookkeeping costs and eventually exhaust system limit.  (TODO: ? recommended value ?)  

The size of the buffer is set by `mapred.job.shuffle.input.buffer.percent`, as a fraction of the total Reducer heap size.  For example, with a 1.5 GB Reducer heap, the default value of 0.66 will yield a 1 GB shuffle input buffer.  In the 1.0 branch of Hadoop, this setting, unfortunately, does not work as advertised; instead, that fraction is applied to the _lesser of the Java heap size and 2 GB_.  This means a Reducer with 2.0 GB heap gets a 1.3 GB shuffle input buffer -- and so does a Reducer of 4, 6, 8 or anything larger!!   

The final size of the spill is limited to the heap size times the shuffle input percent times a further parameter (TODO: look up name) that initiates the spill to disk before the buffer is full.  If that buffer fills, the threads receiving data from the Mappers will have to wait, slowing things down globally.  On the other hand, flushing too early leads to smaller initial spills, so more merge/sort passes.  We would leave this one where it is.  

=== Happy Mappers ===

==== A Happy Mapper is **well-fed**, **finishes with its friends**, **uses local data**, **doesn't have extra spills**, and has a **justifiable data rate**. =====

==== A Happy Mapper is Well-fed

* Map tasks should take longer to run than to start. If mappers finish in less than a minute or two, and you have control over how the input data is allocated, try to feed each more data. In general, 128MB is sufficient; we set our HDFS block size to that value.

==== A Happy Mapper finishes with its friends ====

Assuming well-fed mappers, you would like every mapper to finish at roughly the same time. The reduce cannot start until all mappers have finished. Why would different mappers take different amounts of time?

* large variation in file size
* large variation in load -- for example, if the distribution of reducers is uneven, the machines with multiple reducers will run more slowly in general
* on a large cluster, long-running map tasks will expose which machines are slowest.

==== A Happy Mapper is Busy ====

Assuming mappers are well fed and prompt, you would like to have nearly every mapper running a job.


* Assuming every mapper is well fed and every mapper is running a job, 


Pig can use the combine splits setting to make this intelligently faster. Watch out for weirdness with newer versions of pig and older versions of HBase.

If you're reading from S3, dial up the min split size as large as 1-2 GB (but not 

==== A Happy Mapper has no Reducer =====




The default settings are those that satisfy in some mixture the constituencies of a) Yahoo, Facebook, Twitter, etc; and b) Hadoop developers, ie. peopl who *write* Hadoop but rarely *use* Hadoop. This means that many low-stakes settings (like keeping jobs stats around for more than a few hours) are at the values that make sense when you have a petabyte-scale cluster and a hundred data engineers; 

* If you're going to run two master nodes, you're a bit better off running one master as (namenode only) and the other master as (jobtracker, 2NN, balancer) -- the 2NN should be distinctly less utilized than the namenode. This isn't a big deal, as I assume your master nodes never really break a sweat even during heavy usage.

=== Memory ===

Here's a plausible configuration for a 16-GB physical machine with 8 cores:

--------------------  
  `mapred.tasktracker.reduce.tasks.maximum`   = 2
  `mapred.tasktracker.map.tasks.maximum`      = 5
  `mapred.child.java.opts`                    = 2 GB
  `mapred.map.child.java.opts`                = blank (inherits mapred.child.java.opts)
  `mapred.reduce.child.java.opts`             = blank (inherits mapred.child.java.opts)
  
  total mappers' heap size                    = 10   GB (5 * 2GB)
  total reducers' heap size                   =  4   GB (2 * 2GB)
  datanode heap size                          =  0.5 GB
  tasktracker heap size                       =  0.5 GB
  .....                                         ...
  total                                       = 15   GB on a 16 GB machine
--------------------

  - It's rare that you need to increase the tasktracker heap at all. With both the TT and DN daemons, just monitor them under load; as long as the heap healthily exceeds their observed usage you're fine.

  - If you find that most of your time is spent in reduce, you can grant the reducers more RAM with `mapred.reduce.child.java.opts` (in which case lower the child heap size setting for the mappers to compensate).

* It's standard practice to disable swap -- you're better off OOM'ing footnote[OOM = Out of Memory error, causing the kernel to start killing processes outright] than swapping. If you do not disable swap, make sure to reduce the `swappiness` sysctl (5 is reasonable). Also consider setting `overcommit_memory` (1) and `overcommit_ratio` (100). Your sysadmin might get angry when you suggest these changes -- on a typical server, OOM errors cause pagers to go off. A misanthropically funny T-shirt, or whiskey, will help establish your bona fides.

* `io.sort.mb` default `X`, recommended at least `1.25 * typical output size` (so for a 128MB block size, 160). It's reasonable to devote up to 70% of the child heap size to this value.

[[io_sort_factor]]
* `io.sort.factor`: default `X`, recommended `io.sort.mb * 0.x5 * (seeks/s) / (thruput MB/s)`
  - you want transfer time to dominate seek time; too many input streams and the disk will spend more time switching among them than reading them.
  - you want the CPU well-fed: too few input streams and the merge sort will run the sort buffers dry.
  - My laptop does 76 seeks/s and has 56 MB/s throughput, so with `io.sort.mb = 320` I'd set `io.sort.factor` to 27.
  - A server that does 100 seeks/s with 100 MB/s throughput and a 160MB sort buffer should set `io.sort.factor` to 80.

* `io.sort.record.percent` default `X`, recommended `X` (but adjust for certain jobs)

* `mapred.reduce.parallel.copies`: default `X`, recommended  to be in the range of `sqrt(Nw*Nm)` to `Nw*Nm/2`  You should see the shuffle/copy phase of your reduce tasks speed up.

* `mapred.job.reuse.jvm.num.tasks` default `1`, recommended `10`. If a job requires a fresh JVM for each process, you can override that in its jobconf. Going to `-1` (reuse unlimited times) can fill up the dist if your input format uses "delete on exit" temporary files (as for example the S3 filesystem does), with little additional speedup.

* You never want Java to be doing stop-the-world garbage collection, but for large JVM heap sizes (above 4GB) they can become especially dangerous. If a full garbage collect takes too long, sockets can time out, causing loads to increase, causing garbage collects to happen, causing... trouble, as you can guess.

* Given the number of files and amount of data you're storing, I would set the NN heap size agressively - at least 4GB to start, and keep an eye on it. Having the NN run out of memory is Not Good. Always make sure the secondary name node has the same heap setting as the name node.

=== Handlers and threads ===

* `dfs.namenode.handler.count`: default `X`, recommended: `(0.1 to 1) * size of cluster`, depending on how many blocks your HDFS holds.
* `tasktracker.http.threads` default `X`, recommended `X`

* Set `mapred.reduce.tasks` so that all your reduce slots are utilized -- If you typically only run one job at a time on the cluster, that means set it to the number of reduce slots. (You can adjust this per-job too). Roughly speaking: keep `number of reducers * reducer memory` within a factor of two of your reduce data size.

* `dfs.datanode.handler.count`:  controls how many connections the datanodes can maintain. It's set to 3 -- you need to account for the constant presence of the flume connections. I think this may be causing the datanode problems. Something like 8-10 is appropriate.
* You've increased `dfs.datanode.max.xcievers` to 8k, which is good.

* `io.file.buffer.size`: default `X` recommended `65536`; always use a multiple of `4096`.

=== Storage ===
  
* `mapred.system.dir`: default `X` recommended `/hadoop/mapred/system` Note that this is a path on the HDFS, not the filesystem).

* Ensure the HDFS data dirs (`dfs.name.dir`, `dfs.data.dir` and `fs.checkpoint.dir`), and the mapreduce local scratch dirs (`mapred.system.dir`) include all your data volumes (and are off the root partition). The more volumes to write to the better. Include all the volumes in all of the preceding. If you have a lot of volumes, you'll need to ensure they're all attended to; have 0.5-2x the number of cores as physical volumes.
  - HDFS-3652 -- don't name your dirs `/data1/hadoop/nn`, name them `/data1/hadoop/nn1`  ( final element differs).

* Solid-state drives are unjustifiable from a cost perspective. Though they're radically better on seek they don't improve performance on bulk transfer, which is what limits Hadoop. Use regular disks.

* Do not construct a RAID partition for Hadoop -- it is happiest with a large JBOD. (There's no danger to having hadoop sit on top of a RAID volume; you're just hurting performance).

* We use `xfs`; I'd avoid `ext3`.

* Set the `noatime` option (turns off tracking of last-access-time) -- otherwise the OS updates the disk on every read.

* Increase the ulimits for open file handles (`nofile`) and number of processes (`nproc`) to a large number for the `hdfs` and `mapred` users: we use `32768` and `50000`.
  - be aware: you need to fix the ulimit for root (?instead ? as well?)

* `dfs.blockreport.intervalMsec`: default 3_600_000 (1 hour); recommended 21_600_000 (6 hours)  for a large cluster.
  - 100_000 blocks per data node for a good ratio of CPU to disk

=== Other ===

* `mapred.map.output.compression.codec`: default XX, recommended ``. Enable Snappy codec for intermediate task output.
  - `mapred.compress.map.output`
  - `mapred.output.compress`
  - `mapred.output.compression.type`
  - `mapred.output.compression.codec`

* `mapred.reduce.slowstart.completed.maps`: default `X`, recommended `0.2` for a single-purpose cluster, `0.8` for a multi-user cluster. Controls how long, as a fraction of the full map run, the reducers should wait to start. Set this too high, and you use the network poorly -- reducers will be waiting to copy all their data. Set this too low, and you will hog all the reduce slots.

* `mapred.map.tasks.speculative.execution`: default: `true`, recommended: `true`. Speculative execution (FIXME: explain). So this setting makes jobs finish faster, but makes cluster utilization higher; the tradeoff is typically worth it, especially in a development environment. Disable this for any map-only job that writes to a database or has side effects besides its output. Also disable this if the map tasks are expensive and your cluster utilization is high.
* `mapred.reduce.tasks.speculative.execution`: default `false`, recommended: `false`.

* (hadoop log location): default `/var/log/hadoop`, recommended `/var/log/hadoop` (usually). As long as the root partition isn't under heavy load, store the logs on the root partition. Check the Jobtracker however -- it typically has a much larger log volume than the others, and low disk utilization otherwise. In other words: use the disk with the least competition.

* `fs.trash.interval` default `1440` (one day), recommended `2880` (two days). I've found that files are either a) so huge I want them gone immediately, or b) of no real concern. A setting of two days lets you to realize in the afternoon today that you made a mistake in the morning yesterday, 

* Unless you have a ton of people using the cluster, increase the amount of time the jobtracker holds log and job info; it's nice to be able to look back a couple days at least. Also increase `mapred.jobtracker.completeuserjobs.maximum` to a larger value. These are just for politeness to the folks writing jobs.
  - `mapred.userlog.retain.hours`
  - `mapred.jobtracker.retirejob.interval`
  - `mapred.jobtracker.retirejob.check`
  - `mapred.jobtracker.completeuserjobs.maximum`
  - `mapred.job.tracker.retiredjobs.cache`
  - `mapred.jobtracker.restart.recover`


* Bump `mapreduce.job.counters.limit` -- it's not configurable per-job.

(From http://blog.cloudera.com/blog/2009/12/7-tips-for-improving-mapreduce-performance/ -- 512M block size fairly reasonable)










mapred.tasktracker.map.tasks.maximum
mr










mapred.tasktracker.map.tasks.maximum






mapred.tasktracker.reduce.tasks.maximum
mr










mapred.tasktracker.reduce.tasks.maximum


1
mem-m
dfs.block.size
dfs
67108864
134217728
134217728




dfs.block.size
The default block size for new files. Apache default is 67108864 (64MB); CDH3 is 128MB
2
mem-m
fs.s3.block.size
cor
67108864


134217728




fs.s3.block.size
Block size to use when writing files to S3.
3
mem-m
fs.s3n.block.size
cor
67108864


134217728




fs.s3n.block.size
Block size to use when reading files using the native S3 filesystem (s3n: URIs).
18
mem-m
mapred.min.split.size
mr
0








mapred.min.split.size
The minimum size chunk that map input should be split into. Note that some file formats may have minimum split sizes that take priority over this setting.
4
mem-m
io.sort.mb
mr
100


226
225
block size * 1.2 safety factor / ( (1-i.s.record.percent) * i.s.spill.percent)
io.sort.mb
The cumulative size of the serialization and accounting buffers storing records emitted from the map, in megabytes.
5
mem-m
io.sort.record.percent
mr
0.05




0.15


io.sort.record.percent
The ratio of serialization to accounting space can be adjusted. Each serialized record requires 16 bytes of accounting information in addition to its serialized size to effect the sort. This percentage of space allocated from io.sort.mb affects the probability of a spill to disk being caused by either exhaustion of the serialization buffer or the accounting space. Clearly, for a map outputting small records, a higher value than the default will likely decrease the number of spills to disk.
6
mem-m
io.sort.spill.percent
mr
0.8




0.8
16 / (16 + rec.size in bytes); check the "Map Output Records" vs "Map Output Bytes" counters
io.sort.spill.percent
This is the threshold for the accounting and serialization buffers. When this percentage of either buffer has filled, their contents will be spilled to disk in the background. Let io.sort.record.percent be r, io.sort.mb be x, and this value be q. The maximum number of records collected before the collection thread will spill is r * x * q * 2^16. Note that a higher value may decrease the number of- or even eliminate- merges, but will also increase the probability of the map task getting blocked. The lowest average map times are usually obtained by accurately estimating the size of the map output and preventing multiple spills.
7


max mb per spill










153
max mb per spill


8


block fraction overage










120%
block fraction overage


9


max records per spill










1,769,472
max records per spill


10


"small" record breakeven, bytes










91
"small" record breakeven, bytes


13
mem-m
io.file.buffer.size


4096
65536
65536




io.file.buffer.size
The size of buffer for use in sequence files. The size of this buffer should probably be a multiple of hardware page size (4096 on Intel x86), and it determines how much data is buffered during read and write operations.
59
mem-m
dfs.replication
dfs
3








dfs.replication
Default block replication. The actual number of replications can be specified when the file is created. The default is used if replication is not specified in create time.
61
mem-m
mapred.child.java.opts
mr










mapred.child.java.opts
Java opts for the task tracker child processes. The following symbol, if present, will be interpolated: @taskid@ is replaced by current TaskID. Any other occurrences of '@' will go unchanged. For example, to enable verbose gc logging to a file named for the taskid in /tmp and to set the heap maximum to be a gigabyte, pass a 'value' of: -Xmx1024m -verbose:gc -Xloggc:/tmp/@taskid@.gc The configuration variable mapred.child.ulimit can be used to control the maximum virtual memory of the child processes.
61
mem-m
mapred.map.child.java.opts
mr










mapred.map.child.java.opts


61
mem-r
mapred.reduce.child.java.opts
mr










mapred.reduce.child.java.opts


12




















6
mem-r
io.sort.factor
mr
10






covaries with io.sort.mb, io.file.buffer.size and your disk's latency/bandwidth characteristics
io.sort.factor
Specifies the number of segments on disk to be merged at the same time. It limits the number of open files and compression codecs during the merge. If the number of files exceeds this limit, the merge will proceed in several passes. Though this limit also applies to the map, most jobs should be configured so that hitting this limit is unlikely there.
7
mem-r
mapred.inmem.merge.threshold
mr
1000


0




mapred.inmem.merge.threshold
The number of sorted map outputs fetched into memory before being merged to disk. Like the spill thresholds in the preceding note, this is not defining a unit of partition, but a trigger. In practice, this is usually set very high (1000) or disabled (0), since merging in-memory segments is often less expensive than merging from disk (see notes following this table). This threshold influences only the frequency of in-memory merges during the shuffle. Note: When running with a combiner, the reasoning about high merge thresholds and large buffers may not hold. For merges started before all map outputs have been fetched, the combiner is run while spilling to disk. In some cases, one can obtain better reduce times by spending resources combining map outputs- making disk spills small and parallelizing spilling and fetching- rather than aggressively increasing buffer sizes. Also, when merging in-memory map outputs to disk to begin the reduce, if an intermediate merge is necessary because there are segments to spill and at least io.sort.factor segments already on disk, the in-memory map outputs will be part of the intermediate merge. From the XML: The threshold, in terms of the number of files for the in-memory merge process. When we accumulate threshold number of files we initiate the in-memory merge and spill to disk. A value of 0 or less than 0 indicates we want to DON'T have any threshold and instead depend only on the ramfs's memory consumption to trigger the merge
9
mem-r
mapred.job.shuffle.input.buffer.percent
mr
0.7






reducer heap * m.j.shuffle.in.buf.pct > typical map task out size
mapred.job.shuffle.input.buffer.percent
The percentage of memory- relative to the maximum heapsize as typically specified in mapred.child.java.opts- that can be allocated to storing map outputs during the shuffle. Though some memory should be set aside for the framework, in general it is advantageous to set this high enough to store large and numerous map outputs. The percentage of memory to be allocated from the maximum heap size to storing map outputs during the shuffle.
8
mem-r
mapred.job.shuffle.merge.percent
mr
0.66






if each reducer's total load is less than (reducer heap * m.j.shuffle.in.buf.pct * m.j.shuffle.merge.percent) they will never hit the disk
mapred.job.shuffle.merge.percent
The memory threshold for fetched map outputs before an in-memory merge is started, expressed as a percentage of memory allocated to storing map outputs in memory. Since map outputs that can't fit in memory can be stalled, setting this high may decrease parallelism between the fetch and merge. Conversely, values as high as 1.0 have been effective for reduces whose input can fit entirely in memory. This parameter influences only the frequency of in-memory merges during the shuffle. XML: The usage threshold at which an in-memory merge will be initiated, expressed as a percentage of the total memory allocated to storing in-memory map outputs, as defined by mapred.job.shuffle.input.buffer.percent
10
mem-r
mapred.job.reduce.input.buffer.percent
mr
0


0


if your reduce task itself doesn't need ram (eg for wukong jobs), set this to more like 0.7
mapred.job.reduce.input.buffer.percent
The percentage of memory relative to the maximum heapsize in which map outputs may be retained during the reduce. When the reduce begins, map outputs will be merged to disk until those that remain are under the resource limit this defines. By default, all map outputs are merged to disk before the reduce begins to maximize the memory available to the reduce. For less memory-intensive reduces, this should be increased to avoid trips to disk. XML: The percentage of memory- relative to the maximum heap size- to retain map outputs during the reduce. When the shuffle is concluded, any remaining map outputs in memory must consume less than this threshold before the reduce can begin.
11
mem-r
mapred.reduce.parallel.copies
mr
5
10




reducers * m.r.parallel.copies ~ machines * tt.http.threads; also affected by m.r.slowstart.completed.maps
mapred.reduce.parallel.copies
The default number of parallel transfers run by reduce during the copy(shuffle) phase.
12




















24
mem
datanode heap size
mr




384




datanode heap size


25
mem
tasktracker heap size
mr




384




tasktracker heap size


35




















36
disk
mapred.output.compress
mr
FALSE


TRUE




mapred.output.compress
Should the job outputs be compressed?
39
disk
mapred.compress.map.output
mr
FALSE


FALSE




mapred.compress.map.output
Should the outputs of the maps be compressed before being sent across the network. Uses SequenceFile compression.
37
disk
mapred.output.compression.type
mr
RECORD


SnappyCodec




mapred.output.compression.type
If the job outputs are to compressed as SequenceFiles, how should they be compressed? Should be one of NONE, RECORD or BLOCK.
38
disk
mapred.output.compression.codec
mr
DefaultCodec


SnappyCodec




mapred.output.compression.codec
If the job outputs are compressed, how should they be compressed?
40
disk
mapred.map.output.compression.codec
mr
DefaultCodec








mapred.map.output.compression.codec
If the map outputs are compressed, how should they be compressed?
41
disk
io.seqfile.compress.blocksize
cor
1000000








io.seqfile.compress.blocksize
The minimum block size for compression in block compressed SequenceFiles.
42
disk
io.compression.codecs
cor




add Snappy




io.compression.codecs


21




















45
exec
mapred.reduce.slowstart.completed.maps
mr
0.05








mapred.reduce.slowstart.completed.maps
Fraction of the number of maps in the job which should be complete before reduces are scheduled for the job.
15
exec
mapred.map.tasks.speculative.execution
mr
TRUE
TRUE
TRUE




mapred.map.tasks.speculative.execution
If true, then multiple instances of some map tasks may be executed in parallel. Set this to false on any map-only job that will write to a database or other external resource.
16
exec
mapred.reduce.tasks.speculative.execution
mr
TRUE
FALSE
FALSE




mapred.reduce.tasks.speculative.execution
If true, then multiple instances of some reduce tasks may be executed in parallel.
58




















22




















23


Hadoop












Hadoop


26
exec
mapred.job.reuse.jvm.num.tasks
mr
1


-1




mapred.job.reuse.jvm.num.tasks


27
exec
mapred.child.ulimit
mr










mapred.child.ulimit
The maximum virtual memory, in KB, of a process launched by the Map-Reduce framework. This can be used to control both the Mapper/Reducer tasks and applications using Hadoop Pipes, Hadoop Streaming etc. By default it is left unspecified to let cluster admins control it via limits.conf and other such relevant mechanisms. Note: mapred.child.ulimit must be greater than or equal to the -Xmx passed to JavaVM, else the VM might not start




mapred.map.child.ulimit












mapred.map.child.ulimit






mapred.reduce.child.ulimit












mapred.reduce.child.ulimit




exec
mapred.jobtracker.restart.recover
mr
FALSE




TRUE


mapred.jobtracker.restart.recover
"true" to enable (job) recovery upon restart, "false" to start afresh






















29


dfs.datanode.max.xcievers












dfs.datanode.max.xcievers


30
net
mapred.job.tracker.handler.count
mr
10


40
64


mapred.job.tracker.handler.count
The number of server threads for the JobTracker. This should be roughly 4% of the number of tasktracker nodes.
33
net
dfs.namenode.handler.count
dfs
10


40
64


dfs.namenode.handler.count
The number of server threads for the namenode.
31
net
tasktracker.http.threads
mr
40
46
32




tasktracker.http.threads
The number of worker threads that for the http server. This is used for map output fetching
32
net
dfs.datanode.handler.count
dfs
3


8




dfs.datanode.handler.count
The number of server threads for the datanode.
62
exec
fs.s3.maxRetries
cor
4








fs.s3.maxRetries
The maximum number of retries for reading or writing files to S3, before we signal failure to the application.
63
exec
fs.s3.sleepTimeSeconds
cor
10








fs.s3.sleepTimeSeconds
The number of seconds to sleep between each S3 retry.
49




















50
usage
mapred.heartbeats.in.second
mr
100








mapred.heartbeats.in.second
Expert: Approximate number of heart-beats that could arrive at JobTracker in a second. Assuming each RPC can be processed in 10msec, the default value is made 100 RPCs in a second.


usage
mapreduce.tasktracker.outofband.heartbeat
mr
FALSE




TRUE


mapreduce.tasktracker.outofband.heartbeat
Expert: Set this to true to let the tasktracker send an out-of-band heartbeat on task-completion for better latency.
51
usage
mapred.disk.healthChecker.interval
mr
60000








mapred.disk.healthChecker.interval
How often the TaskTracker checks the health of its local directories. Configuring this to a value smaller than the heartbeat interval is equivalent to setting this to heartbeat interval value.
52
usage
mapred.healthChecker.interval
mr
60000








mapred.healthChecker.interval
Frequency of the node health script to be run, in milliseconds. Default 1 minute
53
usage
dfs.df.interval
dfs
60000


300000




dfs.df.interval
Disk usage statistics refresh interval in msec.
54
usage
dfs.blockreport.intervalMsec
dfs
3600000








dfs.blockreport.intervalMsec
Determines block reporting interval in milliseconds.
55
usage
dfs.heartbeat.interval
dfs
3








dfs.heartbeat.interval
Determines datanode heartbeat interval in seconds.
56
usage
fs.checkpoint.period
cor
3600








fs.checkpoint.period
The number of seconds between two periodic checkpoints.
57
usage
fs.checkpoint.size
cor
67108864








fs.checkpoint.size
The size of the current edit log (in bytes) that triggers a periodic checkpoint even if the fs.checkpoint.period hasn't expired.
44




















28
usage
mapred.submit.replication
mr
10








mapred.submit.replication
The replication level for submitted job files. This should be around the square root of the number of nodes.
17




















19
exec
mapred.max.tracker.blacklists
mr
4








mapred.max.tracker.blacklists
The number of blacklists for a taskTracker by various jobs after which the task tracker could be blacklisted across all jobs. The tracker will be given a tasks later (after a day). The tracker will become a healthy tracker after a restart
20
exec
mapred.max.tracker.failures
mr
4








mapred.max.tracker.failures
The number of task-failures on a tasktracker of a given job after which new tasks of that job aren't assigned to it.
47




















48
disk
dfs.datanode.du.reserved
dfs
0
1073741824






dfs.datanode.du.reserved
Reserved space in bytes per volume. Always leave this much space free for non dfs use.
64




















65


Logging












Logging
http://www.cloudera.com/blog/2010/11/hadoop-log-location-and-retention/
66
interact
fs.trash.interval
cor
0


4320




fs.trash.interval
Number of minutes between trash checkpoints. If zero, the trash feature is disabled.
67
interact
mapreduce.job.counters.max
mr
120








mapreduce.job.counters.max


68
interact
mapreduce.job.counters.groups.max
mr
50








mapreduce.job.counters.groups.max


69
interact
webinterface.private.actions
cor
FALSE


FALSE
TRUE


webinterface.private.actions
If set to true, the web interfaces of JT and NN may contain actions, such as kill job, delete file, etc., that should not be exposed to public. Enable this option if the interfaces are only reachable by those who have the right authorization.
46
log
mapred.combine.recordsBeforeProgress
mr
10000








mapred.combine.recordsBeforeProgress
The number of records to process during merge before sending a progress notification to the TaskTracker.
73
log
mapred.job.tracker.persist.jobstatus.active
mr
FALSE




TRUE


mapred.job.tracker.persist.jobstatus.active
Indicates if persistency of job status information is active or not.
70
log
mapred.jobtracker.completeuserjobs.maximum
mr
100




1000


mapred.jobtracker.completeuserjobs.maximum
The maximum number of complete jobs per user to keep around before delegating them to the job history.
71
log
mapred.userlog.limit.kb
mr
0




10100100


mapred.userlog.limit.kb
The maximum allowed size of the user jobconf.
72
log
mapred.userlog.retain.hours
mr
24




240


mapred.userlog.retain.hours
The maximum time, in hours, for which the user-logs are to be retained after the job completion.
74
log
mapred.job.tracker.persist.jobstatus.hours
mr
0




240


mapred.job.tracker.persist.jobstatus.hours
The number of hours job status information is persisted in DFS. The job status information will be available after it drops of the memory queue and between jobtracker restarts. With a zero value the job status information is not persisted at all in DFS.
75
log
mapred.jobtracker.retirejob.check
mr






1200000


mapred.jobtracker.retirejob.check




log
mapred.jobtracker.retirejob.interval
mr






864000000


mapred.jobtracker.retirejob.interval
























76
log-prof
mapred.task.profile
mr
FALSE








mapred.task.profile
To set whether the system should collect profiler information for some of the tasks in this job? The information is stored in the user log directory. The value is "true" if task profiling is enabled.
77
log-prof
mapred.task.profile.maps
mr
0-2








mapred.task.profile.maps
To set the ranges of map tasks to profile. mapred.task.profile has to be set to true for the value to be accounted.
78
log-prof
mapred.task.profile.reduces
mr
0-2








mapred.task.profile.reduces
To set the ranges of reduce tasks to profile. mapred.task.profile has to be set to true for the value to be accounted.
79




















80
resize
topology.node.switch.mapping.impl
cor










topology.node.switch.mapping.impl
The default implementation of the DNSToSwitchMapping. It invokes a script specified in topology.script.file.name to resolve node names. If the value for topology.script.file.name is not set, the default value of DEFAULT_RACK is returned for all node names.
81
resize
topology.script.file.name
cor










topology.script.file.name
The script name that should be invoked to resolve DNS names to NetworkTopology names. Example: the script would take host.foo.bar as an argument, and return /rack1 as the output.
81
resize
net.topology.table.file.name
cor


(patched)






net.topology.table.file.name
The file name for a topology file, which is used when the topology.script.file.name property is set to org.apache.hadoop.net.TableMapping. The file format is a two column text file, with columns separated by whitespace. The first column is a DNS or IP address and the second column specifies the rack where the address maps. If no entry corresponding to a host in the cluster is found, then /default-rack is assumed.
82
resize
dfs.balance.bandwidthPerSec
dfs


1048576
1100100
20100100


dfs.balance.bandwidthPerSec
Specifies the maximum amount of bandwidth that each datanode can utilize for the balancing purpose in term of the number of bytes per second.
83
resize
dfs.namenode.decommission.nodes.per.interval
dfs


5
20




dfs.namenode.decommission.nodes.per.interval



