== Geographic Analysis

// TODO: Spatial Analysis?

Having conquered time (or, at least, learned basics of timeseries analysis), let's move on to
space. Problems with an essential geoographic footnote:[You'll also see 'Spatial', 'Geospatial',
'Geodata', 'GIS' (Geographic Information Systems), and many other mashups with the prefix 'Geo-'. We
chose 'Geographic' because it seems the friendliest term, and will reserve 'GIS' to mean "the highly
sophisticated traditional geographic analysis toolset"] aspect appear naturally in all sorts of
contexts: "Where should I put this cell tower / oil well / coffee shop?". They're also incredibly
useful because geography is such a good proxy for many key features you can't easily measure.  You
wouldn't think the 32 bits of an IP address described the weather.  But as you saw in the last
chapter an IP address can (imperfectly but actionably) be related to a geolocation, and a
geolocation can (imperfectly but actionably) be related to a weather report.  Without violating a
person's privacy, you can know when to switch from promoting suncreen to Sydneysiders and galoshes
to Glaswegians to quite profitably doing the reverse.

The methods we'll demonstrate here extend naturally to anything with a spatial aspect -- it doesn't
have to be the surface of the Earth. And it's surprisingly easy to extend them to three and higher
dimensions (see the exercise at the end of the chapter). That's good news for anyone analyzing ocean
currents, parts of the brain, sensor feeds from a secure facility, or customer flow through a
mall. But every reader looking to master machine learning techniques will find that the material in
this chapter laid a solid foundation for understanding the highly-dimensional spaces they work in.

Most importantly of all, this chapter will solidify your intuition about how to move and combine
data strategically.  You'll be learning new types of joins that require a more sophistitcated notion
of 'context' (data locality).  But the deep natural intuition about space and physical locality you
already possess makes this challenge much easier.  Most readers should concentrate foremost on the
tactics of applying our analytic patterns. (In fact, if you want to trust us that the sections where
we dive into the underlying map/reduce work as advertised, we've made them easy to skip the first
time through.)  But come back at some point and step through the spatial and multi-scale spatial
joins we introduce in this chapter.  If you understand the underlying mechanics -- how the design of
the keys ensures records that must be related in context show up at the right place in the right
order -- then you have mastered the deep concept of map/reduce.



NOTE: Geographic Analysis is a well developed field.  The number of hard problems with valuable solutions
in intelligence, petroleum, natural resources and other such industries gave rise to highly
sophisticated products capable of processing massive quantities of data well before these kids with
their Hadoops and their JSONs came along.  As you might expect, however these products possess a
correspondingly steep price tag and learning curve and often integrate poorly outside their
domain. Most importantly, they are bound by the limits of traditional database technology in many
aspects. So even users of these powerful GIS tools can benefit by augmenting them with Big Data
analytics. But our focus will be on the reader who needs to get stuff done with geographic data, and
the less they have to learn a new field the better.  We're going to use GeoJSON, a newly-developed
standard for geographic data (friendly to general purpose analytic tools and to rendering data in
the browser), rather than Arcview shapefiles or other geospatial formats (seamless integration with
industry products). We've simplified concepts and minimized jargon to keep the focus on readable,
powerful scripts that give actionable insight.



* Mapping from earth to grid
* Extending the base of analytic patterns to two and more dimensions

long/lat implies elevation -- For the great majority of questions of interest, elevation is irrelevant, and so we discard it
We just need to rough-cut the data and then turn it over to dedicated spatial methods


Our approach will be consistently to
apply the toolkit of Analytic Patterns that we assembled in Chapters 4-9 (REF)
to put all relevant data into context,
and then turn to specialized geospatial libraries to
synthesize results

The large-scale part of this demands no great sophistication
We can pretend that circles are rectangular, that shapes do not ever have holes, that the earth is not only a perfect sphere but in fact is a planar grid. All manner of convenient distortions
so outrageous they literally tear the space-time continuum
are allowable as long as they obey the fundamental strategic rule:

* put all data that might form relevant context together

(TODO: better phrasing)

=== Points, Paths and Regions




==== Geometry Primitives: Points, Polygons and so forth

Back in Chapter 4 (REF), we introduced the simple scalar types (numbers, strings, etc.) and three complex types (`tuple`, `bag`, and `map`). Since every spatial analysis exploration involves

One thing
Spatial analysis libraries
rely on the http://www.opengeospatial.org/[OGC (Open Geospatial Consortium)]

Geometry

Point, LineString, Polygon; and corresponding multi-part geometries MultiPoint, MultiLineString, MultiPolygon.

Behind these smiling friendly inviting abstraction
lies
a host of diabolical complexities

In regular usage, even double-precision floating-point math can introduce
discrepancies large enough to incalidate results
or present visual artifacts
-- pushing the boundary of a shape off the shape itself, causing tears or overlaps where there were none, turning small polygons into degenerate points, introduce numerical instability

But for the big data section of it, where we are chiefly concerned with relating data in context,
there are really only these

* a point in space
* a spatial extent -- paths, regions, etc
// * non-spatial data

In fact, we can go even farther:

* points
* rectangles

Remember, all we're trying to do is land all (possibly) related data onto the same reducer before we bring in the big guns.



==== Smoothing Pointwise Data Locally (Spatial Aggregation of Points)



* You want to "wash out" everything but the spatial variation -- even though the data was gathered for each
* Point measurement of effect with local extent -- for example, the temperature measured at a weather station is understood to be representative of the weather for several surrounding miles.
*
*




===== Pattern Recap: Spatial Aggregation of Points


*
* _Generic Example_ -- mmm
* _When You'll Use It_ -- as mentioned above:
    * data reduction, especially for a heatmap visualization;
    * extracting a continuous measurement from a pointwise sample;
    * providing a common basis for comparison of multiple datasets;
    * smoothing out spatial variation;
    * for all the other reasons you aggregate groups of related values in context
* _Exercises_ --






==== Smoothing Regional Data onto a Consistent Grid (Spatial Aggregation of Regions)













==== Matching Nearby Points (Pointwise Spatial Join)


* Common sense tells you that a weather observation is generally valid for places within a few kilometers, but certainly not useful for places Hundreds of kilometers away. It would be useful to have a more precise guideline for the distance where a weather measurement should not be considered reliable.
    * first find all pairs of weather stations within 50 km of each other. Emit each pair of IDs with the lower-numbered ID in the first slot (making it easy to ensure uniqueness).
    * for each such pair, take a year of weather observations and determine the difference in temperature measurements taken at the same hour
        * HashMap (replicated) join of station-station pairs on the observations table. You could also do a total sort of the pairs table and use a merge-join if you're memory constrained.
        * join the resulting table back onto the observations table.
        * (In this case, most weather stations are a part of at least one pair, and so most of the rows in the observations table are retained. If that weren't the case,
            *

    * As the radius expands, you'll quickly find that the amount of data begins to explode, so restrict that upper radius band initially.
    * (is this also a problem: "You might have also noticed another problem. Even apart from a distance effect, with more neighbors there are more opportunities for observations to disagree.")
    *
    * (you should know that the answer has some bias -- places with a large concentration of weather stations are typically heavily populated, and heavily populated places don't tend to have extreme weather. We're just looking for a good rule-of-thumb though)


What makes a good exemplar?
* Head-of-the-tail --
    * extreme specimens will pop on their own. You want to see what's happening to the
* Ones that are unusual without being weird. The solstice is
* Essential troublemakers: leap years, the centennial leap-year-exceptions, and the quad-centennial leap-year-exception-exceptions.
* Well represented
    * it's no fun if your exemplars disappear mid-journey -- most commonly because they failed to find a match during a join.
* Chosen by out-of-band criteria -- deciding to look for "this date three years ago" and then finding a record is better than choosing the first record you see -- that particular record may have been the first one you saw because it is unrepresentative in some way.
    * just as a magician will pull back their shirtsleeves to show they have no rabbit concealed within, this keeps you from fooling yourself. http://en.wikipedia.org/wiki/Nothing_up_my_sleeve_number
    * (in fact, Cryptographers have a concept of a "nothing-up-my-sleeve" number: when a large arbitrary collection of numbers is needed, choosing the first twenty-five digits of Pi is believably arbitrary, whereas choosing the 387'th through 412'th digits raises the specter of a purposeful "backdoor").



We will start, as we always do, by applying patterns that turn Big Data into Much a Less Data. In particular,
A great tool for visualizing a large spatial data set



==== Smoothing Pointwise Data Locally (Spatial Aggregations)



There are a great many occasions where it's useful to translate

* You have sampled data at points in order to estimate something with spatial extent. The weather dataset is an example:

* Data that manifests at a single point
  represents a process with
  For example, the number of airline passengers in and out of the major airport
  are travelling to and from local destinations

* Smoothing pointwise data
  into a
  easier to compare or manage

* continuous approximation
  represents just the variation due to spatial
  variables


The straightforward approach we'll take is to divide the world up into a grid of tiles and map the position of each point onto the unique grid tile it occupies. We can then group on each tile

// TODO-qem: do we use just plain x / y coordinates?

footnote:[Instead of the ]


Area of a spherical segment is 2*pi*R*h --
so for lat from equator to 60


===== Pattern in Use


* _Further Reading_:
  - A https://en.wikipedia.org/wiki/Dot_distribution_map[Dot Distribution Map] is in some sense the counterpart to a spatial average.


==== Exporting data for Presentation by a Tileserver

==== Finding the Centroid of an Extent



==== Finding the Bounding Box of an Extent


==== Finding the Bounding Box of Points Within a Radius





* _choose exemplars_:
  - Midway, because it's large; Austin, because it's one of our exemplar cities; and (TODO something tiny) because it's very small.
  - the sightings X, y, which each have a fun description and are near multiple airports; and Z, which is not near an airport.
  - weather observations:
      - a date with a new moon and a full moon; 8/8/08, because auspicious; an equinox and a solstice
  -


==== Combining Regions with Set Operations

(intersection, union, diff, xor)


==== Testing the Relationship of two Regions

DE-9IM

equals
disjoint
touches
contains
covers

intersects,
within
covered_by

crosses
overlaps

From Wikipedia:

	Equals:   a = b    that is    (a ∩ b = a) ∧ (a ∩ b = b)
	Within:   a ∩ b = a
	Intersects:   a ∩ b ≠ ∅
	Touches:   (a ∩ b ≠ ∅) ∧ (aο ∩ bο = ∅)

	point/point	Equals, Disjoint	Other valid predicates collapses into Equals.
	point/line	adds Intersects	Intersects is a flexibilization of Equals, "some equal point at the line".
	line/line	adds Touches, Crosses, ...	Touches is a constraint of Intersects, about "only boundaries"; Crosses about "only one point".

{0,1,2,T,F,*} -- dimensions 0, 1, 2; T / F; dont-care



=== Key Strategic Pattern: Tile / Cull / Process


* _Tile_    -- tile the grid
* _Cull_    -- eliminate
* _Process_ --

=== Matching Points in a Table with Nearby Points in Another (Spatial Join)


* scatter points to nine tiles


=== Matching Points with the Regions










=== Mechanics of Geographic Data

==== Longitude and Latitude, Points and Features

* floating point vs decimal -- The level of precision we're working with here doesn't justify giving up the benefits of a direct representation.

==== GeoJSON


* OpenDataLab POJOs for Jackson
  - https://github.com/opendatalab-de/geojson-jackson

* GeoTools http://www.geotools.org/
  -

* ESRI library:
  - https://github.com/Esri/geometry-api-java -- The Esri Geometry API for Java enables developers to write custom applications for analysis of spatial data. This API is used in the Esri GIS Tools for Hadoop and other 3rd-party data processing solutions.
  - https://github.com/Esri/spatial-framework-for-hadoop
*

==== Creating a Spatial Density Map

Map points to quad cells, plot number density of airports as a heat map

Then geonames places -- show lakes and streams (or something nature-y) vs something urban-y

(just call out that rollup, summing trick, or group-decorate-flatten would work: do no pursue)

Do that again, but for a variable: airport flight volume -- researching
epidemiology

// FAA flight data http://www.faa.gov/airports/planning_capacity/passenger_allcargo_stats/passenger/media/cy07_primary_np_comm.pdf

=== Spatial Nearness Join (Within-X-Distance)

* quad cells have different sizes; lookup table

All sightings near airport

(dispatch to 9 nearest you)

=== Spatial Nearest-Feature Join

==== Voronoi Cells

How do we extend region of a
How would you help find the nearest 7-11?
  -- one way would be to look for stores within X distance,
  but a customer in the western US might be excited to learn there's one within an hour's drive,
  while that same radius centered on Manhattan would require sorting through thousande




==== Breaking Regions into Quad Cells

* recursively decompose a region on quadcells

(what do numbers look like doing this for the US, daily)

==== Joining Stadiums onto Quad Cells

join games on parks_info to get location, quadkey

foreach stadiums to get months in action and quadkey
join stadiums by (quadkey, month), weather voronois by (quadkey, month)
find actual nearest weather station for that stadium

now park_info has month, quadkey, weather station, date, parkid
join park_info on games -- get game_wstns (game_id, scorecard weather, scorecard_wind, park_id, wstn_id)
join game_wstns on wobs to get game_weather


A bounding box around the

* Continental US: `-125.0011, 24.9493, -66.9326, 49.5904`; centroid: `-95.9669, 37.1669`.
* Alaska: `179.1506, 51.2097, -129.9795, 71.4410

=== Multi-Scale Spatial Data

If we want to combine weather

==== Breaking regions into Multi-Cell Quads










===  Turning Points of Measurements Into Regions of Influence

Frequently, geospatial data is, for practical reasons, sampled at discrete points but should be understood to represent measurements at all points in space.  For example, the measurements in the NCDC datasets are gathered at locations chosen for convenience or value -- in some cases, neighboring stations are separated by blocks, in other cases by hundreds of miles.  It is useful to be able to reconstruct the underlying spatial distribution from point-sample measurements.

Given a set of locations -- broadcast towers, 7-11 stores, hospitals -- it is also useful to be able to determine, for any point in space, which of those objects is nearest.  When the distribution of objects is even, this is straightforward:  choose a bounding box or quad tile you are sure will encompass the point in question and all candidate locations, then choose the nearest candidate.  When the distribution is highly uneven, though, the bounding box that works well in rural Montana may return overwhelmingly many results in midtown Manhattan.

We can solve both those problems with a single elegant approach known as Voronoi partitioning.  Given a set of seed locations, the Voronoi partitioning returns a set of polygons with the following properties:

*  The polygon’s ‘partition’ is the space divided such that every piece of the plane belongs to exactly one polygon.
*  There is exactly one polygon for each seed location and all points within it are closer to that seed location than to any other seed location.
*  All points on the boundary of two polygons are equidistant from the two neighboring seed locations; and all vertices where Voronoi polygons meet are equidistant from the respective seed locations.

This effectively precomputes the “nearest x” problem:  For any point in question, find the unique polygon within which it resides (or rarely, the polygon boundaries upon which it lies). Breaking those polygons up by quad tile at a suitable zoom level makes it easy to either store them in HBase (or equivalent) for fast querying or as data files optimized for a spatial JOIN.

It also presents a solution to the spatial sampling problem by assigning the measurements taken at each sample location to its Voronoi region.  You can use these piece-wise regions directly or follow up with some sort of spatial smoothing as your application requires.  Let’s dive in and see how to do this in practice.

==== Finding Nearby Objects

Let’s use the GeoNames dataset to create a “nearest <whatever> to you” application, one that, given a visitor’s geolocation, will return the closest hospital, school, restaurant and so forth.  We will do so by effectively pre-calculating all potential queries; this could be considered overkill for the number of geofeatures within the GeoNames dataset but we want to illustrate an approach that will scale to the number of cell towers, gas stations or anything else.

We will not go into the details of computing a decomposition; most scientific computing libraries have methods to do so and we have included a Python script (TODO: credits), which, when fed a set of locations, returns a set of GeoJSON regions, the Voronoi polygon for each location.

Run the script 'examples Geo Voronoi points to polygons.pi' (TODO: fix up command line).  After a few minutes, it will produce 'output GeoJSON' files.  To see the output (TODO: give instructions for seeing it in browser).

These polygons are pretty but not directly useful; we need a way to retrieve the relevant polygons for a given visitor’s location.  What we will do is store, for every quad key, the truncated Voronoi regions that lie within its quad tile.  We can then turn the position of a visitor into its corresponding quad key, retrieve the set of regions on that quad tile and find the specific region within which it lies.

Pig does not have any built-in geospatial features, so we will have to use a UDF.  In fact, we will reach into the future and use one of the ones you will learn about in the Advanced Pig chapter (TODO:  REF). Here is the script to

----
Register the UDF
Give it an alias
Load the polygons file
Turn each polygon into a bag of quad key polygon metadata tuples
Group by quad key
FOREACH generate the output data structure
Store results
----

Transfer the output of the Voronoi script onto the HDFS and run the above Pig script.  Its output is a set of TSV files in which the first column is a quad key and the second column is a set of regions in GeoJSON format.  We will not go into the details, but the example code shows how to use this to power the nearest x application.  Follow the instructions to load the data into HBase and start the application.

The application makes two types of requests:  One is to determine which polygon is the nearest; it takes the input coordinates and uses the corresponding quad tile to retrieve the relevant regions.  It then calls into a geo library to determine which polygon contains the point and sends a response containing the GeoJSON polygon.  The application also answers direct requests for a quad tile with a straight GeoJSON stored in its database -- exactly what is required to power the drivable "slippy map" widget that is used on the page.  This makes the front end code simple, light and fast, enough that mobile devices will have no trouble rendering it.  If you inspect the Javascript file, in fact, it is simply the slippy map's example with the only customization being the additional query for the region of interest.  It uses the server's response to simply modify the style sheet rule for that portion of the map.

The same data locality advantages that the quad key scheme grants are perhaps even more valuable in a database context, especially ones like HBase that store data in sorted form.  We are not expecting an epic storm of viral interest in this little app but you might be for the applications you write.

The very thing that makes such a flood difficult to manage -- the long-tail nature of the requests -- makes caching a suitable remedy.  You will get a lot more repeated requests for downtown San Francisco than you will for downtown Cheboygan, so those rows will always be hot in memory.  Since those points of lie within compact spatial regions, they also lie within not many more quad key regions, so the number of database blocks contending for cache space is very much smaller than the number of popular quad keys.

It also addresses the short-tail caching problem as well.  When word does spread to Cheboygan and the quad tile for its downtown is loaded, you can be confident requests for nearby tiles driven by the slippy map will follow as well.  Even if those rows are not loaded within the same database block, the quad key helps the operating system pick up the slack -- since this access pattern is so common, when a read causes the OS to go all the way to disk, it optimistically pre-fetches not just the data you requested but a bit of what follows.  When the database gets around to loading a nearby database block, there is a good chance the OS will have already buffered its contents.

The strategies employed here -- precalculating all possible requests, identifying the nature of popular requests, identifying the nature of adjacent requests and organizing the key space to support that adjacency -- will let your database serve large-scale amounts of data with millisecond response times even under heavy load.

.Sidebar:  Choosing A Decomposition Zoom Level
----
When you are decomposing spatial data onto quad tiles, you will face the question of what zoom level or zoom levels to choose.  At some point, coarser (lower indexed) zoom levels will lead to overpopulated tiles, tiles whose record size is unmanageably large; depending on your dataset, this could happen at zoom level 9 (the size of outer London), zoom level 12 (the size of Manhattan south of Central Park) or even smaller.  At the other end, finer zoom levels will produce unjustifiably many boring or empty tiles.

To cover the entire globe at zoom level 13 requires 67 million records, each covering about four kilometers; at zoom level 16, you will need four billion records, each covering about a half kilometer on a side; at zoom level 18, you will need 69 billion records, each covering a city block or so.  To balance these constraints, build a histogram of geofeature counts per quad tile at various zoom levels.  Desirable zoom levels are such that the most populous bin will have acceptable size while the number of bins with low geofeature count are not unmanageably numerous.  Quad keys up to zoom level 16 will fit within a 32-bit unsigned integer; the improved efficiency of storage and computation make a powerful argument for using zoom levels 16 and coarser, when possible.

If the preceding considerations leave you with a range of acceptable zoom levels, choose one in the middle.  If they do not, you will need to use the multiscale decomposition approach (TODO:  REF) described later in this chapter.
----

==== Voronoi Polygons turn Points into Regions

Now, let's use the Voronoi trick to turn a distribution of measurements at discrete points into the distribution over regions it is intended to represent.  In particular, we will take the weather-station-by-weather-station measurements in the NCDC dataset and turn it into an hour-by-hour map of global data.  Spatial distribution of weather stations varies widely in space and over time; for major cities in recent years, there may be many dozens while over stretches of the Atlantic Ocean and in many places several decades ago, weather stations might be separated by hundreds of miles.  Weather stations go in and out of service, so we will have to prepare multiple Voronoi maps.  Even within their time of service, however, they can also go offline for various reasons, so we have to be prepared for missing data.  We will generate one Voronoi map for each year, covering every weather station active within that year, acknowledging that the stretch before and after its time of service will therefore appear as missing data.

In the previous section, we generated the Voronoi region because we were interested in its seed location.  This time, we are generating the Voronoi region because we are interested in the metadata that seed location imputes.  The mechanics are otherwise the same, though, so we will not repeat them here (they are described in the example codes documentation (TODO:  REF).

At this point, what we have are quad tiles with Voronoi region fragments, as in the prior example, and we could carry on from there.  However, we would be falling into the trap of building our application around the source data and not around the user and the application domain.  We should project the data onto regions that make sense for the domain of weather measurements not regions based on where it is convenient to erect a weather vane.

The best thing for the user would be to choose a grid size that matches the spatial extent of weather variations and combine the measurements its weather stations into a consensus value; this will render wonderfully as a heat map of values and since each record corresponds to a full quad cell, will be usable directly by downstream analytics or applications without requiring a geospatial library.  Consulting the quad key grid size cheat sheet (TODO:  REF), zoom level 12 implies 17 million total grid cells that are about five to six miles on a side in populated latitudes, which seems reasonable for the domain.

As such, though, it is not reasonable for the database.  The dataset has reasonably global coverage going back at least 50 years or nearly half a million hours.  Storing 1 KB of weather data per hour at zoom-level 12 over that stretch will take about 7.5 PB but the overwhelming majority of those quad cells are boring.  As mentioned, weather stations are sparse over huge portions of the earth.  The density of measurements covering much of the Atlantic Ocean would be well served by zoom-level 7; at that grid coarseness, 50 years of weather data occupies a mere 7 TB; isn't it nice to be able to say a "mere" 7 TB?

What we can do is use a multi-scale grid.  We will start with a coarsest grain zoom level to partition; 7 sounds good.  In the Reducers (that is, after the group), we will decompose down to zoom-level 12 but stop if a region is completely covered by a single polygon.  Run the multiscale decompose script (TODO: demonstrate it).  The results are as you would hope for; even the most recent year's map requires only x entries and the full dataset should require only x TB.

The stunningly clever key to the multiscale JOIN is, well, the keys.  As you recall, the prefixes of a quad key (shortening it from right to left) give the quad keys of each containing quad tile.  The multiscale trick is to serialize quad keys at the fixed length of the finest zoom level but where you stop early to fill in with an '.' - because it sorts lexicographically earlier than the numerals do.  This means that the lexicographic sort order Hadoop applies in the midstream group-sort still has the correct spatial ordering just as Zorro would have it.

Now it is time to recall how a JOIN works covered back in the Map/Reduce Patterns chapter (TODO:  REF).  The coarsest Reduce key is the JOIN value, while the secondary sort key is the name of the dataset.  Ordinarily, for a two-way join on a key like 012012, the Reducer would buffer in all rows of the form <012012 | A | ...>, then apply the join to each row of the form <012012 | B | ...>.  All rows involved in the join would have the same join key value.  For a multiscale spatial join, you would like rows in the two datasets to be matched whenever one is the same as or a prefix of the other.  A key of 012012 in B should be joined against a key of `0120..`, '01201.' and '012012' but not, of course, against '013...'.

We can accomplish this fairly straightforwardly.  When we defined the multiscale decomposition, we a coarsest zoom level at which to begin decomposing and the finest zoom level which defined the total length of the quad key.  What we do is break the quad key into two pieces; the prefix at the coarsest zoom level (these will always have numbers, never dots) and the remainder (fixed length with some number of quad key digits then some number of dots).  We use the quad key prefix as the partition key with a secondary sort on the quad key remainder then the dataset label.

Explaining this will be easier with some concrete values to use, so let's say we are doing a multiscale join between two datasets partitioning on a coarsest zoom level of 4, and a total quad key length of 6, leading to the following snippet of raw reducer input.

.Snippet of Raw Reducer Input for a Multiscale Spatial Join
----
0120    1.   A
0120    10   B
0120    11   B
0120    12   B
0120    13   B
0120    2.   A
0120    30   B
0121    00   A
0121    00   B
----

As before, the reducer buffers in rows from A for a given key -- in our example, the first of these look like <0120 | 1. | A | ...>. It will then apply the join to each row that follows of the form <0120 | (ANYTHING) | B | ...>.  In this case, the 01201. record from A will be joined against the 012010, 012011, 012012 and 012013 records from B.  Watch carefully what happens next, though.  The following line, for quad key 01202. is from A and so the Reducer clears the JOIN buffer and gets ready to accept records from B to join with it.  As it turns out, though, there is no record from B of the form 01202-anything.  In this case, the 01202. key from A matches nothing in B and the 012030 key in B is matched by nothing in A (this is why it is important the replacement character is lexicographically earlier than the digits; otherwise, you would have to read past all your brothers to find out if you have a parent).  The behavior is the same as that for a regular JOIN in all respects but the one, that JOIN keys are considered to be equal whenever their digit portions match.

The payoff for all this is pretty sweet.  We only have to store and we only have to ship and group-sort data down to the level at which it remains interesting in either dataset.  (TODO: do we get to be multiscale in both datasets?)  When the two datasets meet in the Reducer, the natural outcome is as if they were broken down to the mutually-required resolution.  The output is also efficiently multiscale.

NOTE:  The multiscale keys work very well in HBase too.  For the case where you are storing multiscale regions and querying on points, you will want to use a replacement character that is lexicographically after the digits, say, the letter "x."  To find the record for a given point, do a range request for one record on the interval starting with that point's quad key and extending to infinity (xxxxx…).  For a point with the finest-grain quad key of 012012, if the database had a record for 012012, that will turn up; if, instead, that region only required zoom level 4, the appropriate row (0120xx) would be correctly returned.

==== Smoothing the Distribution

We now have in hand, for each year, a set of multiscale quad tile records with each record holding the weather station IDs that cover it.  What we want to produce is a dataset that has, for each hour and each such quad tile, a record describing the consensus weather on that quad tile.  If you are a meteorologist, you will probably want to take some care in forming the right weighted summarizations -- averaging the fields that need averaging, thresholding the fields that need thresholding and so forth.  We are going to cheat and adopt the consensus rule of "eliminate weather stations with missing data, then choose the weather station with the largest area coverage on the quad tile and use its data unmodified."  To assist that, we made a quiet piece of preparation and have sorted the weather station IDs from largest to smallest in area of coverage, so that the Reducer simply has to choose from among its input records the earliest one on that list.

What we have produced is gold dataset useful for any number of explorations and applications.  An exercise at the end of the chapter (TODO:  REF) prompts you to make a visual browser for historical weather.  Let's take it out for a simple analytical test drive, though.

The tireless members of Retrosheet.org have compiled box scores for nearly every Major League Baseball game since its inception in the late 1800s.  Baseball score sheets typically list the game time weather and wind speed and those fields are included in the Retrosheet data; however, values are missing for many records and since this is hand-entered data, surely many records have coding errors as well.  For example, on October 1, 2006, the home-team Brewers pleased a crowd of 44,133 fans with a 5-3 win over the Cardinals on a wonderful fall day recorded as having game-time temperature of 83 degrees, wind 60 miles per hour out to left field and sunny.  In case you are wondering, 60-mile per hour winds cause 30-foot waves at sea, trees to be uprooted and structural damage to buildings becomes likely, so it is our guess that the scoresheet is, in this respect, wrong.

Let's do a spatial drawing of the Retrosheet data for each game against the weather estimated using the NCDC dataset for that stadium's location at the start of the game; this will let us fill in missing data and flag outliers in the Retrosheet scores.

Baseball enthusiasts are wonderfully obsessive, so it was easy to find online data listing the geographic location of every single baseball stadium -- the file sports/baseball/stadium_geolocations.tsv lists each Retrosheet stadium ID followed by its coordinates and zoom-level 12 quad key.  Joining that on the Retrosheet game logs equips the game log record with the same quad key and hour keys used in the smoothed weather dataset.  (Since the data is so small, we turned parallelism down to 1.)

Next, we will join against the weather data; this data is so large, it is worth making a few optimizations.  First, we will apply the guideline of "join against the smallest amount of data possible."  There are fewer than a hundred quad keys we are interested in over the whole time period of interest and the quad key breakdown only changes year by year, so rather than doing a multiscale join against the full hourly record, we will use the index that gives the quad key breakdown per year to find the specific containing quad keys for each stadium over time.  For example (TODO: find an example where a quad key was at a higher zoom level one year and a lower one a different year).  Doing the multiscale join of stadium quad keys against the weather quad key year gives (TODO: name of file).

Having done the multiscale join against the simpler index, we can proceed using the results as direct keys; no more multiscale magic is required.  Now that we know the specific quad keys and hours, we need to extract the relevant weather records.  We will describe two ways of doing this.  The straightforward way is with a join, in this case of the massive weather quad tile data against the relatively tiny set of quad key hours we are interested in.  Since we do not need multiscale matching any more, we can use Pig and Pig provides a specialized join for the specific case of joining a tiny dataset to a massive one, called the replicated join.  You can skip ahead to the Advanced Pig chapter (TODO:  REF) to learn more about it; for now, all you need to know is that you should put the words "`USING 'replicated'`" at the end of the line, and that the smallest dataset should be on the _right_. (Yes, it's backwards: for replicated joins the smallest should be on the right, while for regular joins it should be on the left.)  This type of join loads the small dataset into memory and simply streams through the larger dataset, so no Reduce is necessary.  It's always a good thing when you can avoid streaming TB of data through the network card when all you want are a few MB.

In this case, there are a few thousand lines in the small dataset, so it is reasonable to do it the honest way, as just described.  In the case where you are just trying to extract a few dozen keys, your authors have been known to cheat by inlining the keys in a filter.  Regular expression engines are much faster than most people realize and are perfectly content to accept patterns with even a few hundred alternations.  An alternative approach here is to take the set of candidate keys, staple them together into a single ludicrous regexp and template it into the PIg script you will run.

.Cheat to Win: Filtering down to only joinable keys using a regexp
----
huge_data = LOAD '...' AS f1, f2, f3;
filtered_data = FILTER huge_data BY MATCH(f1, '^(012012|013000|020111| [...dozens more...])$');
STORE filtered_data INTO '...';
----

==== Results

With just the relevant records extracted, we can compare the score sheet data with the weather data.  Our script lists output columns for the NCDC weather and wind speed, the score sheet weather and wind speed, the distance from the stadium to the relevant weather station and the percentage difference for wind speed and temperature.

It would be an easy mistake to, at this point, simply evict the Retrosheet measurements and replace with the NCDC measurements; we would not argue for doing so.  First, the weather does vary, so there is some danger in privileging the measurement at a weather station some distance away (even if more precise) over a direct measurement at a correct place and time.  In fact, we have far better historical coverage of the baseball data than the weather data.  The weather data we just prepared gives a best-effort estimate of the weather at every quad tile, leaving it in your hands to decide whether to accept a reading from a weather station dozens or hundreds of miles away.  Rather, the philosophically sound action would be to flag values for which the two datasets disagree as likely outliers.

The successful endpoint of most Big Data explorations is a transition to traditional statistical packages and elbow grease -- it shows you've found domain patterns worth exploring. If this were a book about baseball or forensic econometrics, we'd carry forward comparing those outliers with local trends, digging up original entries, and so forth.  Instead, we'll just label them with a scarlet "O" for outlier, drop the mic and walk off stage.
