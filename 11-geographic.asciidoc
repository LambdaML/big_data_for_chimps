[[geographic]]
== Spatial and Geographic Analysis
// TODO: should title be "Spatial Analysis", "Geographic Analysis", "Geospatial Analysis"?

 (for our purposes represented as bounding rectangles, arbitrary polygons and occasionally circles).


* Mechanics:
  - for point, path, polygon, polygon with hole in it:
      - Area, IsClosed, Dimensionality, Number of Coordinates, Number of Points
      - Centroid, Envelope
      - convex hull, simplify, densify, buffer, generalize, offset, transform
      - (polygon should have a hole; a convex hull different than itself and a circle; enough but not too much complexity)
      - exploding shape into points
  - for two polygons, and part is in the hole of a polygon
      - intersect, XOR, a-b, b-a, union)
      -

* Tiles (Sp Agg Pts)
* Decompose Tiles (Sp Agg Regions)
* Z-order Tiles (
* Naive Join Points (9 tiles)
* Point to bbox; sp join
* Spatial Merge Join
* Voronoi
* Weather for Stadium

* **Spatial Aggregation**
  - Smoothing Pointwise Data Locally (Spatial Aggregation of Points)
  - Creating a Spatial Density Map

* **Spatial Data**
  - Points, Paths and Regions
  - Geometry Primitives: Points, Polygons and so forth
  - Longitude and Latitude, Points and Features
  - Always: longitude then latitude (x then y); always: minx, miny, maxx, maxy.
  - GeoJSON, WKT, etc

* **Working with Regions**
  - Smoothing Regional Data onto a Consistent Grid (Spatial Aggregation of Regions)
    - another way to do this later under voronoi
  - **Geographic Projections**
    - Equirectangular, Web-Mercator, Lambert, CoolHat, HEALPix
    - important that it have a well-conditioned inverse
    - CRS, datum, spatial reference
    - concerns are not the same as for other uses

* **Join points**
    * points to tiles, cross points on tile, calculate distances
    * point to geographic radius shape, use contains
    * point to bounding box, index into quadtree, calculate distance

* Mechanics of Spatial Analysis
  -
    - ...
  - Spatial Relationship Tests

* **Grid Tiles and Quad Cells**
  - Exporting data for Presentation by a Tileserver
  - Key Strategic Pattern: Tile / Cull / Process

* Spatial Nearness Join (Within-X-Distance)
  - Matching Nearby Points (Pointwise Spatial Join)
  - Region Within a Geographic Distance of a Geometry
    - Shape; Bounding Box; cautions on spherical geometry

* Matching Points with the Regions


* Nearest-Feature Spatial Join
  - Voronoi Polygons turn Points into Regions
  - Turning Points of Measurements into Regions of Influence
  - Decompose Voronoi Polygons onto Grid Tiles
  - Map Observations to Grid Tiles
  - Joining Stadiums onto Grid Tiles
  - Finding Nearby Objects
  - Smoothing the Distribution
  - Results


* Mapping from earth to grid
* Extending the base of analytic patterns to two and more dimensions

long/lat implies elevation -- For the great majority of questions of interest, elevation is irrelevant, and so we discard it
We just need to rough-cut the data and then turn it over to dedicated spatial methods


Our approach will be consistently to
apply the toolkit of Analytic Patterns that we assembled in Chapters 4-9 (REF)
to put all relevant data into context,
and then turn to specialized geospatial libraries to
synthesize results

The large-scale part of this demands no great sophistication
We can pretend that circles are rectangular, that shapes do not ever have holes, that the earth is not only a perfect sphere but in fact is a planar grid. All manner of convenient distortions
so outrageous they literally tear the space-time continuum
are allowable as long as they obey the fundamental strategic rule:

* put all data that might form relevant context together

(TODO: better phrasing)

==== Smoothing Pointwise Data Locally (Spatial Aggregation of Points)


We will start, as we always do, by applying patterns that turn Big Data into Much a Less Data. In particular,
A great tool for visualizing a large spatial data set


* You want to "wash out" everything but the spatial variation -- even though the data was gathered for each
* Point measurement of effect with local extent -- for example, the temperature measured at a weather station is understood to be representative of the weather for several surrounding miles.
*
*
* data reduction, especially for a heatmap visualization;
* extracting a continuous measurement from a pointwise sample;
* providing a common basis for comparison of multiple datasets;
* smoothing out spatial variation;
* for all the other reasons you aggregate groups of related values in context
* You have sampled data at points in order to estimate something with spatial extent. The weather dataset is an example:
* Data that manifests at a single point
  represents a process with
  For example, the number of airline passengers in and out of the major airport
  are travelling to and from local destinations
* Smoothing pointwise data
  into a
  easier to compare or manage
* continuous approximation
  represents just the variation due to spatial
  variables

The straightforward approach we'll take is to divide the world up into a grid of tiles and map the position of each point onto the unique grid tile it occupies. We can then group on each tile

Area of a spherical segment is 2*pi*R*h --
so for lat from equator to 60

------
%default binsz 2.0
-- place into half-degree bins -- ~ 120x50 cells for US
gridded = FOREACH sightings GENERATE
    FLOOR(lng * $binsz) / $binsz AS bin_x,
    FLOOR(lat * $binsz) / $binsz AS bin_y;
-- number density
grid_cts = FOREACH (GROUP gridded BY (bin_x, bin_y))
  GENERATE
    group.bin_x, group.bin_y,
    COUNT_STAR(gridded) AS ct;
------

* US:	-125 24 to -66, 50	(-124.7625, 24.5210, -66.9326, 49.3845) -- about 60 x 26

==== Creating a Spatial Density Map

Map points to quad cells, plot number density of airports as a heat map

Then geonames places -- show lakes and streams (or something nature-y) vs something urban-y

(just call out that rollup, summing trick, or group-decorate-flatten would work: do no pursue)

Do that again, but for a variable: airport flight volume -- researching
epidemiology

// FAA flight data http://www.faa.gov/airports/planning_capacity/passenger_allcargo_stats/passenger/media/cy07_primary_np_comm.pdf

We can plot the number of air flights handled by every airport

------
%default binsz 2.0
-- place into half-degree bins -- ~ 120x50 cells for US
gridded = FOREACH sightings GENERATE
    FLOOR(lng * $binsz) / $binsz AS bin_x,
    FLOOR(lat * $binsz) / $binsz AS bin_y,
    n_flights;
-- number density
grid_cts = FOREACH (GROUP gridded BY (bin_x, bin_y))
  GENERATE
    group.bin_x, group.bin_y,
    COUNT_STAR(gridded) AS ct,
    SUM(n_flights) AS tot_flights;
------

An epidemiologist or transportation analyst interested in knowing the large-scale flux of people could throughout the global transportation network


Our reindeer friends are deflated to learn that the two maps do not resemble each other.

===== Pattern Recap: Spatial Aggregation of Points

* _Generic Example_ -- group on tile cell, then apply the appropriate aggregation function
* _When You'll Use It_ -- as mentioned above: summarizing data; converting point samples into a continuous value; smoothing out spatial variation; reassigning spatial data to grid-aligned regions
* _Exercises_ --
* _Important to Know_ --
  - A https://en.wikipedia.org/wiki/Dot_distribution_map[Dot Distribution Map] is in some sense the counterpart to a spatial average -- turning data over a region into data at synthesized points


==== Smoothing Regional Data onto a Consistent Grid (Spatial Aggregation of Regions)


...

===== Pattern in Use

* _Where You'll a Use It_:



=== Geographic Data Model ===

==== Geometry Primitives: Points, Polygons and so forth

Geographic data shows up in the form of

* Points -- a pair of coordinates. When given as an ordered pair (a "Position"), always use `[longitude,latitude]` in that order, matching the familiar `X,Y` order for mathematical points. When it's a point with other metadata, it's a Place footnote:[in other works you'll see the term Point of Interest ("POI") for a place.], and the coordinates are named fields.
* Paths -- an array of points `[[longitude,latitude],[longitude,latitude],...]`
* Region -- an array of paths, understood to connect and bound a region of space. `[ [[longitude,latitude],[longitude,latitude],...], [[longitude,latitude],[longitude,latitude],...]]`. Your array will be of length one unless there are holes or multiple segments
* "Bounding Box" (or `bbox`) -- a rectangular bounding region, `[-5.0, 30.0, 5.0, 40.0]`

Back in Chapter 4 (REF), we introduced the simple scalar types (numbers, strings, etc.) and three complex types (`tuple`, `bag`, and `map`). Since every spatial analysis exploration involves

One thing
Spatial analysis libraries
rely on the http://www.opengeospatial.org/[OGC (Open Geospatial Consortium)]

Geometry

Point, LineString, Polygon; and corresponding multi-part geometries MultiPoint, MultiLineString, MultiPolygon.

Behind these smiling friendly inviting abstraction
lies
a host of diabolical complexities

In regular usage, even double-precision floating-point math can introduce
discrepancies large enough to incalidate results
or present visual artifacts
-- pushing the boundary of a shape off the shape itself, causing tears or overlaps where there were none, turning small polygons into degenerate points, introduce numerical instability

But for the big data section of it, where we are chiefly concerned with relating data in context,
there are really only these

* a point in space
* a spatial extent -- paths, regions, etc
// * non-spatial data

In fact, we can go even farther:

* points
* rectangles

Remember, all we're trying to do is land all (possibly) related data onto the same reducer before we bring in the big guns.


=== Spatial Nearness Join (Within-X-Distance)

* quad cells have different sizes; lookup table

All sightings near airport

(dispatch to 9 nearest you)

==== Matching Nearby Points (Pointwise Spatial Join)

* Common sense tells you that a weather observation is generally valid for places within a few kilometers, but certainly not useful for places Hundreds of kilometers away. It would be useful to have a more precise guideline for the distance where a weather measurement should not be considered reliable.
    * first find all pairs of weather stations within 50 km of each other. Emit each pair of IDs along with their distance: put the lower-numbered ID in the first slot (making it easy to ensure uniqueness).
    * for each such pair, take a year of weather observations and determine the difference in temperature measurements taken at the same hour
        * HashMap (replicated) join of station-station pairs on the observations table. You could also do a total sort of the pairs table and use a merge-join if you're memory constrained.
        * join the resulting table back onto the observations table.
        * (In this case, most weather stations are a part of at least one pair, and so most of the rows in the observations table are retained. If that weren't the case,
            * if most elements on the left are also not elements on the right, do a second semi-join to filter for observations that are on the right of some pair. That is, one join to get the observation-pairs and a second HashMap (fragment-replicate) join on ids that are the right member of some pair. (Most people on an auction site are buyers or sellers, though a very few are both.) (?if the pairs were from airports to not-airports)
            * if as in this case, most weather stations are reasonably likely to be on either side, do a semi-join of observations against all distinct ids that are on either side of a pair. This means a single HashMap join against he huge table and then
            * Cogroup observations with id_a-sorted-pairs by id_a, id_b-sorted-pairs on id_b. (Preparing the tables in sorted form lets you use merge. Flatten (left_id, right_id, temp, distance) to get the left-observations. Filter for rows with at least one right id and project `(id_b, temp)` to get the semi-join. Don't flatten for this second table -- you want to join this table with one row per observation to the table with one row per observation-pair.

    * As the radius expands, you'll quickly find that the amount of data begins to explode, so restrict that upper radius band initially.
    * (is this also a problem: "You might have also noticed another problem. Even apart from a distance effect, with more neighbors there are more opportunities for observations to disagree.")
    *
    * (you should know that the answer has some bias -- places with a large concentration of weather stations are typically heavily populated, and heavily populated places don't tend to have extreme weather. We're just looking for a good rule-of-thumb though)


=== Projections and Tiling Schemes


* Equal-area:
  - features uniformly distributed on the globe will be uniformly distributed among grid cells.
* Platte-Careé (Equirectangular)
  - Extremely simple to compute
  - Plot directly into screen coordinates with
  -

==== Exporting data for Presentation by a Tileserver

The most commonly

Features following a constant bearing in any direction -- Manhattan's Broadway, or the borderlines of Algeria or Nevada -- remain straight lines on the map. This is important for navigational purposes

The locality properties of quadtiles indexing



Typically you will store the shape clipped to the given quad-tile.
 metadata about that region -- population, metric tons of bananas exported annually, an image of its flag, lyrics to its national anthem -- is stored under the same key but in independent columns footnote:[typically the regions are heavyweight, heavily requested and read-only, so they deserve their own table or at least their own column family.



You don't have to break
What we do is
When tile 0231_1 is requested

    0230_103 us (California
    0230_11X us
    0230_2XX pacific
    0230_30*
    0320_31*
    0320_32X
    0320_33*
    0231_0XX  US
    0231_1xx  US
    0231_2**  Mexico, SW US, pacific
    0231_3**  Mexico, SW US, Gulf of Mexico
    0231_322     Monterey
    0231_323     Tampico
    0231_33      Caribbean east of Mexico
    0232_3333 4  between Hawaii and Baka calif

Querying on the ZL-5 tile 0231_0 (using key 0231_033), any of its four children (using keys 0231_003, 0231_013, 0231_023, or 0231_033) or any of the sixteen ZL-7 descendants 0231_000 through 0231_033 will retrieve a single tile. Querying on its parent 0231 will return tiles for 0231_0 and 0231_1, and a collection of zoom level 6 and 7 tiles covering the southwest US, Mexico and the Gulf of Mexico.

TODO: screenshot.

Our data is stored with no duplication, but


Its easy to decompose or clip a super-tile to a requested (finer) zoom level. The longitude just divides normally along the tile: a hypothetical tile from 16 to 32 would have spatial coordinates 16, 18, 20, .... The latitude cut points do not subdivide directly, but only need to be calculated for one edge: if tile 0230_00 has bottom edge XX latitude, so does tile 0230_01, 0230_10, 0231_11, and others in its horizontal grid row.

==== Finding the Centroid of an Extent



==== Finding the Bounding Box of an Extent


==== Finding the Bounding Box of Points Within a Radius


==== Combining Regions with Set Operations

(intersection, union, diff, xor)

==== Testing the Relationship of two Regions

DE-9IM

equals
disjoint
touches
contains
covers

intersects,
within
covered_by

crosses
overlaps

From Wikipedia:

	Equals:   a = b    that is    (a ∩ b = a) ∧ (a ∩ b = b)
	Within:   a ∩ b = a
	Intersects:   a ∩ b ≠ ∅
	Touches:   (a ∩ b ≠ ∅) ∧ (aο ∩ bο = ∅)

	point/point	Equals, Disjoint	Other valid predicates collapses into Equals.
	point/line	adds Intersects	Intersects is a flexibilization of Equals, "some equal point at the line".
	line/line	adds Touches, Crosses, ...	Touches is a constraint of Intersects, about "only boundaries"; Crosses about "only one point".

{0,1,2,T,F,*} -- dimensions 0, 1, 2; T / F; dont-care



=== Key Strategic Pattern: Tile / Cull / Process


* _Tile_    -- tile the grid
* _Cull_    -- eliminate
* _Process_ --

=== Matching Points in a Table with Nearby Points in Another (Spatial Join)


* scatter points to nine tiles


=== Matching Points with the Regions




=== Mechanics of Geographic Data

==== Longitude and Latitude, Points and Features

* floating point vs decimal -- The level of precision we're working with here doesn't justify giving up the benefits of a direct representation.

==== GeoJSON


* OpenDataLab POJOs for Jackson
  - https://github.com/opendatalab-de/geojson-jackson

* GeoTools http://www.geotools.org/
  -

* ESRI library:
  - https://github.com/Esri/geometry-api-java -- The Esri Geometry API for Java enables developers to write custom applications for analysis of spatial data. This API is used in the Esri GIS Tools for Hadoop and other 3rd-party data processing solutions.
  - https://github.com/Esri/spatial-framework-for-hadoop
*

=== Spatial Nearest-Feature Join


You might not expect it, but it can be more complex to match shapes with their _nearest_ point than to match shapes with _all nearby_ points.


In New York City, you'd be disappointed to learn that the nearest Starbucks was more than a few blocks away; in rural Montana, you'd be pleased to learn that one opened up less than an hour's drive away. http://www.ifweassume.com/2012/10/the-united-states-of-starbucks.html
Naively working at the coarse grain of Montana will pour every coffee joint in the big apple onto the same reducer, naively working on the fine grain of New York City will split Montana into a wasteful number of empty fragments that contain no coffee shops at all.
There's a wonderful tool we can borrow from our mathematician friends called a


==== Voronoi Cells

How do we extend region of a
How would you help find the nearest 7-11?
  -- one way would be to look for stores within X distance,
  but a customer in the western US might be excited to learn there's one within an hour's drive,
  while that same radius centered on Manhattan would require sorting through thousande




==== Breaking Regions into Quad Cells

* recursively decompose a region on quadcells

(what do numbers look like doing this for the US, daily)


==== Map Polygons to Grid Tiles



              +----------------------------+
              |                            |
              |              C             |
              |      ~~+---------\         |
              |     /  |          \       /
              |    /   |           \     /|
              |   /    |            \   / |
               \ /     |     B       \ /  |
                |      |              |   |
                |  A   +--------------'   |
                |      |                  |
                |      |     D            /
                |      |               __/
                 \____/ \             |
                         \____________,


            +-+-----------+-------------+--+------
            | |           |             |  |
            | |           |         C   |  |
      000x  | |   C  ~~+--+------\      |  |      0100
            | |     / A|B |  B    \     | /
            |_|____/___|__|________\____|/|_______
            | | C /    |  |         \ C / |
            |  \ /     |B |  B       \ /| |
      001x  |   |      |  |           | |D|       0110
            |   |  A   +--+-----------' | |
            |   |      |D |  D          | |
            +---+------+--+-------------+-/-------
            |   |  A   |D |            _|/
            |    \____/ \ |    D      | |
      100x  |            \|___________, |         1100
            |             |             |
            |             |             |
            +-------------+-------------+---------
                ^ 1000        ^ 1001

* Tile 0000: `[A, B, C   ]`
* Tile 0001: `[   B, C   ]`
* Tile 0010: `[A, B, C, D]`
* Tile 0011: `[   B, C, D]`

* Tile 0100: `[      C,  ]`
* Tile 0110: `[      C, D]`

* Tile 1000: `[A,       D]`
* Tile 1001: `[         D]`
* Tile 1100: `[         D]`

For each grid, also calculate the area each polygon covers within that grid.

Pivot:

* A:          `[ 0000       0010                   1000          ]`
* B:          `[ 0000 0001 0010 0011                             ]`
* C:          `[ 0000 0001 0010 0011 0100 0110                   ]`
* D:          `[             0010 0011       0110 1000 1001 1100 ]`



==== Joining Stadiums onto Quad Cells

join games on parks_info to get location, quadkey

foreach stadiums to get months in action and quadkey
join stadiums by (quadkey, month), weather voronois by (quadkey, month)
find actual nearest weather station for that stadium

now park_info has month, quadkey, weather station, date, parkid
join park_info on games -- get game_wstns (game_id, scorecard weather, scorecard_wind, park_id, wstn_id)
join game_wstns on wobs to get game_weather


A bounding box around the

* Continental US: `-125.0011, 24.9493, -66.9326, 49.5904`; centroid: `-95.9669, 37.1669`.
* Alaska: `179.1506, 51.2097, -129.9795, 71.4410


=== Multi-Scale Spatial Data

If we want to combine weather

==== Breaking regions into Multi-Cell Quads


==== Adaptive Grid Size

The world is a big place, but we don't use all of it the same. Most of the world is water. Lots of it is Siberia. Half the tiles at zoom level 2 have only a few thousand inhabitantsfootnote:[000 001 100 101 202 203 302 and 303].

Suppose you wanted to store a "what country am I in" dataset -- a geo-joinable decomposition of the region boundaries of every country. You'll immediately note that Monaco fits easily within on one zoom-level 12 quadtile; Russia spans two zoom-level 1 quadtiles. Without multiscaling, to cover the globe at 1-km scale and 64-kB records would take 70 terabytes -- and 1-km is not all that satisfactory. Huge parts of the world would be taken up by grid cells holding no border that simply said "Yep, still in Russia".

There's a simple modification of the grid system that lets us very naturally describe multiscale data.

The figures (REF: multiscale images) show the quadtiles covering Japan at ZL=7. For reasons you'll see in a bit, we will split everything up to at least that zoom level; we'll show the further decomposition down to ZL=9.

image::images/fu05-quadkeys-multiscale-ZL7.png[Japan at Zoom Level 7]

Already six of the 16 tiles shown don't have any land coverage, so you can record their values:

    1330000xx  { Pacific Ocean }
    1330011xx  { Pacific Ocean }
    1330013xx  { Pacific Ocean }
    1330031xx  { Pacific Ocean }
    1330033xx  { Pacific Ocean }
    1330032xx  { Pacific Ocean }

Pad out each of the keys with `x`'s to meet our lower limit of ZL=9.

The quadkey `1330011xx` means "I carry the information for grids `133001100`, `133001101`, `133001110`, `133001111`, ".

image::images/fu05-quadkeys-multiscale-ZL8.png[Japan at Zoom Level 8]

image::images/fu05-quadkeys-multiscale-ZL9.png[Japan at Zoom Level 9]

You should uniformly decompose everything to some upper zoom level so that if you join on something uniformly distributed across the globe you don't have cripplingly large skew in data size sent to each partition.  A zoom level of 7 implies 16,000 tiles -- a small quantity given the exponential growth of tile sizes

With the upper range as your partition key, and the whole quadkey is the sort key, you can now do joins. In the reducer,

* read keys on each side until one key is equal to or a prefix of the other.
* emit combined record using the more specific of the two keys
* read the next record from the more-specific column,  until there's no overlap

Take each grid cell; if it needs subfeatures, divide it else emit directly.

You must emit high-level grid cells with the lsb filled with XX or something that sorts after a normal cell; this means that to find the value for a point,

* Find the corresponding tile ID,
* Index into the table to find the first tile whose ID is larger than the given one.

     00.00.00
     00.00.01
     00.00.10
     00.00.11
     00.01.--
     00.10.--
     00.11.00
     00.11.01
     00.11.10
     00.11.11
     01.--.--
     10.00.--
     10.01.--
     10.10.01
     10.10.10
     10.10.11
     10.10.00
     10.11.--


==== Tree structure of Quadtile indexing

You can look at quadtiles is as a tree structure. Each branch splits the plane exactly in half by area, and only leaf nodes hold data.

The first quadtile scheme required we develop every branch of the tree to the same depth. The multiscale quadtile scheme effectively says "hey, let's only expand each branch to its required depth". Our rule to break up a quadtile if any section of it needs development preserves the "only leaf nodes hold data". Breaking tiles always exactly in two makes it easy to assign features to their quadtile and facilitates joins betweeen datasets that have never met. There are other ways to make these tradeoffs, though -- read about K-D trees in the "keep exploring" section at end of chapter.





* _choose exemplars_:
  - Midway, because it's large; Austin, because it's one of our exemplar cities; and (TODO something tiny) because it's very small.
  - the sightings X, y, which each have a fun description and are near multiple airports; and Z, which is not near an airport.
  - weather observations:
      - a date with a new moon and a full moon; 8/8/08, because auspicious; an equinox and a solstice
  -

What makes a good exemplar?
* Head-of-the-tail --
    * extreme specimens will pop on their own. You want to see what's happening to the
* Ones that are unusual without being weird. The solstice is
* Essential troublemakers: leap years, the centennial leap-year-exceptions, and the quad-centennial leap-year-exception-exceptions.
* Well represented
    * it's no fun if your exemplars disappear mid-journey -- most commonly because they failed to find a match during a join.
* Chosen by out-of-band criteria -- deciding to look for "this date three years ago" and then finding a record is better than choosing the first record you see -- that particular record may have been the first one you saw because it is unrepresentative in some way.
    * just as a magician will pull back their shirtsleeves to show they have no rabbit concealed within, this keeps you from fooling yourself. http://en.wikipedia.org/wiki/Nothing_up_my_sleeve_number
    * (in fact, Cryptographers have a concept of a "nothing-up-my-sleeve" number: when a large arbitrary collection of numbers is needed, choosing the first twenty-five digits of Pi is believably arbitrary, whereas choosing the 387'th through 412'th digits raises the specter of a purposeful "backdoor").


===  Turning Points of Measurements Into Regions of Influence

Frequently, geospatial data is, for practical reasons, sampled at discrete points but should be understood to represent measurements at all points in space.  For example, the measurements in the NCDC datasets are gathered at locations chosen for convenience or value -- in some cases, neighboring stations are separated by blocks, in other cases by hundreds of miles.  It is useful to be able to reconstruct the underlying spatial distribution from point-sample measurements.

Given a set of locations -- broadcast towers, 7-11 stores, hospitals -- it is also useful to be able to determine, for any point in space, which of those objects is nearest.  When the distribution of objects is even, this is straightforward:  choose a bounding box or quad tile you are sure will encompass the point in question and all candidate locations, then choose the nearest candidate.  When the distribution is highly uneven, though, the bounding box that works well in rural Montana may return overwhelmingly many results in midtown Manhattan.

We can solve both those problems with a single elegant approach known as Voronoi partitioning.  Given a set of seed locations, the Voronoi partitioning returns a set of polygons with the following properties:

*  The polygon’s ‘partition’ is the space divided such that every piece of the plane belongs to exactly one polygon.
*  There is exactly one polygon for each seed location and all points within it are closer to that seed location than to any other seed location.
*  All points on the boundary of two polygons are equidistant from the two neighboring seed locations; and all vertices where Voronoi polygons meet are equidistant from the respective seed locations.

This effectively precomputes the “nearest x” problem:  For any point in question, find the unique polygon within which it resides (or rarely, the polygon boundaries upon which it lies). Breaking those polygons up by quad tile at a suitable zoom level makes it easy to either store them in HBase (or equivalent) for fast querying or as data files optimized for a spatial JOIN.

It also presents a solution to the spatial sampling problem by assigning the measurements taken at each sample location to its Voronoi region.  You can use these piece-wise regions directly or follow up with some sort of spatial smoothing as your application requires.  Let’s dive in and see how to do this in practice.

==== Finding Nearby Objects

Let’s use the GeoNames dataset to create a “nearest <whatever> to you” application, one that, given a visitor’s geolocation, will return the closest hospital, school, restaurant and so forth.  We will do so by effectively pre-calculating all potential queries; this could be considered overkill for the number of geofeatures within the GeoNames dataset but we want to illustrate an approach that will scale to the number of cell towers, gas stations or anything else.

We will not go into the details of computing a decomposition; most scientific computing libraries have methods to do so and we have included a Python script (TODO: credits), which, when fed a set of locations, returns a set of GeoJSON regions, the Voronoi polygon for each location.

Run the script 'examples Geo Voronoi points to polygons.pi' (TODO: fix up command line).  After a few minutes, it will produce 'output GeoJSON' files.  To see the output (TODO: give instructions for seeing it in browser).

These polygons are pretty but not directly useful; we need a way to retrieve the relevant polygons for a given visitor’s location.  What we will do is store, for every quad key, the truncated Voronoi regions that lie within its quad tile.  We can then turn the position of a visitor into its corresponding quad key, retrieve the set of regions on that quad tile and find the specific region within which it lies.

Pig does not have any built-in geospatial features, so we will have to use a UDF.  In fact, we will reach into the future and use one of the ones you will learn about in the Advanced Pig chapter (TODO:  REF). Here is the script to

----
Register the UDF
Give it an alias
Load the polygons file
Turn each polygon into a bag of quad key polygon metadata tuples
Group by quad key
FOREACH generate the output data structure
Store results
----

Transfer the output of the Voronoi script onto the HDFS and run the above Pig script.  Its output is a set of TSV files in which the first column is a quad key and the second column is a set of regions in GeoJSON format.  We will not go into the details, but the example code shows how to use this to power the nearest x application.  Follow the instructions to load the data into HBase and start the application.

The application makes two types of requests:  One is to determine which polygon is the nearest; it takes the input coordinates and uses the corresponding quad tile to retrieve the relevant regions.  It then calls into a geo library to determine which polygon contains the point and sends a response containing the GeoJSON polygon.  The application also answers direct requests for a quad tile with a straight GeoJSON stored in its database -- exactly what is required to power the drivable "slippy map" widget that is used on the page.  This makes the front end code simple, light and fast, enough that mobile devices will have no trouble rendering it.  If you inspect the Javascript file, in fact, it is simply the slippy map's example with the only customization being the additional query for the region of interest.  It uses the server's response to simply modify the style sheet rule for that portion of the map.

The same data locality advantages that the quad key scheme grants are perhaps even more valuable in a database context, especially ones like HBase that store data in sorted form.  We are not expecting an epic storm of viral interest in this little app but you might be for the applications you write.

The very thing that makes such a flood difficult to manage -- the long-tail nature of the requests -- makes caching a suitable remedy.  You will get a lot more repeated requests for downtown San Francisco than you will for downtown Cheboygan, so those rows will always be hot in memory.  Since those points of lie within compact spatial regions, they also lie within not many more quad key regions, so the number of database blocks contending for cache space is very much smaller than the number of popular quad keys.

It also addresses the short-tail caching problem as well.  When word does spread to Cheboygan and the quad tile for its downtown is loaded, you can be confident requests for nearby tiles driven by the slippy map will follow as well.  Even if those rows are not loaded within the same database block, the quad key helps the operating system pick up the slack -- since this access pattern is so common, when a read causes the OS to go all the way to disk, it optimistically pre-fetches not just the data you requested but a bit of what follows.  When the database gets around to loading a nearby database block, there is a good chance the OS will have already buffered its contents.

The strategies employed here -- precalculating all possible requests, identifying the nature of popular requests, identifying the nature of adjacent requests and organizing the key space to support that adjacency -- will let your database serve large-scale amounts of data with millisecond response times even under heavy load.

.Sidebar:  Choosing A Decomposition Zoom Level
----
When you are decomposing spatial data onto quad tiles, you will face the question of what zoom level or zoom levels to choose.  At some point, coarser (lower indexed) zoom levels will lead to overpopulated tiles, tiles whose record size is unmanageably large; depending on your dataset, this could happen at zoom level 9 (the size of outer London), zoom level 12 (the size of Manhattan south of Central Park) or even smaller.  At the other end, finer zoom levels will produce unjustifiably many boring or empty tiles.

To cover the entire globe at zoom level 13 requires 67 million records, each covering about four kilometers; at zoom level 16, you will need four billion records, each covering about a half kilometer on a side; at zoom level 18, you will need 69 billion records, each covering a city block or so.  To balance these constraints, build a histogram of geofeature counts per quad tile at various zoom levels.  Desirable zoom levels are such that the most populous bin will have acceptable size while the number of bins with low geofeature count are not unmanageably numerous.  Quad keys up to zoom level 16 will fit within a 32-bit unsigned integer; the improved efficiency of storage and computation make a powerful argument for using zoom levels 16 and coarser, when possible.

If the preceding considerations leave you with a range of acceptable zoom levels, choose one in the middle.  If they do not, you will need to use the multiscale decomposition approach (TODO:  REF) described later in this chapter.
----

=== Weather Near You

The weather station data is sampled at each weather station, and forms our best estimate for the surrounding region's weather.

So weather data is gathered at a _point_, but imputes information about a _region_. You can't just slap each point down on coarse-grained tiles -- the closest weather station might lie just over on the next quad, and you're writing a check for very difficult calculations at run time.

We also have a severe version of the multiscale problem.  The coverage varies wildly over space: a similar number of weather stations cover a single large city as cover the entire Pacific ocean. It also varies wildly over time: in the 1970s, the closest weather station to Austin, TX was about 150 km away in San Antonio. Now, there are dozens in Austin alone.


==== Find the Voronoi Polygon for each Weather Station
////I think readers will need for you to draw out what's key here.  Say why this matters.  Connect the dots for readers.  This is important for them to grasp.  Amy////

These factors rule out any naïve approach to locality, but there's an elegant solution known as a Voronoi diagram footnote:[see http://en.wikipedia.org/wiki/Voronoi_diagram[Wikipedia entry] or (with a Java-enabled browser) this http://www.cs.cornell.edu/home/chew/Delaunay.html[Voronoi Diagram applet]].

The Voronoi diagram covers the plane with polygons, one per point -- I'll call that the "centerish" of the polygon. Within each polygon, you are closer to its centerish than any other. By extension, locations on the boundary of each Voronoi polygon are equidistant from the centerish on either side; polygon corners are equidistant from centerishes of all touching polygons footnote:[John Snow, the father of epidemiology, mapped cholera cases from an 1854 outbreak against the voronoi regions defined by each neighborhood's closest water pump. The resulting infographic made plain to contemporary physicians and officials that bad drinking water, not "miasma" (bad air), transmitted cholera. http://johnsnow.matrix.msu.edu/book_images12.php].

If you'd like to skip the details, just admire the diagram (REF) and agree that it's the "right" picture. As you would in practice, we're going to use vetted code from someone with a PhD and not write it ourselves.

The details: Connect each point with a line to its neighbors, dividing the plane into triangles; there's an efficient alorithm (http://en.wikipedia.org/wiki/Delaunay_triangulation[Delaunay Triangulation]) to do so optimally. If I stand at the midpoint of the edge connecting two locations, and walk perpendicular to the edge in either direction, I will remain equidistant from each point. Extending these lines defines the Voronoi diagram -- a set of polygons, one per point, enclosing the area closer to that point than any other.

<remark>TODO: above paragraph not very clear, may not be necessary.</remark>

==== Break polygons on quadtiles

Now let's put Mr. Voronoi to work. Use the weather station locations to define a set of Voronoi polygons, treating each weather station's observations as applying uniformly to the whole of that polygon.

Break the Voronoi polygons up by quadtile as we did above -- quadtiles will either contain a piece of boundary (and so are at the lower-bound zoom level), or are entirely contained within a boundary. You should choose a lower-bound zoom level that avoids skew but doesn't balloon the dataset's size.

Also produce the reverse mapping, from weather station to the quadtile IDs its polygon covers.

==== Map Observations to Grid Cells

Now join observations to grid cells and reduce each grid cell.

==== Voronoi Polygons turn Points into Regions

Now, let's use the Voronoi trick to turn a distribution of measurements at discrete points into the distribution over regions it is intended to represent.  In particular, we will take the weather-station-by-weather-station measurements in the NCDC dataset and turn it into an hour-by-hour map of global data.  Spatial distribution of weather stations varies widely in space and over time; for major cities in recent years, there may be many dozens while over stretches of the Atlantic Ocean and in many places several decades ago, weather stations might be separated by hundreds of miles.  Weather stations go in and out of service, so we will have to prepare multiple Voronoi maps.  Even within their time of service, however, they can also go offline for various reasons, so we have to be prepared for missing data.  We will generate one Voronoi map for each year, covering every weather station active within that year, acknowledging that the stretch before and after its time of service will therefore appear as missing data.

In the previous section, we generated the Voronoi region because we were interested in its seed location.  This time, we are generating the Voronoi region because we are interested in the metadata that seed location imputes.  The mechanics are otherwise the same, though, so we will not repeat them here (they are described in the example codes documentation (TODO:  REF).

At this point, what we have are quad tiles with Voronoi region fragments, as in the prior example, and we could carry on from there.  However, we would be falling into the trap of building our application around the source data and not around the user and the application domain.  We should project the data onto regions that make sense for the domain of weather measurements not regions based on where it is convenient to erect a weather vane.

The best thing for the user would be to choose a grid size that matches the spatial extent of weather variations and combine the measurements its weather stations into a consensus value; this will render wonderfully as a heat map of values and since each record corresponds to a full quad cell, will be usable directly by downstream analytics or applications without requiring a geospatial library.  Consulting the quad key grid size cheat sheet (TODO:  REF), zoom level 12 implies 17 million total grid cells that are about five to six miles on a side in populated latitudes, which seems reasonable for the domain.

As such, though, it is not reasonable for the database.  The dataset has reasonably global coverage going back at least 50 years or nearly half a million hours.  Storing 1 KB of weather data per hour at zoom-level 12 over that stretch will take about 7.5 PB but the overwhelming majority of those quad cells are boring.  As mentioned, weather stations are sparse over huge portions of the earth.  The density of measurements covering much of the Atlantic Ocean would be well served by zoom-level 7; at that grid coarseness, 50 years of weather data occupies a mere 7 TB; isn't it nice to be able to say a "mere" 7 TB?

What we can do is use a multi-scale grid.  We will start with a coarsest grain zoom level to partition; 7 sounds good.  In the Reducers (that is, after the group), we will decompose down to zoom-level 12 but stop if a region is completely covered by a single polygon.  Run the multiscale decompose script (TODO: demonstrate it).  The results are as you would hope for; even the most recent year's map requires only x entries and the full dataset should require only x TB.

The stunningly clever key to the multiscale JOIN is, well, the keys.  As you recall, the prefixes of a quad key (shortening it from right to left) give the quad keys of each containing quad tile.  The multiscale trick is to serialize quad keys at the fixed length of the finest zoom level but where you stop early to fill in with an '.' - because it sorts lexicographically earlier than the numerals do.  This means that the lexicographic sort order Hadoop applies in the midstream group-sort still has the correct spatial ordering just as Zorro would have it.

Now it is time to recall how a JOIN works covered back in the Map/Reduce Patterns chapter (TODO:  REF).  The coarsest Reduce key is the JOIN value, while the secondary sort key is the name of the dataset.  Ordinarily, for a two-way join on a key like 012012, the Reducer would buffer in all rows of the form <012012 | A | ...>, then apply the join to each row of the form <012012 | B | ...>.  All rows involved in the join would have the same join key value.  For a multiscale spatial join, you would like rows in the two datasets to be matched whenever one is the same as or a prefix of the other.  A key of 012012 in B should be joined against a key of `0120..`, '01201.' and '012012' but not, of course, against '013...'.

We can accomplish this fairly straightforwardly.  When we defined the multiscale decomposition, we a coarsest zoom level at which to begin decomposing and the finest zoom level which defined the total length of the quad key.  What we do is break the quad key into two pieces; the prefix at the coarsest zoom level (these will always have numbers, never dots) and the remainder (fixed length with some number of quad key digits then some number of dots).  We use the quad key prefix as the partition key with a secondary sort on the quad key remainder then the dataset label.

Explaining this will be easier with some concrete values to use, so let's say we are doing a multiscale join between two datasets partitioning on a coarsest zoom level of 4, and a total quad key length of 6, leading to the following snippet of raw reducer input.

.Snippet of Raw Reducer Input for a Multiscale Spatial Join
----
0120    1.   A
0120    10   B
0120    11   B
0120    12   B
0120    13   B
0120    2.   A
0120    30   B
0121    00   A
0121    00   B
----

As before, the reducer buffers in rows from A for a given key -- in our example, the first of these look like <0120 | 1. | A | ...>. It will then apply the join to each row that follows of the form <0120 | (ANYTHING) | B | ...>.  In this case, the 01201. record from A will be joined against the 012010, 012011, 012012 and 012013 records from B.  Watch carefully what happens next, though.  The following line, for quad key 01202. is from A and so the Reducer clears the JOIN buffer and gets ready to accept records from B to join with it.  As it turns out, though, there is no record from B of the form 01202-anything.  In this case, the 01202. key from A matches nothing in B and the 012030 key in B is matched by nothing in A (this is why it is important the replacement character is lexicographically earlier than the digits; otherwise, you would have to read past all your brothers to find out if you have a parent).  The behavior is the same as that for a regular JOIN in all respects but the one, that JOIN keys are considered to be equal whenever their digit portions match.

The payoff for all this is pretty sweet.  We only have to store and we only have to ship and group-sort data down to the level at which it remains interesting in either dataset.  (TODO: do we get to be multiscale in both datasets?)  When the two datasets meet in the Reducer, the natural outcome is as if they were broken down to the mutually-required resolution.  The output is also efficiently multiscale.

NOTE:  The multiscale keys work very well in HBase too.  For the case where you are storing multiscale regions and querying on points, you will want to use a replacement character that is lexicographically after the digits, say, the letter "x."  To find the record for a given point, do a range request for one record on the interval starting with that point's quad key and extending to infinity (xxxxx…).  For a point with the finest-grain quad key of 012012, if the database had a record for 012012, that will turn up; if, instead, that region only required zoom level 4, the appropriate row (0120xx) would be correctly returned.

==== Smoothing the Distribution

We now have in hand, for each year, a set of multiscale quad tile records with each record holding the weather station IDs that cover it.  What we want to produce is a dataset that has, for each hour and each such quad tile, a record describing the consensus weather on that quad tile.  If you are a meteorologist, you will probably want to take some care in forming the right weighted summarizations -- averaging the fields that need averaging, thresholding the fields that need thresholding and so forth.  We are going to cheat and adopt the consensus rule of "eliminate weather stations with missing data, then choose the weather station with the largest area coverage on the quad tile and use its data unmodified."  To assist that, we made a quiet piece of preparation and have sorted the weather station IDs from largest to smallest in area of coverage, so that the Reducer simply has to choose from among its input records the earliest one on that list.

What we have produced is gold dataset useful for any number of explorations and applications.  An exercise at the end of the chapter (TODO:  REF) prompts you to make a visual browser for historical weather.  Let's take it out for a simple analytical test drive, though.

The tireless members of Retrosheet.org have compiled box scores for nearly every Major League Baseball game since its inception in the late 1800s.  Baseball score sheets typically list the game time weather and wind speed and those fields are included in the Retrosheet data; however, values are missing for many records and since this is hand-entered data, surely many records have coding errors as well.  For example, on October 1, 2006, the home-team Brewers pleased a crowd of 44,133 fans with a 5-3 win over the Cardinals on a wonderful fall day recorded as having game-time temperature of 83 degrees, wind 60 miles per hour out to left field and sunny.  In case you are wondering, 60-mile per hour winds cause 30-foot waves at sea, trees to be uprooted and structural damage to buildings becomes likely, so it is our guess that the scoresheet is, in this respect, wrong.

Let's do a spatial drawing of the Retrosheet data for each game against the weather estimated using the NCDC dataset for that stadium's location at the start of the game; this will let us fill in missing data and flag outliers in the Retrosheet scores.

Baseball enthusiasts are wonderfully obsessive, so it was easy to find online data listing the geographic location of every single baseball stadium -- the file sports/baseball/stadium_geolocations.tsv lists each Retrosheet stadium ID followed by its coordinates and zoom-level 12 quad key.  Joining that on the Retrosheet game logs equips the game log record with the same quad key and hour keys used in the smoothed weather dataset.  (Since the data is so small, we turned parallelism down to 1.)

Next, we will join against the weather data; this data is so large, it is worth making a few optimizations.  First, we will apply the guideline of "join against the smallest amount of data possible."  There are fewer than a hundred quad keys we are interested in over the whole time period of interest and the quad key breakdown only changes year by year, so rather than doing a multiscale join against the full hourly record, we will use the index that gives the quad key breakdown per year to find the specific containing quad keys for each stadium over time.  For example (TODO: find an example where a quad key was at a higher zoom level one year and a lower one a different year).  Doing the multiscale join of stadium quad keys against the weather quad key year gives (TODO: name of file).

Having done the multiscale join against the simpler index, we can proceed using the results as direct keys; no more multiscale magic is required.  Now that we know the specific quad keys and hours, we need to extract the relevant weather records.  We will describe two ways of doing this.  The straightforward way is with a join, in this case of the massive weather quad tile data against the relatively tiny set of quad key hours we are interested in.  Since we do not need multiscale matching any more, we can use Pig and Pig provides a specialized join for the specific case of joining a tiny dataset to a massive one, called the replicated join.  You can skip ahead to the Advanced Pig chapter (TODO:  REF) to learn more about it; for now, all you need to know is that you should put the words "`USING 'replicated'`" at the end of the line, and that the smallest dataset should be on the _right_. (Yes, it's backwards: for replicated joins the smallest should be on the right, while for regular joins it should be on the left.)  This type of join loads the small dataset into memory and simply streams through the larger dataset, so no Reduce is necessary.  It's always a good thing when you can avoid streaming TB of data through the network card when all you want are a few MB.

In this case, there are a few thousand lines in the small dataset, so it is reasonable to do it the honest way, as just described.  In the case where you are just trying to extract a few dozen keys, your authors have been known to cheat by inlining the keys in a filter.  Regular expression engines are much faster than most people realize and are perfectly content to accept patterns with even a few hundred alternations.  An alternative approach here is to take the set of candidate keys, staple them together into a single ludicrous regexp and template it into the PIg script you will run.

.Cheat to Win: Filtering down to only joinable keys using a regexp
----
huge_data = LOAD '...' AS f1, f2, f3;
filtered_data = FILTER huge_data BY MATCH(f1, '^(012012|013000|020111| [...dozens more...])$');
STORE filtered_data INTO '...';
----

==== Results

With just the relevant records extracted, we can compare the score sheet data with the weather data.  Our script lists output columns for the NCDC weather and wind speed, the score sheet weather and wind speed, the distance from the stadium to the relevant weather station and the percentage difference for wind speed and temperature.

It would be an easy mistake to, at this point, simply evict the Retrosheet measurements and replace with the NCDC measurements; we would not argue for doing so.  First, the weather does vary, so there is some danger in privileging the measurement at a weather station some distance away (even if more precise) over a direct measurement at a correct place and time.  In fact, we have far better historical coverage of the baseball data than the weather data.  The weather data we just prepared gives a best-effort estimate of the weather at every quad tile, leaving it in your hands to decide whether to accept a reading from a weather station dozens or hundreds of miles away.  Rather, the philosophically sound action would be to flag values for which the two datasets disagree as likely outliers.

The successful endpoint of most Big Data explorations is a transition to traditional statistical packages and elbow grease -- it shows you've found domain patterns worth exploring. If this were a book about baseball or forensic econometrics, we'd carry forward comparing those outliers with local trends, digging up original entries, and so forth.  Instead, we'll just label them with a scarlet "O" for outlier, drop the mic and walk off stage.


=== Conclusions

Most importantly of all, this chapter will solidify your intuition about how to move and combine data strategically.  You'll be learning new types of joins that require a more sophistitcated notion of 'context' (data locality).  But the deep natural intuition about space and physical locality you already possess makes this challenge much easier.  Most readers should concentrate foremost on the tactics of applying our analytic patterns. (In fact, if you want to trust us that the sections where we dive into the underlying map/reduce work as advertised, we've made them easy to skip the first time through.)  But come back at some point and step through the spatial and multi-scale spatial joins we introduce in this chapter.  If you understand the underlying mechanics -- how the design of the keys ensures records that must be related in context show up at the right place in the right order -- then you have mastered the deep concept of map/reduce.
