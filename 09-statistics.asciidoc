[[statistics]]
== Statistics

=== Skeleton: Statistics

Data is worthless. Actually, it's worse than worthless. It costs you money to gather, store, manage, replicate and analyze. What you really want is insight -- a relevant summary of the essential patterns in that data -- produced using relationships to analyze data in context.

Statistical summaries are the purest form of this activity, and will be used repeatedly in the book to come, so now that you see how Hadoop is used it's a good place to focus.

Some statistical measures let you summarize the whole from summaries of the parts: I can count all the votes in the state by summing the votes from each county, and the votes in each county by summing the votes at each polling station. Those types of aggregations -- average/standard deviation, correlation, and so forth -- are naturally scalable, but just having billions of objects introduces some practical problems you need to avoid. We'll also use them to introduce Pig, a high-level language for SQL-like queries on large datasets.

Other statistical summaries require assembling context that grows with the size of the whole dataset. The amount of intermediate data required to count distinct objects, extract an accurate histogram, or find the median and other quantiles can become costly and cumbersome. That's especially unfortunate because so much data at large scale has a long-tail, not normal (Gaussian) distribution -- the median is far more robust indicator of the "typical" value than the average. (If Bill Gates walks into a bar, everyone in there is a billionaire on average.)

=== Summary Statistics


Criteria for being in the structural patterns part: in remainder of book,(a) there's only one way to do it; (b) we don't talk about how it's done.

Later or here or at all demonstrate combiners in m-r?


TODO: content to come


Join      pl-yr on pk-te-yr: pl-pk-te-yr
Group   ppty on pl: pl-g-pks-tes-yrs
Agg       pgty on pk and yr: pl-g-tes-stadia
Flatten  pgty on pk and yr: pl-te-stadia
Teammates: pl-yr to tm-yr-g-pls; cross to tm-yr-g-plaplbs; project to plas-plbs-gs
    flatten to pla-plbs group to pla-g-plbs 
    distinct to pla-d-plbs (or pl[teammates])
    flatten to pla-plb (or teammates)



Weather stations: wstn-info ws-dt-hr[obs]
Wp: art[text] art[info] art-dt-h[views]
Server Logs: ip-url-time[reqs]
UFOs: sightings[plc-dt-tm]
airports: ap-tm[alid,flid,dest,flights] 


-- Group on year; find COUNT(), count distinct, MIN(), MAX(), SUM(), AVG(), STDEV(), byte size

SELECT
    MIN(HR)              AS hr_min,
    MAX(HR)              AS hr_max,
    AVG(HR)              AS hr_avg,
    STDDEV_POP(HR)       AS hr_stddev,
    SUM(HR)              AS hr_sum,
    COUNT(*)             AS n_recs,
    COUNT(*) - COUNT(HR) AS hr_n_nulls,
    COUNT(DISTINCT HR)   AS hr_n_distinct -- doesn't count NULL
  FROM bat_season bat
;

SELECT
    MIN(nameFirst)                     AS nameFirst_min,
    MAX(nameFirst)                     AS nameFirst_max,
    --
    MIN(CHAR_LENGTH(nameFirst))        AS nameFirst_strlen_min,
    MAX(CHAR_LENGTH(nameFirst))        AS nameFirst_strlen_max,
    MIN(OCTET_LENGTH(nameFirst))       AS nameFirst_bytesize_max,
    MAX(OCTET_LENGTH(nameFirst))       AS nameFirst_bytesize_max,
    AVG(CHAR_LENGTH(nameFirst))        AS nameFirst_strlen_avg,
    STDDEV_POP(CHAR_LENGTH(nameFirst)) AS nameFirst_strlen_stddev,
    LEFT(GROUP_CONCAT(nameFirst),25)   AS nameFirst_examples,
    SUM(CHAR_LENGTH(nameFirst))        AS nameFirst_strlen_sum,
    --
    COUNT(*)                           AS n_recs,
    COUNT(*) - COUNT(nameFirst)        AS nameFirst_n_nulls,
    COUNT(DISTINCT nameFirst)          AS nameFirst_n_distinct
  FROM bat_career bat
;

SELECT
  player_id,
  MIN(year_id) AS yearBeg,
  MAX(year_id) AS yearEnd,
  COUNT(*)    AS n_years,
    MIN(HR)              AS hr_min,
    MAX(HR)              AS hr_max,
    AVG(HR)              AS hr_avg,
    STDDEV_POP(HR)       AS hr_stddev,
    SUM(HR)              AS hr_sum,
    COUNT(*)             AS n_recs,
    COUNT(*) - COUNT(HR) AS hr_n_nulls,
    COUNT(DISTINCT HR)   AS hr_n_distinct -- doesn't count NULL
  FROM bat_season bat
  GROUP BY player_id
  ORDER BY hr_max DESC
;
==== Transpose Columns Into `field name, field value` Pairs

Our next pattern is to transpose fields from each row into records having a column with the field name and a column with the field value, sometimes called attribute-value form.




=== Sampling


* Random sampling using the traditional pseudo-random number generators (which can be dangerous; we'll tell you how to do it right) (use input filename as seed)
* Consistent sampling returns a fraction of records by _key_: if a record with the key "chimpanzee" is selected into the sample, all records with that key are selected into the sample.
* (with/without replacement; weighted)
* Reservoir sampling selects a given number of records. A uniform reservoir sample with count 100, say, would return 100 records, each with the same chance of being selected, regardless of the size of the dataset.
* Subuniverse sampling selects a set of records and all associated records with it -- useful when you want to be able to joins on the sampled data, or to select a dense subgraph of a network. (TECH: is "dense subgraph" right?)
* Stratified sampling: sampling from groups/bins/strata/whatever - http://en.wikipedia.org/wiki/Stratified_sampling
* Sampling into multiple groups eg for bootstrapping
* Note that pig sample is mathematically lame (see Datafu for why)
* Note that pig sample is nice about eliminating records while loading (find out if Datafu does too)
* Warning I may have written lies about reservoir sampling make sure to review
* Spatial Sampling
* Also: generating distributions (use the random.org data set and generate a column for each dist using it)
* Expand the random.org by taking each r.o number as seed


==== Sample Records Consistently


----
-- Consistent sample of events
SELECT ev.event_id,
    LEFT(MD5(CONCAT(ev.game_id, ev.event_id)), 4) AS evid_hash,
    ev.*
  FROM events ev WHERE LEFT(MD5(CONCAT(ev.game_id, ev.event_id)), 2) = '00';
----

----
-- Consistent sample of games -- all events from the game are retained
-- FLO200310030 has gid_hash 0000... but evid_hash 0097 and so passes both
SELECT ev.event_id,
    LEFT(MD5(ev.game_id),4) AS gid_hash,
    ev.*
  FROM events ev WHERE LEFT(MD5(ev.game_id),2) = '00';
----

Out of 1962193 events in the 2010, 7665 expected (1/256th of the total);
got 8159 by game, 7695 by event

----
SELECT n_events, n_events/256, n_by_game, n_by_event
  FROM
    (SELECT COUNT(*) AS n_events    FROM events) ev,
    (SELECT COUNT(*) AS n_by_event  FROM events WHERE LEFT(MD5(CONCAT(game_id,event_id)),2) = '00') ev_e,
    (SELECT COUNT(*) AS n_by_game   FROM events WHERE LEFT(MD5(game_id),2) = '00') ev_g
    ;
----


=== Generating Data



-- === Generating an Integers table

DROP TABLE IF EXISTS numbers1k;
CREATE TABLE `numbers1k` (
  `idx`  INT(20) UNSIGNED PRIMARY KEY AUTO_INCREMENT,
  `ix0`  INT(20) UNSIGNED NOT NULL DEFAULT '0',
  `ixN`  INT(20) UNSIGNED          DEFAULT '0',
  `ixS`  INT(20) SIGNED   NOT NULL DEFAULT '0',
  `zip`  INT(1)  UNSIGNED NOT NULL DEFAULT '0',
  `uno`  INT(1)  UNSIGNED NOT NULL DEFAULT '1'
) ENGINE=INNODB DEFAULT CHARSET=utf8;

INSERT INTO numbers1k (ix0, ixN, ixS, zip, uno)
SELECT
  (@row := @row + 1) - 1 AS ix0,
  IF(@row=1, NULL, @row-2) AS ixN,
  (@row - 500) AS ixS,
  0 AS zip, 1 AS uno
 FROM
(select 0 union all select 1 union all select 3 union all select 4 union all select 5 union all select 6 union all select 6 union all select 7 union all select 8 union all select 9) t,
(select 0 union all select 1 union all select 3 union all select 4 union all select 5 union all select 6 union all select 6 union all select 7 union all select 8 union all select 9) t2,
(select 0 union all select 1 union all select 3 union all select 4 union all select 5 union all select 6 union all select 6 union all select 7 union all select 8 union all select 9) t3,
(SELECT @row:=0) r
;

DROP TABLE IF EXISTS numbers;
CREATE TABLE `numbers` (
  `idx`  INT(20) UNSIGNED PRIMARY KEY AUTO_INCREMENT,
  `ix0`  INT(20) UNSIGNED NOT NULL DEFAULT '0',
  `ixN`  INT(20) UNSIGNED          DEFAULT '0',
  `ixS`  INT(20) SIGNED   NOT NULL DEFAULT '0',
  `zip`  INT(1)  UNSIGNED NOT NULL DEFAULT '0',
  `uno`  INT(1)  UNSIGNED NOT NULL DEFAULT '1'
) ENGINE=INNODB DEFAULT CHARSET=utf8;

INSERT INTO numbers (ix0, ixN, ixS, zip, uno)
SELECT
  (@row := @row + 1) - 1 AS ix0,
  IF(@row=1, NULL, @row-2) AS ixN,
  (@row - 500000) AS ixS,
  0 AS zip, 1 AS uno
FROM
(SELECT zip FROM numbers1k) t1,
(SELECT zip FROM numbers1k) t2,
(SELECT @row:=0) r
;


----
    # generate 100 files of 100,000 integers each; takes about 15 seconds to run
    time ruby -e '10_000_000.times.map{|num| puts num }' | gsplit -l 100000 -a 2 --additional-suffix .tsv -d - numbers

    # in mapper, read N and generate `(0 .. 99).map{|offset| 100 * N + offset }`
----


==== Season leaders

-- * Selecting top-k Records within Group
-- GROUP...FOREACH GENERATE TOP
-- most hr season-by-season

==== Transpose record into attribute-value pairs

Group by season, transpose, and take the top 10 for each season, attribute pair

=== Overflow, Underflow and other Dangers

TODO: content to come

=== Quantiles and Histograms

TODO: content to come


In the structural operations chapter, we brought up the subject of calculating quantiles (an equal-width histogram), but postponed the discussion, judging it to be fiendishly hard. Calculating even an exact median -- the simplest case -- in a single map-reduce flow is not just hard, it's provably impossible (REF cormode paper). 

The issue is that you need to get all candidates for the edge of a bin onto the same reducer, and know the number of elements that precede the candidates on your reducer. From the mapper, however, it's impossible to know what keys to assign without knowing the global distribution -- the very thing we want to calculate! /end move to statistics)

==== Median

----
SELECT COUNT(*), CEIL(COUNT(*)/2) AS midrow
  FROM bat_career
 ;
SELECT G, cols.*
  FROM bat_career bat,
    (SELECT COUNT(*) AS n_entries, CEIL(COUNT(*)/2) AS midrow FROM bat_career) cols
  ORDER BY HR
  LIMIT 1 OFFSET 8954
;
----

==== Exact median using RANK

Well, we've met another operation with this problem, namely the sort (ORDER BY) operation. It does a first pass to sample the global distribution of keys, then a full map-reduce to place ordered values on the same reducer. Its numerate younger brother, RANK, will do what we need. The quartiles -- the boundaries of the four bins bins each holding 25% of the values -- ...

(Show using RANK and then filter; use the "pre-inject and assert global values" trick for the bin size. Handle the detail of needing to average two values when boundary splits an index, eg median of a table with even number of rows)

==== Approximate median & quantiles using DataFu
 (get better title)

 
=== Algebraic vs Holistic Aggregations

TODO: content to come

=== "Sketching" Algorithms

TODO: content to come

