[[statistics]]
== Statistics

=== Skeleton: Statistics

Data is worthless. Actually, it's worse than worthless. It costs you money to gather, store, manage, replicate and analyze. What you really want is insight -- a relevant summary of the essential patterns in that data -- produced using relationships to analyze data in context.

Statistical summaries are the purest form of this activity, and will be used repeatedly in the book to come, so now that you see how Hadoop is used it's a good place to focus.

Some statistical measures let you summarize the whole from summaries of the parts: I can count all the votes in the state by summing the votes from each county, and the votes in each county by summing the votes at each polling station. Those types of aggregations -- average/standard deviation, correlation, and so forth -- are naturally scalable, but just having billions of objects introduces some practical problems you need to avoid. We'll also use them to introduce Pig, a high-level language for SQL-like queries on large datasets.

Other statistical summaries require assembling context that grows with the size of the whole dataset. The amount of intermediate data required to count distinct objects, extract an accurate histogram, or find the median and other quantiles can become costly and cumbersome. That's especially unfortunate because so much data at large scale has a long-tail, not normal (Gaussian) distribution -- the median is far more robust indicator of the "typical" value than the average. (If Bill Gates walks into a bar, everyone in there is a billionaire on average.)

=== Summary Statistics

TODO: content to come

==== Transpose Columns Into `field name, field value` Pairs

Our next pattern is to transpose fields from each row into records having a column with the field name and a column with the field value, sometimes called attribute-value form.

=== Sampling


We'll cover sampling in the chapter on statistics, but so you know what to look for there:

* Random sampling using the traditional pseudo-random number generators (which can be dangerous; we'll tell you how to do it right) (use input filename as seed)
* Consistent sampling returns a fraction of records by _key_: if a record with the key "chimpanzee" is selected into the sample, all records with that key are selected into the sample.
* (with/without replacement; weighted)
* Reservoir sampling selects a given number of records. A uniform reservoir sample with count 100, say, would return 100 records, each with the same chance of being selected, regardless of the size of the dataset.
* Subuniverse sampling selects a set of records and all associated records with it -- useful when you want to be able to joins on the sampled data, or to select a dense subgraph of a network. (TECH: is "dense subgraph" right?)
* Stratified sampling: sampling from groups/bins/strata/whatever - http://en.wikipedia.org/wiki/Stratified_sampling
* Sampling into multiple groups eg for bootstrapping
* Note that pig sample is mathematically lame (see Datafu for why)
* Note that pig sample is nice about eliminating records while loading (find out if Datafu does too)
* Warning I may have written lies about reservoir sampling make sure to review
* Spatial Sampling
* Also: generating distributions (use the random.org data set and generate a column for each dist using it)
* Expand the random.org by taking each r.o number as seed

==== Sample Records Consistently


----
-- Consistent sample of events
SELECT ev.event_id,
    LEFT(MD5(CONCAT(ev.game_id, ev.event_id)), 4) AS evid_hash,
    ev.*
  FROM events ev WHERE LEFT(MD5(CONCAT(ev.game_id, ev.event_id)), 2) = '00';
----

----
-- Consistent sample of games -- all events from the game are retained
-- FLO200310030 has gid_hash 0000... but evid_hash 0097 and so passes both
SELECT ev.event_id,
    LEFT(MD5(ev.game_id),4) AS gid_hash,
    ev.*
  FROM events ev WHERE LEFT(MD5(ev.game_id),2) = '00';
----

Out of 1962193 events in the 2010, 7665 expected (1/256th of the total);
got 8159 by game, 7695 by event

----
SELECT n_events, n_events/256, n_by_game, n_by_event
  FROM
    (SELECT COUNT(*) AS n_events    FROM events) ev,
    (SELECT COUNT(*) AS n_by_event  FROM events WHERE LEFT(MD5(CONCAT(game_id,event_id)),2) = '00') ev_e,
    (SELECT COUNT(*) AS n_by_game   FROM events WHERE LEFT(MD5(game_id),2) = '00') ev_g
    ;
----


=== Generating Data



-- === Generating an Integers table

DROP TABLE IF EXISTS numbers1k;
CREATE TABLE `numbers1k` (
  `idx`  INT(20) UNSIGNED PRIMARY KEY AUTO_INCREMENT,
  `ix0`  INT(20) UNSIGNED NOT NULL DEFAULT '0',
  `ixN`  INT(20) UNSIGNED          DEFAULT '0',
  `ixS`  INT(20) SIGNED   NOT NULL DEFAULT '0',
  `zip`  INT(1)  UNSIGNED NOT NULL DEFAULT '0',
  `uno`  INT(1)  UNSIGNED NOT NULL DEFAULT '1'
) ENGINE=INNODB DEFAULT CHARSET=utf8;

INSERT INTO numbers1k (ix0, ixN, ixS, zip, uno)
SELECT
  (@row := @row + 1) - 1 AS ix0,
  IF(@row=1, NULL, @row-2) AS ixN,
  (@row - 500) AS ixS,
  0 AS zip, 1 AS uno
 FROM
(select 0 union all select 1 union all select 3 union all select 4 union all select 5 union all select 6 union all select 6 union all select 7 union all select 8 union all select 9) t,
(select 0 union all select 1 union all select 3 union all select 4 union all select 5 union all select 6 union all select 6 union all select 7 union all select 8 union all select 9) t2,
(select 0 union all select 1 union all select 3 union all select 4 union all select 5 union all select 6 union all select 6 union all select 7 union all select 8 union all select 9) t3,
(SELECT @row:=0) r
;

DROP TABLE IF EXISTS numbers;
CREATE TABLE `numbers` (
  `idx`  INT(20) UNSIGNED PRIMARY KEY AUTO_INCREMENT,
  `ix0`  INT(20) UNSIGNED NOT NULL DEFAULT '0',
  `ixN`  INT(20) UNSIGNED          DEFAULT '0',
  `ixS`  INT(20) SIGNED   NOT NULL DEFAULT '0',
  `zip`  INT(1)  UNSIGNED NOT NULL DEFAULT '0',
  `uno`  INT(1)  UNSIGNED NOT NULL DEFAULT '1'
) ENGINE=INNODB DEFAULT CHARSET=utf8;

INSERT INTO numbers (ix0, ixN, ixS, zip, uno)
SELECT
  (@row := @row + 1) - 1 AS ix0,
  IF(@row=1, NULL, @row-2) AS ixN,
  (@row - 500000) AS ixS,
  0 AS zip, 1 AS uno
FROM
(SELECT zip FROM numbers1k) t1,
(SELECT zip FROM numbers1k) t2,
(SELECT @row:=0) r
;


----
    # generate 100 files of 100,000 integers each; takes about 15 seconds to run
    time ruby -e '10_000_000.times.map{|num| puts num }' | gsplit -l 100000 -a 2 --additional-suffix .tsv -d - numbers

    # in mapper, read N and generate `(0 .. 99).map{|offset| 100 * N + offset }`
----

=== Overflow, Underflow and other Dangers

TODO: content to come

=== Quantiles and Histograms

TODO: content to come

=== Algebraic vs Holistic Aggregations

TODO: content to come

=== "Sketching" Algorithms

TODO: content to come

