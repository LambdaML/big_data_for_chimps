[[transform_pivot]]
== Chimpanzee and Elephant Save Christmas ==

// Say more about conceptualizing big data and tie in what readers have already learned (up to this chapter, the simple transform and first exploration material) and weave that in to help them have 'ah ha' moments and really grasp this material. Then, write your "...in this chapter..."  Finally, a good discussion of "locality" would go well anchored here.  And then you can dive into the Elves.  Amy////

=== Chimpanzee and Elephant Save Christmas (pt 1) ===

It was holiday time at the North Pole, and letters from little boys and little girls all over the world flooded in as they always do. But this year there was a problem: the world had grown, and the elves just could not keep up with the scale of requests. Luckily, their friends at the Elephant & Chimpanzee Data Shipment Company were available to simplify the process.

==== A Non-scalable approach ====

To meet the wishes of children from every corner of the earth, each elf specializes in a certain kind of toy, from Autobot to Pony to Yo-Yo.

[[elf_workstation]]
.The elves' workbenches are meticulous and neat.
image::images/chimps_and_elves/bchm_0201.png[Elf Workstations, pre-Hadoop]

[[mail_tree]]
.Little boys and girls' mail is less so.
image::images/chimps_and_elves/bchm_0202.png[Fetching the next letter to Santa]

As bags of mail arrived from every town and city, they were hung from the branches of the Bag Tree. Each time an elf was ready for the next letter, a big claw arm swung out to the right spot on the Bag Tree to retrieve it. "The locality of access is all wrong!" bellowed the chimps. "The next request for Lego is as likely to be from Cucamonga as from Novosibirsk, and letters can't be pulled from the tree any faster than the crane arm can move!"

What's worse, the size of Santa's operation meant that the workbenches were very far from where letters came in, and from the warehouses that held completed toys. The hallways were clogged with frazzled elves running back and forth between the Bag Tree, their workbench, and the shipping center -- they spent almost as much effort on the mechanics of retrieving letters and dropping off gifts as they did making toys. "Throughput, not Latency!" trumpeted the elephants. "For hauling heavy loads, you need a stately elephant parade, not a swarm of frazzled elves!"

==== Letters to Toy Requests ====

In marched the chimps and elephants, who (singing a rather bawdy version of the Map-Reduce Haiku) built the following system.

As you might guess from the last chapter, they set up a finite number of chimpanzees at a finite number of typewriters, each with an elephant desk-mate. Rather than the centralized Bag Tree, the elephants mustered to distribute mailbags all across the processing stations -- now they could be processed locally, rather than clogging up the hallways as the old Bag Tree required.

The chimps' job was to take letters one-by-one from a mailbag, and fill out a toyform for each requested toy. A toyform has a prominent label showing the type of toy, and a body with all the information you'd expect: Name, Goodness, Location, and so forth. As with all North Pole employees, they're able to tell who is naughty and who is nice. The first note, from a very good girl who is thoughtful for her brother, creates two toyforms: one for Joe's robot and one for Julia's doll. The second note is spam, so it creates no toyforms; the third one is from a kid who is not just a jerk but also a Yankees fan, and so Santa will put coal in his stocking.

        # Good kids, generates a toy for Julia and a toy for her brother

        Deer SANTA                                     robot | type="optimus prime" recipient="Joe"
                                                       doll  | type="green hair"  recipient="Joe's sister Julia"
        I wood like a doll for me and 
        and an optimus prime robot for my
	brother joe

        I have been good this year

        love julia

        # Spam, no action

        Greetings to you Mr Claus, I came to know
        of you in my search for a  reliable and
        reputable person to handle a very confidential
        business transaction, which involves the
        transfer of a huge sum of money...


        # Frank is a jerk. He will get coal.

        HEY SANTA I WANT A YANKEES HAT AND NOT             coal  | type="anthracite" recipient="Frank" reason="doesn't like to read"
        ANY DUMB BOOKS THIS YEAR

        FRANK

        ---------------------------------------         # Spam, no action

image::images/chimps_and_elves/bchm_0203.png[Chimps read each letter]
image::images/chimps_and_elves/bchm_0204.png[Letters become toyforms]

=== Chimpanzee and Elephant Save Christmas, part 2 ===

So far, this looks a lot like the process for the translation project, with letters in, toyforms out, and the elephant librarians ensuring a steady supply of mail. But a simple transform isn't enough. You see, the set-up for tying ribbons on Ponies is very different from the set-up for assembling a Yo-Yo. The elves would like to process each type of toy as a group, designating a specific workbench for each type of toy. All the robots and hats might go to workbench A, while all the ponies and yo-yos go to workbench B, and all the coal orders to workbench C.

image::images/chimps_and_elves/bchm_0205.png[Each toy at a unique station]

Here's the system C&E, Inc constructed. Next to each chimpanzee stands a line of pygmy elephants, one for each workbench. Each pygmy elephant bears a vertical file like you find on a very organized person's desk:

image::images/paper_sorter.jpg[paper sorter]
//// Perhaps a smaller sizing for the image? Amy////

The chimps places their toyforms into these file folders as fast as their dexterous fingers can file. Once all the letters in a mailbag have been handled, the elephants march off to the workbench they serve, and behind them a new line of elephants trundle into place, as the chimpanzee tears into a new mailbag. What fun!

image::images/chimps_and_elves/bchm_0206.png[toyforms go off in batches]

=== Hadoop vs Traditional Databases ===

Fundamentally, the storage engine at the heart of a traditional relational database does two things: it holds all the records, and it maintains a set of indexes for lookups and other operations. To retrieve a record, it must consult the appropriate index to find the location of the record, then load it from the disk. This is very fast for record-by-record retrieval, but becomes cripplingly inefficient for general high-throughput access. If the records are stored by location and arrival time (as the mailbags were on the Bag Tree), then
there is no "locality of access"
for records retrieved by, say, type of toy --
records for Lego will be spread all across the disk. With traditional drives, the disk's read head has to physically swing back and forth in a frenzy across the disk,
and though the newer flash drives have smaller retrieval latency it's still far too high for bulk operations.

What's more, traditional database applications lend themselves very well to low-latency operations (such as rendering a webpage showing the toys you requested), but very poorly to high-throughput operations (such as requesting every single doll order in sequence). Unless you invest specific expertise and effort, you have little ability to organize requests for efficient retrieval. You either suffer a variety of non-locality and congestion based inefficiencies, or wind up with an application that caters to the database more than to its users. You can to a certain extent use the laws of economics to bend the laws of physics -- as the commercial success of Oracle and Netezza show -- but the finiteness of time, space and memory present an insoluble scaling problem for traditional databases.

Hadoop solves the scaling problem by not solving the data organization problem. Rather than insist that the data be organized and indexed as it's written to disk, catering to every context that could be requested. Instead, it focuses purely on the throughput case. 
TODO explain disk is the new tape It takes X to seek but

The typical Hadoop operation streams large swaths of data 
The locality 


==== Toy Assembly ====

Each pygmy elephant carries its work orders to the workstation it serves. A workstation might handle several types of toys in a factory run, but always in a continuous batch -- the elves that handle ponies and yo-yos will see all the ponies first, and then all the yo-yos:

	Doll	Julia    	Anchorage	AK, USA
	Doll	Wei Ju		Shenzen	        China
        Doll	Michael   	Berlin  	Germany
	...
	Yo-Yo	Jim		Mountain View	CA, USA
	...
	Robot	Joe         	Austin    	TX, USA
	

You should wonder how, from all the medley outputs of all the rambunctious chimpanzees, these organized groups are formed. The answer is very clever: the elephants accomplish _grouping_ the toyforms by _sorting_ the toyforms.

[NOTE]
==========
You can try this for yourself: take a deck of cards, sort them by number, and fan the cards out in a line. You'll see that trivially separating piles where one number meets the next groups the cards by number.
==========

==== Sorted Groups ====

Earlier, as I mentioned that the chimps place the toyforms into the file folders, I skipped past an important detail.

The chimpanzees actually file their toyforms into the holder in sorted order. Since the folders aren't very big, and are immediately adjacent to the chimp's dexterous fingers, this doesn't take much additional time.

So when a pygmy elephant delivers its toyform sorter to a workbench, the forms within it are in order -- but of course there is no order across all the file folders. Elephant A might have "apple-cheeked", "cabbage patch" and "yellow-haired" dolls, elephant B "malibu", "raggedy", and "yellow-haired", and elephant C only a tall stack of "real Armenian" dolls. That's no problem though. Each elephant holds its topmost workform at the ready, and passes it to the elves once it's the next one in order to be processed. So in this case, workforms would come from elephant A, then A again, then B, and so on.

image::images/chimps_and_elves/bchm_0210.png[Secondary sort]

Elves do not have the prodigious memory that elephants do, but they can easily keep track of the next few dozen work orders each elephant holds. That way there is very little time spent seeking out the next work order. Elves assemble toys as fast as their hammers can fly, and the toys come out in the order Santa needs to make little children happy.

// You've seen that receiving all the toyforms for Dolls in a single batch make the elves more efficient. The elves requested that the toyforms be further grouped within each batch: so that all the dolls with "purple hair" arrive in a run, followed by dolls with "rosy cheeks", and so forth.

=== Sorted Batches ===

Santa delivers presents in order as the holidays arrive, racing the sun from New Zealand, through Asia and Africa and Europe, until the finish in American Samoa.

This is a literal locality: the presents for Auckland must go in a sack together, and Sydney, and Petropavlovsk, and so forth.

=== The Map-Reduce Haiku ===

As you recall, the bargain that Map/Reduce proposes is that you agree to only write programs that fit this Haiku:

      data flutters by
          elephants make sturdy piles
        insight shuffles forth

More prosaically,

1. *label*      -- turn each input record into any number of labelled records
2. *group/sort* -- hadoop groups those records uniquely under each label, in a sorted order
3. *reduce*     -- for each group, process its records in order; emit anything you want.

The trick lies in the 'group/sort' step: assigning the same label to two records in the 'label' step ensures that they will become local in the reduce step.

The machines in stage 1 ('label') are allowed no locality. They see each record exactly once, but with no promises as to order, and no promises as to which one sees which record. We've 'moved the compute to the data', allowing each process to work quietly on the data in its work space.

As each pile of output products starts to accumulate, we can begin to group them. Every group is assigned to its own reducer. When a pile reaches a convenient size, it is shipped to the appropriate reducer while the mapper keeps working. Once the map finishes, we organize those piles for its reducer to process, each in proper order.

If you notice, the only time data moves from one machine to another is when the intermediate piles of data get shipped. Instead of monkeys flinging poo, we now have a dignified elephant parade conducted in concert with the efforts of our diligent workers.

=== Hadoop's Contract ===

Hadoop imposes a few seemingly-strict constraints and provides a very few number of guarantees in return. As you're starting to see, that simplicity provides great power and is not as confining as it seems. You can gain direct control over things like partitioning, input splits and input/output formats. We'll touch on a very few of those, but for the most part this book concentrates on using Hadoop from the outside -- (TODO: ref) _Hadoop: The Definitive Guide_ covers this stuff (definitively).

==== The Mapper Guarantee ====

The contract Hadoop presents for a map task is simple, because there isn't much of one. Each mapper will get a continuous slice (or all) of some file, split at record boundaries, and in order within the file. You won't get lines from another input file, no matter how short any file is; you won't get partial records; and though you have no control over the processing order of chunks ("file splits"), within a file split all the records are in the same order as in the original file.

For a job with no reducer -- a "mapper-only" job -- you can then output anything you like; it is written straight to disk. For a Wukong job with a reducer, your output should be tab-delimited data, one record per line. You can designate the fields to use for the partition key, the sort key and the group key. (By default, the first field is used for all three.)

The typical job turns each input record into zero, one or many records in a predictable manner, but such decorum is not required by Hadoop. You can read in lines from Shakespeare and emit digits of _pi_; read in all input records, ignore them and emit nothing; or boot into an Atari 2600 emulator, publish the host and port and start playing Pac-Man. Less frivolously: you can accept URLs or filenames (local or HDFS) and emit their contents; accept a small number of simulation parameters and start a Monte Carlo simulation; or accept a database query, issue it against a datastore and emit each result.

==== The Group/Sort Guarantee ====

When Hadoop does the group/sort, it establishes the following guarantee for the data that arrives at the reducer:

* each labelled record belongs to exactly one sorted group;
* each group is processed by exactly one reducer;
* groups are sorted lexically by the chosen group key;
* and records are further sorted lexically by the chosen sort key.

It's very important that you understand what that unlocks, so I'm going to redundantly spell it out a few different ways:

* Each mapper-output record goes to exactly one reducer, solely determined by its key.
* If several records have the same key, they will all go to the same reducer.
* From the reducer's perspective, if it sees any element of a group it will see all elements of the group. 

You should typically think in terms of groups and not about the whole reduce set: imagine each partition is sent to its own reducer. It's important to know, however, that each reducer typically sees multiple partitions. (Since it's more efficient to process large batches, a certain number of reducer processes are started on each machine. This is in contrast to the mappers, who run one task per input split.) Unless you take special measures, the partitions are distributed arbitrarily among the reducers footnote:[Using a "consistent hash"; see (TODO: ref) the chapter on Sampling]. They are fed to the reducer in order by key.

Similar to a mapper-only task, your reducer can output anything you like, in any format you like. It's typical to output structured records of the same or different shape, but you're free engage in any of the shenanigans listed above.

