=== Exercises ===

==== Locate Documents by Content ====

The "regional flavor" question comes from a paper by Jason Baldridge and Ben Wing (TODO ref), who go one step further and show you can actually locate articles by their text alone.

Implement a model
that quantifies the probability 

=== Notes ===


* http://en.wikipedia.org/wiki/Wikipedia:List_of_controversial_issues
* http://wordnet.princeton.edu/wordnet/download/
* http://www.infochimps.com/datasets/list-of-dirty-obscene-banned-and-otherwise-unacceptable-words
* http://urbanoalvarez.es/blog/2008/04/04/bad-words-list
* entity names within angle brackets. Where possible these are drawn from Appendix D to ISO 8879:1986, Information Processing - Text & Office Systems - Standard Generalized Markup Language (SGML).
* http://faculty.cs.byu.edu/~ringger/CS479/papers/Gale-SimpleGoodTuring.pdf
* http://www.aclweb.org/anthology-new/P/P11/P11-1096.pdf
* http://www.ling.uni-potsdam.de/~gerlof/docs/npmi-pfd.pdf
* http://nltk.googlecode.com/svn/trunk/doc/howto/collocations.html
* Stanford Named Entity Parser - http://nlp.stanford.edu/software/CRF-NER.shtml
* http://nlp.stanford.edu/software/corenlp.shtml - 
  > Stanford CoreNLP provides a set of natural language analysis tools which can take raw English language text input and give the base forms of words, their parts of speech, whether they are names of companies, people, etc., normalize dates, times, and numeric quantities, and mark up the structure of sentences in terms of phrases and word dependencies, and indicate which noun phrases refer to the same entities. Stanford CoreNLP is an integrated framework, which make it very easy to apply a bunch of language analysis tools to a piece of text. Starting from plain text, you can run all the tools on it with just two lines of code. Its analyses provide the foundational building blocks for higher-level and domain-specific text understanding applications.
  > 
  > Stanford CoreNLP integrates all our NLP tools, including the part-of-speech (POS) tagger, the named entity recognizer (NER), the parser, and the coreference resolution system, and provides model files for analysis of English. The goal of this project is to enable people to quickly and painlessly get complete linguistic annotations of natural language texts. It is designed to be highly flexible and extensible. With a single option you can change which tools should be enabled and which should be disabled.
  > 
  > The Stanford CoreNLP code is written in Java and licensed under the GNU General Public License (v2 or later). Source is included. Note that this is the full GPL, which allows many free uses, but not its use in distributed proprietary software. The download is 259 MB and requires Java 1.6+.

* http://cogcomp.cs.illinois.edu/page/software_view/4
* http://opennlp.apache.org/
	
=== Refs ===


* http://thedatachef.blogspot.com/2011/04/tf-idf-with-apache-pig.html
* http://hortonworks.com/blog/pig-as-duct-tape-part-three-tf-idf-topics-with-cassandra-python-streaming-and-flask/
* http://hortonworks.com/blog/pig-macro-for-tf-idf-makes-topic-summarization-2-lines-of-pig/
