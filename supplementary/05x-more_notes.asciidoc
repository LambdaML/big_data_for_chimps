


-- EXERCISE: find the Bill James' "Gray Ink" score for each player: four points
--   per season they were in the top ten for HR, RBI or AVG; three points for R,
--   H, SLG; two points for 2B, BB or SB, and one point for each season in the
--   top ten for G, AB or 3B.  (See
--   http://baseball-reference.com/about/leader_glossary.shtml[Baseball
--   Reference] for a bit more of a description and the pitching equivalent).
--   uses GROUP-TOP-FOREACH (isEmpty ? 1 : 0)-GROUP-FOREACH (summing and
--   weighting the composite pieces of ink.



==== Flatten a Tuple of `field name, field value` pairs

If you want to simultaneously apply a dataflow to a large number of columns,
skip ahead to the statistics chapter, where we'll show how to use a UDF to transpose columns Into `field name, field value` pairs


=== (notes for later) 


* http://www.stat.berkeley.edu/users/spector/sql.pdf
  * SELECT * FROM kids GROUP BY id HAVING COUNT(*) = 10;
  * SELECT * FROM kids WHERE id IN (SELECT id FROM kids GROUP BY id HAVING COUNT(*) = 10);
  * SELECT * FROM kids INNER JOIN (SELECT id FROM kids GROUP BY id HAVING COUNT(*) = 10) AS t USING(id);
  * SELECT * FROM kids WHERE weight = (SELECT MAX(weight) FROM kids);
  * SELECT k.id,k.sex,k.race,k.age,k.weight,k.height FROM kids AS k, (SELECT sex,race,max(weight) AS weight from kids) AS m WHERE k.sex=m.sex AND k.race=m.race AND k.weight=m.weight; SELECT a.title,r.name FROM album AS a, artist AS r WHERE a.aid = r.aid;
  * To improve our previous example, we need to combine the track information with album and artist information. Suppose we want to find the 10 longest albums in the collection:
SELECT a.title,r.name,SUM(time) AS duration FROM track AS t, album as a, artist as r WHERE t.alid = a.alid AND a.aid = r.aid GROUP BY t.alid ORDER BY duration DESC LIMIT 1,10;
* http://justpain.com/eBooks/Databases/MySQL/MySQL%20Cookbook.pdf
* Art of SQL
  * Hierarchy: WITH parent (ckey) AS (SELECT DISTINCT pkey FROM   hierarchy WHERE  pkey = 'AAA' UNION ALL SELECT C.ckey FROM   hierarchy C ,parentP WHERE  P.ckey = C.pkey ) SELECT ckey FROM   parent; Figure 858, 
  * True if NONE Match Find all rows in TABLE1 where there are no rows in TABLE2 that have a T2C value equal to the current T1A value in the TABLE1 table: SELECT * FROM   table1 t1 WHERE  0 = (SELECT COUNT(*) FROM   table2 t2 WHERE  t1.t1a = t2.t2c); TABLE1 TABLE2 +-------+ +-----------+ |T1A|T1B| |T2A|T2B|T2C| |---|---| |---|---|---| |A|AA| |A|A|A| |B|BB| |B |A |-| |C |CC | +-----------+ SELECT * FROM   table1 t1 WHERE  NOT EXISTS (SELECT * FROM   table2 WHERE  t1.t1a SELECT * FROM   table1 WHERE  t1a NOT IN +-------+ "-" = null ANSWER ======= T1A T1B --- --- B   BB C   CC (SELECT t2c FROM   table2 WHERE  t2c IS NOT NULL); Figure 716, Sub-queries, true if none match t2 = t2.t2c);
  * Figure 720, Sub-queries, true if ten match The first two queries above use a correlated sub-query. The third is uncorrelated: True if TEN Match Find all rows in TABLE1 where there are exactly ten rows in TABLE2 that have a T2B value equal to the current T1A value in the TABLE1 table: SELECT * FROM   table1 WHERE  10 = (SELECT FROM WHERE SELECT * FROM   table1 WHERE  EXISTS (SELECT FROM WHERE GROUP BY t2b HAVING   COUNT(*) = 10); SELECT * FROM   table1 WHERE  t1a IN (SELECT   t2b FROM     table2 GROUP BY t2b HAVING   COUNT(*) = 10); 
* Using geo spatial to do graph queries
  - From preso on titanDB: 33. Use case 2 ● In human proteome, find all chemical groups A and B separated by less then x Å – Database Structure: ● Suppose all the proteins are connected to a “Type node” ● Each protein is linked to it's domains, each domain is linked to it's amino acids, each amino-acid linked to it's chemical groups and ultimately atoms ● Chemical groups have assigned distance between them and groups they are close to – Algorithm ● Select a protein of interest ● Get all of it's chemical groups: 1000(a.a)*3(ch.gr/a.a) ● Filter all of the Relations longer than k: 1000*3*100(possible contacts per ch.gr) ● Recover the proteins: 1000*3*100*2 ● With 1M traversals per second => 0.6 sec. to execute the query – If TitanDB with ElasticSearch and geo-queries (all within circle of radius x), higher speeds possible

* A hint: for some operations on all pairs (a,b) you can filter by (a <= b)...

==== Generating good random numbers

* http://blog.codinghorror.com/shuffling/
* http://opencoursesfree.org/archived_courses/cs.berkeley.edu/~mhoemmen/cs194/Tutorials/prng.pdf
    * "numbers with statistical properties of randomness. Note that I didn’t write “random numbers,” but rather, “numbers with statistical properties of randomness.”"
* Make sure you have enough bits
* Even 52 cards has 52! =~ 255 bits of permutation... can't possibly get every permutation for a table of even modest size
* Make sure you look out for ties and shuffle them as well
* Do you have to be think-y about the partitioner?
* Download about (8 years *365 days * 1 mebibyte) of randoms from random.org. This is however only 90 million 256-bit (32-byte) numbers, or 350 million 64-bit (8-byte) numbers.
* Don't just (rand mod 25) for a 1-in-25 random sample -- you'll be biased because it's not an exact number of bits. Instead reject if > 25 and try again.
* Watch out for non-reentrant rand() -- mutex or something (do we need to worry about this in hadoop?)
* http://blog.cloudera.com/blog/2013/02/how-to-resample-from-a-large-data-set-in-parallel-with-r-on-hadoop/
    * Sampling-with-replacement is the most popular method for sampling from the initial data set to produce a collection of samples for model fitting. This method is equivalent to sampling from a multinomial distribution where the probability of selecting any individual input data point is uniform over the entire data set. Unfortunately, it is not possible to sample from a multinomial distribution across a cluster without using some kind of communication between the nodes (i.e., sampling from a multinomial is not embarrassingly parallel). But do not despair: we can approximate a multinomial distribution by sampling from an identical Poisson distribution on each input data point independently, lending itself to an embarrassingly parallel implementation.

Here's a clip from the PokerStars website (they did their homework):

* A deck of 52 cards can be shuffled in 52! ways. 52! is about 2^225 (to be precise, 80,658,175,170,943,878,571,660,636,856,404,000,000,000,000,000 ways). We use 249 random bits from both entropy sources (user input and thermal noise) to achieve an even and unpredictable statistical distribution.
* Furthermore, we apply conservative rules to enforce the required degree of randomness; for instance, if user input does not generate required amount of entropy, we do not start the next hand until we obtain the required amount of entropy from Intel RNG.
* We use the SHA-1 cryptographic hash algorithm to mix the entropy gathered from both sources to provide an extra level of security
* We also maintain a SHA-1-based pseudo-random generator to provide even more security and protection from user data attacks
* To convert random bit stream to random numbers within a required range without bias, we use a simple and reliable algorithm. For example, if we need a random number in the range 0-25:
      o we take 5 random bits and convert them to a random number 0-31
      o if this number is greater than 25 we just discard all 5 bits and repeat the process
* This method is not affected by biases related to modulus operation for generation of random numbers that are not 2n, n = 1,2,..
* To perform an actual shuffle, we use another simple and reliable algorithm:
      o first we draw a random card from the original deck (1 of 52) and place it in a new deck - now original deck contains 51 cards and the new deck contains 1 card
      o then we draw another random card from the original deck (1 of 51) and place it on top of the new deck - now original deck contains 50 cards and the new deck contains 2 cards
      o we repeat the process until all cards have moved from the original deck to the new deck
* This algorithm does not suffer from "Bad Distribution Of Shuffles" described in [2]

[2] "How We Learned to Cheat at Online Poker: A Study in Software Security" - http://itmanagement.earthweb.com/entdev/article.php/616221
[3] "The Intel Random Number Generator" - http://www.cryptography.com/resources/whitepapers/IntelRNG.pdf"

==== Tuning

Use the fewest reduce steps reasonable,
And reduce on the least data reasonable.
But it rarely makes sense to use an extra reduce to ship less data 

===== From programming pig:

Table 8-1. When Pig pushes filters
Preceding operator	Filter will be pushed before?	Comments
cogroup	Sometimes	The filter will be pushed if it applies to only one input of the cogroup and does not contain a UDF.
cross	Sometimes	The filter will be pushed if it applies to only one input of the cross.
distinct	Yes	 
filter	No	Will seek to merge them with and to avoid passing data through a second operator. This is done only after all filter pushing is complete.
foreach	Sometimes	The filter will be pushed if it references only fields that exist before and after the foreach, and foreach does not transform those fields.
group	Sometimes	The filter will be pushed if it does not contain a UDF.
join	Sometimes	The filter will be pushed if it applies to only one input of the join, and if the join is not outer for that input.
load	No	 
mapreduce	No	mapreduce is opaque to Pig, so it cannot know whether pushing will be safe.
sort	Yes	 
split	No	 
store	No	 
stream	No	stream is opaque to Pig, so it cannot know whether pushing will be safe.
union	Yes	 
Also, consider adding filters that are implicit in your script. For example, all of the records with null values in the key will be thrown out by an inner join. If you know that more than a few hundred of your records have null key values, put a filter input by key is not null before the join. This will enhance the performance of your join.



SELECT COUNT(*), SUM(IF(stint = 1, 1, 0)), SUM(IF(stint =0, 1, 0)), SUM(IF(stint = 2, 1, 0)) FROM batting WHERE stint <= 99
;

SELECT tot_bl_stints, tot_bl_seasons, tot_bw_stints, COUNT(*), COUNT(*)/tot_bl_seasons FROM
  (SELECT COUNT(*) AS tot_bl_seasons FROM bat_season) t4,
  (SELECT COUNT(*) AS tot_bw_stints FROM bat_war) t2,
  (SELECT COUNT(*) AS tot_bl_stints FROM batting) t3,
  (SELECT COUNT(*) AS n_stints FROM batting GROUP BY playerID, yearID HAVING n_stints > 1) stintful
  ;

SELECT COUNT(*), stints FROM 
(SELECT GROUP_CONCAT(stint) AS stints FROM batting GROUP BY playerID, yearID) t1
GROUP BY stints;

SELECT * FROM bat_war WHERE playerID LIKE "%purceda01%";
SELECT GROUP_CONCAT(stint) AS stints, batting.*  FROM batting GROUP BY playerID, yearID HAVING stints = "2" OR stints = "3";


SELECT bw.*
FROM bat_war bw
LEFT JOIN batting bl ON bw.playerID = bl.playerID AND bw.yearID = bl.yearID AND bw.stint = bl.stint
WHERE bl.playerID IS NULL AND bw.yearID != 2013


=== Somewhere

* Strings That Include Quotes or Special Characters

=== FOREACH to transform records individually

* modify the contents of records individually with FOREACH
  - ba, slg, from rate stats
* Binning records (See Statistics Chapter for more)
* coalesce
* ternary
* String Relative Ordering
  - Generate pairs of teams, use ternary to choose lexicographic firstmost


=== Blowing up data

* String/Collection decomposition Decomposing or Combining Strings
  - generating chars: str.split("(?!^)")
* Ungrouping operations (FOREACH..FLATTEN) expand records
  - call ahead to Section on Tidy data by FLATTENing an inline record
* See time series chapter: Discrete interval sampling (convert value-over-range to date-value)
* See text chapter: Wordbag, Flatten
* See statistics chapter: generating data
* See statistics chapter: Transpose data

=== eliminating records or fields

* Filter:
  - players after 1900
  - Testing String Equality: players for Red Sox
  - Substring or Regular Expressions: players named Q.* OR .*lip.* OR Die.*
  - Select at-bats with three straight caught-looking strikes (the most ignominious of outcomes)
  - isNull, isEmpty, vs Boolean test
  - Caution on Floating Point comparisons
* Select Fixed Number of Arbitrary Records (LIMIT)
  - note: Limit doesn't stop reading so only supply a few filenames
  - no "OFFSET" like there is in SQL.
* Note: To select the top K items from a table or from groups of records is covered below
* Select only the fields you need ("projection") using a FOREACH
  - project just the core stats -- Specifying Which Columns to Display and Giving Names to Output Columns
* Sample: see statistics chapter
* Ssee below: JOINs are often used to filter items in one table using fields from another table

=== Splitting into pieces

* Split using filter: Bill James' black ink metric?
    * Write into named files: game logs by team. Warn about files count.
    * Combine small files: (find the worst offender and repair it)
    * case statement?
* splitting into uniform chunks
  - records: use RANK then group on rank mod record size
  - byte size: use HDFS block size?
  - fraction: approximate -- use sort and N reducers
* Files named for key using Piggybank multistorage
* Files Named for explicit filter: Pitchers vs Non-pitchers; hofPlayers, All-stars, all qualified
  - note that it does not short-circuit and their is no "else" clause
  - call ahead to the transpose part of the summarizinator in statistics chapter
* Combine tables with UNION

For sort note a udf to unique (distinct) won't work because keys can be split


== Structural Operations

=== Aggregation for summary statistics

* Group and agg:
    * career stats
    * HR Stats by year
* Summarizing with MIN(), MAX(), SUM(), AVG(), STDDEV, COUNT(), count star, count distinct, byte size, character size, line / word count
* Count vs COUNTSTAR
   - number of missing values using countstar-count
* Fancy `FOREACH` lets you  operate on bags
  - batting average, slg and OPS for career
* GROUP BY ALL
  - explain algebraic aggregators make this ok (but disaster if not algebraic)
  - season-by-season trends
* Note: HAVING not needed, just use a filter after the group by.
* Re-injecting global totals
* Histogram
  - histogram of home runs per season (doesn't need bin)
  - histogram of career games
  - categorical bins for non-categorical data
* Cube and rollup
  - stats by team, division and league

=== Putting tables in context with JOIN and friends

* Join is a Group and Flatten
* Direct Join: Extend Records with Uniquely Matching Records from Another Table
  - hang full names off records from master file
* Many-to-many join: teams to stadiums; players to teams
  - parks: team seasons and count; distinct teams and count
* Sparse join for matching: geo names for stadiums
* Sparse join for filtering: all-star table (hall of fame table?)
* Self-join
* Distinct: players with a unique first name (once again we urge you: crawl through your data. Big data is a collection of stories; the power of its unusual effectiveness mode comes from the comprehensiveness of those stories. even if you aren't into baseball this celebration of the diversity of our human race and the exuberance of identity should fill you with wonder.)
* bag left outer join from DataFu
* Left outer join on three tables: http://datafu.incubator.apache.org/docs/datafu/guide/more-tips-and-tricks.html
* Sparse joins for filtering
    * HashMap (replicated) join
    * bloom filter join
* (add note) Joins on null values are dropped even when both are null. Filter nulls.
* Range query
    * using cross
    * using prefix and UDFs
* Semi-join
* Bitmap index
* Self-join for successive row differences
* Combining Rows in One Table with Rows in Another
* Finding Rows in One Table That Match Rows in Another
* Finding Rows with No Match in Another Table
* Section 12-10 Using a Join to Fill in Holes in a List
* Enumerating a Many-to-Many Relationship
* Comparing a Table to Itself
* Eliminating Duplicates from a Query Result:
    * and from a Self-Join Result Section
    * Eliminating Duplicates from a Table
* Getting the duplicated values -- group by, then emit bags with more than one size

=== Set Operations

* Union (make sure to note it doesn't dedupe and doesn't order)
* Intersect
* Distinct
* Difference (in a but not in b)
* Equality (use symmetric difference)
* Symmetric difference: in A or B but not in A intersect B -- do this with aggregation: count 0 or 1 and only keep 1
* http://datafu.incubator.apache.org/docs/datafu/guide/set-operations.html
* http://www.cs.tufts.edu/comp/150CPA/notes/Advanced_Pig.pdf

* Using DISTINCT to Eliminate Duplicates
* Eliminating rows that have a duplicated value (ie you're not comparing the whole thing)
* Finding Values Associated with Minimum and Maximum Values
* Selecting Only Groups with Certain Characteristics
* Determining Whether Values are Unique

=== Structural Group Operations (ie non aggregating)

* Group flatten regroup
    * OPS+ -- group on season, normalize, reflatten
    * player's highest OPS+: season, normalize, flatten, group on player, top
* Group Elements From Multiple Tables On A Common Attribute (COGROUP)
* GROUP/COGROUP To Restructure Tables
* Self join of table on its next row (eg timeseries at regular sample)
* Working with NULL Values: Negating a Condition on a Column That Contains NULL Values Section; Writing Comparisons Involving NULL in Programs; Mapping NULL Values to Other Values
* Cogroup and aggregate (vs SQL Cookbook 3.10)
* Using DISTINCT to Eliminate Duplicates
* Finding Values Associated with Minimum and Maximum Values
* Selecting Only Groups with Certain Characteristics
* Determining Whether Values are Unique
* Finding Rows Containing Per-Group Minimum or Maximum Values
* Computing Team Standings
* Producing Master-Detail Lists and Summaries
* Find Overlapping Rows
* Find Gaps in Time-Series..
* Find Missing Rows in Series / Count all Values
* Normalize Denormalized
* Denormalize Normalized
* Transpose Numeric Data
* Calculating Differences Between Successive Rows
* Finding Cumulative Sums and Running Averages
* Section 13.3. Per-Group Descriptive Statistics Section
* Counting Missing Values

=== Sorting and Ordering

* Operations on the order of records: Sorting, Shuffling, Ranking and Numbering
  - ORDER: multiple fields
  - (how do NULLs sort?)
  - RANK: Dense, not dense
  - ASC / DESC
  - in SQL you can omit the sort expression from the table; fields must be there in Pig
* Note
* Top K:
    * whole table: most hr in a season
    * most hr season-by-season

* Top K Records within a table using ORDER..LIMIT
    * Top K Within a Group using GROUP...FOREACH GENERATE TOP
  - middle K (LIMIT..OFFSET)
* Number records with a serial or unique index
* Running total http://en.wikipedia.org/wiki/Prefix_sum
* prefix sum value; by combining list ranking, prefix sums, and Euler tours, many important problems on trees may be solved by efficient parallel algorithms.[3]
* Shuffle a set of records
    * See notes on random numbers.
    * Don't use the pig ORDER operation for this (two passes) (can you count on the built-in sort?)
* Sorting a Result Set
* Selecting Records from the Beginning or End of a Result Set
* Pulling a Section from the Middle of a Result Set
* Calculating LIMIT Values from Expressions
* What to Do When LIMIT Requires the "Wrong" Sort Order
* Sorting with Order by; Sorting and NULL Values; Controlling Case Sensitivity of String Sorts
* Sorting Subsets of a Table;
* Displaying One Set of Values While Sorting by Another
* Controlling Summary Display Order
* Finding Smallest or Largest Summary Values
* Randomizing a Set of Rows
* Assigning Ranks
* Counting and Identifying Duplicates

=== Graph Operatioms

* Neighborhood extraction
* Graph statistics: degree, clustering coefficient
* symmetrize a graph
* Triangles
* Eulerian Walk
* Connected components, Union find
* Graph matching
* Minimum spanning tree
* Pagerank
* label propagation
* k-means clustering
* Layout / Lgl
* List all children of AAA

=== Time Series Operations

* Interval coalesce: given a set of intervals, what is the smallest set of intervals that covers all of them?
    * for each team, what is the smallest number of stints (continuous player for team) needed so that every player was a teammate of one of them for that team? http://www.dba-oracle.com/t_sql_patterns_interval_coalesce.htm
* Turn player-seasons into stints (like the sessionize operation I think)
* Sessionize
  - sessionize web logs
  - Continuous game streak

=== Statistics

* Data Generation
* Make Reproducible Random Data - Varying Distribution
* Calculating Linear Regressions or Correlation Coefficients

* Calculate the summary statistics
  - Transpose (datafu) and flatten
  - group on attribute
  - calculate statistics
  - unionize
* Sniff through the data: extrema, mountweazels, exemplars
* Make a histogram
  - by scale and mod
  - by log scale and mod
  - by lookup table
  - by Z-score
  - equal-width
* Plot it: time series, trellis plot

* Summarizing with COUNT(), count star, count distinct, MIN(), MAX(), SUM(), AVG(), byte size, character size, line / word count
* Number of Distinct elements (Cardinality)
  - count distinct
  - hyperloglog
* Sum, sumsq, Entropy, Standard Deviation, variance, moments (eg GINI)
  - Correlation /covariance: what rate stats go with game time temp?
* Streaming moments (see Alon, Matias, and Szegedy)
* Histogram
  - quantiles
  - Median (approx, exact)
* Heavy hitters -- Count-Min sketch
* Running averages
* note: see below for Graph summaries



=== Advanced Patterns

* True if NONE Match: Find all rows in TABLE1 where there are no rows in TABLE2 that have a T2C value equal to the current T1A value in the TABLE1 table:
* True if ten match: Find all rows in TABLE1 where there are exactly ten rows in TABLE2 that have a T2B value equal to the current T1A value in the TABLE1 table
* Entity-Attribute-Value: bad idea in SQL
* Vertical and horizontal partitioning
* Serial ids -- natural ids
* Composite keys, foreign keys
* Small record with large blob (eg video file and metadata)
* Using float data type when you should use fixed point
* Group by has functionally dependent value (ie we know all elements of bag have same value for group)

* Pivot
* Histogram
* Skyline query (elements not dominated)
    * eliminate all players with no claim to be the best ever: their full set of core stats are less than some other player's full set of core stats. Related to convex hull http://www.cs.umd.edu/class/spring2005/cmsc828s/slides/skyline.pdf
    * like the hipmunk "agony" ranking
    * http://projekter.aau.dk/projekter/files/77335632/Scientific_Article.pdf - do this with quad keys - http://www.vldb.org/pvldb/vol6/p2002-shim.pdf
* Relational division
    * for each job listing (table of name, qualification pairs), find applicants who have all job qualifications (table is listing if, qualification pairs)
    * an applicant who is not qualified has one (listing, qual) pair missing
    * or use counting?
* Outer union
* Complex constraint
* Nested intervals
* Transitive closure
* Hierarchical total
* Small result set from a few tables with specific criteria applied to those tables
* Small result set based on criteria applied to tables other than the data source tables
* Small result set based on the intersection of several broad criteria
* Small result set from one table, determined by broad selection criteria applied to two or more additional tables
* Large result set
* Result set obtained by self-joining on one table
* Result set obtained on the basis of aggregate function(s)
* Result set obtained by simple searching or by range searching on dates
* Result set predicated on the absence of other data


* Chapter 1 - Counting in SQL
    * List of patterns
    * Introduction to SQL Counting
    * Counting Ordered Rows
    * Conditional Summation with CASE Operator
    * Indicator and Step Functions
    * A Case for the CASE Operator
    * Summarizing by more than one Relation
    * Interval Coalesce
* Chapter 2 - Integer Generators in SQL
    * Integers Relation
    * Recursive With
    * Big Table
    * Table Function
    * Cube
    * Hierarchical Query
    * String Decomposition
    * Enumerating Pairs
    * Enumerating Sets of Integers
    * Discrete Interval Sampling
* Chapter 3 - Exotic Operators in SQL
    * Introduction to SQL exotic operators
    * List Aggregate
    * Product
    * Factorial
    * Interpolation
    * Pivot
    * Symmetric Difference
    * Histograms in SQL
    * Equal-Width Histogram
    * Equal-Height Histogram
    * Logarithmic Buckets
    * Skyline Query
    * Relational Division
    * Outer Union
* Chapter 4 - SQL Constraints
    * Function Based Constraints
    * Symmetric Functions
    * Materialized View Constraints
    * Disjoint Sets
    * Disjoint Intervals
    * Temporal Foreign Key Constraint
    * Cardinality Constraint
* Chapter 5 - Trees in SQL
    * Materialized Path
    * Nested Sets
    * Interval Halving
    * From Binary to N-ary Trees
    * Matrix Encoding
    * Parent and Children Query
    * Nested Intervals
    * Descendants Query
    * Ancestor Criteria
    * Ancestors Query
    * Converting Matrix to Path
    * Inserting Nodes
    * Relocating Tree Branches
    * Ordering
    * Exotic Labeling Schemas
    * Dietz Encoding
    * Pre-order – Depth Encoding
    * Reversed Nesting
    * Ordered Partitions
* Chapter 6 - Graphs in SQL
    * Schema Design
    * Tree Constraint
    * Transitive Closure
    * Recursive SQL
    * Connect By
    * Incremental Evaluation
    * Hierarchical Weighted Total
    * Generating Baskets
    * Comparing Hierarchies



Credits

* Art of SQL
* SQL patterns
* Baseball hacks
* MySQL patterns
* SQL Design Patterns http://www.rampant-books.com/book_0601_sql_coding_styles.htm http://www.nocoug.org/download/2006-11/sql_patterns.ppt
* DB2 cookbook
* Patterns for improving runtime: http://www.idi.ntnu.no/~noervaag/papers/VLDBJ2013_MapReduceSurvey.pdf

Instead of counting with the count( ) function, we can, at the
same time as we compute the total count, add 1 if amount_diff is not 0, and 0 otherwise.

==== combining into fewer files

=== SQL-to-Pig-to-Hive

* SELECT..WHERE
* SELECT...LIMit
* GROUP BY...HAVING
* SELECT WHERE... ORDER BY
* SELECT WHERE... SORT BY (just use reducer sort) ~~ (does reducer in Pig guarantee this?)
* SELECT … DISTRIBUTE BY … SORT BY ...
* SELECT ... CLUSTER BY (equiv of distribute by X sort by X)
* Indexing tips
* CASE...when...then
* Block Sampling / Input pruning
* SELECT country_name, indicator_name, `2011` AS trade_2011 FROM wdi WHERE (indicator_name = 'Trade (% of GDP)' OR indicator_name = 'Broad money (% of GDP)') AND `2011` IS NOT NULL CLUSTER BY indicator_name;

SELECT columns or computations FROM table WHERE condition GROUP BY columns HAVING condition ORDER BY column  [ASC | DESC] LIMIT offset,count;
