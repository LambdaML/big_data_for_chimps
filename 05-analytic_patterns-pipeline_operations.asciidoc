== Analytic Patterns part 1: Pipeline Operations

Now that you've met the fundamental analytic operations -- in both their map/reduce and table-operation form -- it's time to put them to work in an actual data exploration.

This chapter will equip you to think tactically, to think in terms of the changes you would like to make to the data. Each section introduces a repeatedly-useful data transformation pattern, demonstrated in Pig (and, where we'd like to reinforce the record-by-record action, in Wukong as well).

Most of the operations in this chapter require no reduce phase of their own. When they stand alone, they lead to map-only jobs. When they are combined with the grouping / sorting operations you'll meet in the next chapter, they become additional pipeline stages of the mapper or reducer (depending on whether they come before or after).

The section on "pattern in use" will become most useful once you've read the book a first time and (we hope) begin using it as your go-to reference. If you find the level of detail here to be a bit intense, skim past them on the first reading. footnote:[The authors' universal experience is that when we do come back, we read past at least problem that we wasted two days figuring out on our own... But of course it was only by figuring out that problem that the other things became intelligible anyway.]

Most of the examples in this chapter will involve our baseball dataset, for which we provide a thorough description of the dataset in [TODO: ref to sidebar]. Even if you're not a baseball fan, we expect you'll learn a lot about data pipelines here.

=== Eliminating Data

For this first round, we'll focus on patterns that somehow shrink your dataset.  This may sound  counterintuitive to the novice ear: isn't the whole point of "Big Data" that we get to work with the entire dataset at once? We finally develop models based on the entire population, not a sample thereof, so why should we scale down our data?

Truth be told, even if you work with every _record_ in a dataset, you may be interested in a subset of _fields_ relevant to your research.  For reasons of memory and computational efficiency, and also your sanity, you'd do yourself a favor to immediately trim a working dataset down to just those records and fields relevant to the task at hand. footnote:[This will certainly simplify debugging.  It also plays to Q's favorite refrain of, _know your data_.  If you're working on a dataset and there are additional fields or records you don't plan to use, can you be certain they won't somehow creep into your model?  The worst-case scenario here is what's called a feature leak, wherein your target variable winds up in your training data. (In essence: imagine saying you can predict today's high temperature, so long as you are first provided today's high temperature.) A feature leak can lead to painful surprises when you deploy this model to the real world.]  Furthermore, you may wish to test some code on a small sample before unleashing it on a long-running job. footnote:[This is generally a good habit to develop, especially if you're one to kick off jobs before leaving the office, going to bed, or boarding a long-haul flight.]  Last, but not least, you may want to draw a random sample just to spot-check a dataset when it's too computationally expensive to inspect every element.

If you're uncomfortable with the notion of _eliminating_ data, then consider these techniques for being _selective_ about your data.

=== Selecting Rows that Satisfy a Condition: FILTER

The first step to eliminating (or being very selective about) data is to remove records that don't match certain criteria. Pig's `FILTER` statement does this for you.  Technically it doesn't remove the data; it creates a new collection that omits certain records from the original dataset.

We've prepared the players' offensive stats in per-season (`bat_season`) and career (`bat_career`) forms. The `people` table gives personal stats such as height and weight, birth year, and so forth. Additionally, there are tables for each team season (`teams`), the stadiums they played in (`park_team_years`), and individual games (`games`).

The stats go back to 1871 (!), but it took a few decades for the game to reach its modern form.  Let's say we're only interested in seasons since 1900.  If we were using a relational database, we'd use the handy `SELECT` statement such as:

------
SELECT bat_season.* FROM bat_season WHERE year_id >= 1900;
------

In Pig, the equivalent `FILTER` syntax would be:

------
player_year_stats = LOAD 'data/sports/baseball/bat_seasons.tsv'    AS (
    player_id:chararray, year_id:int, team_id:chararray,
    lg_id:chararray, age: int,  G:int,     PA:int,    AB:int, ...(omitted)...);

modern_stats = FILTER bats BY (year_id >= 1900);

STORE modern_stats INTO 'data/out/modern_stats';
------

==== Selecting Records that Satisfy Multiple Conditions

The range of conditional expressions you'd expect are present. Use `==` (double-equals) to express an equality condition footnote:[SQL users take note: `==`, not `=`]; use `AND`, `OR`, `NOT` to combine conditional statements footnote:[programmers take note: `AND`, not `&&`]. In a data exploration, you may want to eliminate subjects with sparse data, either to eliminate small-sample-size artifacts, or because they are not in the focus of interest. Major-league players come to bat a bit over 4 times a game on average in a season of 154 to 162 games (it increased in 1960), and so 450 plate appearances (roughly 2/3 of the maximum)footnote:[450 PA is close to the "qualified" season threshold of 3.1 plate appearances per team game that are required for seasonal performance awards] captures the idea of a regular player while allowing for injury or situational replacement. The following filters for what we'll call "qualified modern seasons" (regular players competing in the modern era and either of the two modern leagues) footnote:[In this and in further scripts, we're going omit the `LOAD`, `STORE` and other boilerplate except to prove a point. See the example code (REF) for fully-working snippets]:

------
modsig_stats = FILTER bats BY
  (PA >= 450) AND (year_id >= 1900) AND ((lg_id == 'AL') OR (lg_id == 'NL'));
------

==== Selecting Records that Match a Regular Expression

It's also possible to employ pattern matching against string values. Regular expressions are incredibly powerful and we urge all readers to acquire basic familiarity. There's no better path to mastery than the http://regexp.info[regexp.info] website, and we've provided a brief cheatsheet at the end of the book (REF). This operation uses a regular expression to select players whose names are either similar to your authors' names:

------
-- Name contains a Q; is `Flip` or anything in the Philip/Phillip/... family. (?i) means be case-insensitive:
namesakes = FILTER people BY (nameFirst MATCHES '(?i).*(q|flip|phil+ip).*');
------

It's easy to forget that people's names can contain spaces, dots, dashes, apostrophes; start with lowercase letters or apostrophes, and have accented or other non-latin characters footnote:[This demonstrates the general principle that If you believe a thing involving people will be simple, you're probably wrong.]. For a less silly example, this snippet extracts all names which do not start with a capital letter or which contain a non-word non-space character:

------
funnychars = FILTER people BY (nameFirst MATCHES '^([^A-Z]|.*[^\\w\\s]).*');
------

There aren't any baseball players whose names are represented as starting with a lowercase character footnote:[None of the Nobel Prize-winning physicists Gerard 't Hooft, Louis-Victor Pierre Raymond de Broglie, or Tomonaga Shin'ichirō ever made the major leagues. Or tried out as far as we know. But their names are great counter-examples to keep in mind when dealing with names. Prof de Broglie's full name is 38 characters long, has a last name that starts with a lowercase letter, and is non-trivial to segment. "Tomonaga" is a family name, though it comes first. You'll see Prof. Tomonaga's name given variously as "Tomonaga Shin'ichirō", "Sin-Itiro Tomonaga", or "朝永 振一郎", each correct and the others not in appropriate context. And Prof. 't Hooft\'s last name starts with an apostrophe, a lower-case-letter, and a space.], but in early drafts of the book it caught the value "nameFirst" -- showing that header rows from a source datafile had contaminated the table. Sanity checks like these are a good idea always, and when you have billions of records your one-in-a-million exceptions appear thousands of times.

.Important Notes about String Matching
******
* Regular expressions in Pig are supplied to the MATCHES operator as plain strings. A single backslash serves the purposes of the string literal and does not appear in the string sent to the regexp engine. To pass along the shorthand `[^\\w\\s]` (non-word non-space characters), we have to use two backslashes.
* Yes, that means matching a literal backslash in the target string is done with four backslashes: `\\\\`!
* Options for matching are supplied within the string. For example, `(?i)` matches without regard to case (as we did above), `(?m)` to do multi-line matches, and so forth -- see the documentation.
* Pig Regular Expressions are implicitly anchored at the beginning and end of the string, the equivalent of adding `^` at the start and `$` at the end. (This mirrors Java but is unlike most other languages.) Use `.*` at both ends, as we did above, to regain the conventional "greedy" behavior. Supplying explicit `^` or `$` when intended is a good habit for readability.
* `MATCHES` is an expression, like `AND` or `==` -- you write `str MATCHES regexp`.  The other regular expression mechanisms you'll meet are functions -- you write `REGEX_EXTRACT(str, regexp, 1)`. You will forget we told you so the moment you finish this book.
* In the crop of results: Peek-A-Boo Veach, Quincy Trouppe, and Flip Lafferty.
* You're allowed to have the regular expression be a value from the record, though Pig is able to pre-compile a constant (literal) regexp string for a nice speedup.
* Pig doesn't offer an exact equivalent to the SQL `%` expression for simple string matching. The rough equivalents are dot-star (`.*`) for the SQL `%` (zero or more arbitrary characters), dot (`.`) for the SQL `_` (a single character); and square brackets (e.g. `[a-z]`) for a character range, similar to SQL.
* The string equality expression is case sensitive: `'Peek-A-Boo'` does not equal `'peek-a-boo'`  For case-insensitive string matching, use the `EqualsIgnoreCase` function: `EqualsIgnoreCase('Peek-A-Boo', 'peek-a-boo')` is true. This simply invokes Java's `String.equalsIgnoreCase()` method and does not support regular expressions.
******
==== Matching Records against a Fixed List of Lookup Values

If you plan to filter by matching against a small static list of values, Pig offers the handy `IN` expression: true if the value is equal (case-sensitive) to any of the listed values. This selects the stadiums used each year by the current teams in baseball's AL-east division:

------
al_east_parks = FILTER park_team_years BY
  team_id IN ('BAL', 'BOS', 'CLE', 'DET', 'ML4', 'NYA', 'TBA', 'TOR', 'WS2');
------

When the list grows somewhat larger, an alternative is to read it into a set-membership data structure footnote:[For a dynamic language such as Ruby, it can often be both faster and cleaner to reformat the table into the language itself than to parse a data file. Loading the table is now a one-liner (`require "lookup_table"`), and there's nothing the Ruby interpreter does faster than interpret Ruby.], but ultimately large data sets belong in data files.

The general case uses a join, as described in the next chapter (REF) under "Selecting Records Having a Match in Another Table (semi-join)", and see in particular the specialized merge join and HashMap (replicated) join, which can offer a great speedup. Finally, in the case that the list is extremely large, but few elements are expected to match, a Bloom Filter may be appropriate. They're discussed more in the statistics chapter, where use a Bloom Filter to match every phrase in a large document set against a large list of place names, effectively geolocating the documents.

=== Project Only Chosen Columns by Name

While a `FILTER` selects _rows_ based on an expression, Pig's `FOREACH` selects specific _fields_ chosen by name. The fancy word for this simple action is "projection". We'll try to be precise in using _project_ for choosing columns, _select_ for choosing rows by any means, and _filter_ where we specifically mean selecting rows that satisfy a conditional expression.

The tables we're using come with an overwhelming wealth of stats, but we only need a few of them to do fairly sophisticated explorations. The gamelogs table has more than 90 columns; to extract just the teams and the final score, use a FOREACH:

------
game_scores = FOREACH games GENERATE
  away_team_id, home_team_id, home_runs_ct, away_runs_ct;
------

You're not limited to simply restricting the number of columns; you can also rename and reorder them in a projection. Each row in the table above has _two_ game outcomes, one for the home team and one for the away team. We can represent the same data in a table listing outcomes purely from each team's perspective:

----

games_a = FOREACH games GENERATE
  year_id, home_team_id AS team,
  home_runs_ct AS runs_for, away_runs_ct AS runs_against, 1 AS is_home:int;
games_b = FOREACH games GENERATE
  away_team_id AS team,     year_id,
  away_runs_ct AS runs_for, home_runs_ct AS runs_against, 0 AS is_home:int;

team_scores = UNION games_a, games_b;

DESCRIBE team_scores;
-- team_scores: {team: chararray,year_id: int,runs_for: int,runs_against: int,is_home: int}
----

The first projection puts the `home_team_id` into the team slot, renaming it `team`; retains the `year_id` field unchanged; and files the home and away scores under `runs_for` and `runs_against`. Lastly, we slot in an indicator field for home games, supplying both the name and type as a matter of form. Next we generate the corresponding table for away games, then stack them together with the `UNION` operation (to which you'll be properly introduced in a few pages). All the tables have the identical schema shown, even though their values come from different columns in the original tables.

=== Select a Random Sample of Records

Another common operation is to extract a _uniform_ sample -- one where every record has an equivalent chance of being selected.  For example, you could use this to test new code before running it against the entire dataset (and possibly having a long-running job fail due to a large number of mis-handled records).  By calling the `SAMPLE`operator, you ask Pig to pluck out some records at random.

The following Pig code will return a randomly-selected 25% (that is, 25/100 = 0.25) of the records from our baseball dataset:

---
some_seasons_samp = SAMPLE bat_seasons 0.0625;
---

The `SAMPLE` operation does so by generating a random number to select records, which means each call to `SAMPLE` should yield a different set of records.  Sometimes this is what you want, or in the very least, you don't mind.  In other cases, you may want to draw a uniform sample once, then repeatedly work through those _same_ records.  (Consider our example of spot-checking new code against a dataset: you'd need to run your code against the same sample in order to confirm your changes work as expected.)

Experienced software developers will reach for a "seeding" function -- such as R's `set.seed()` or Python's `random.seed()` --  to make the randomness a little less so.  At the moment, Pig does not have an equivalent function.

==== Extracting a Consistent Sample of Records by Key

Another way to stabilize the sample from run to run is to use a "consistent hash digest". A hash digest function creates a fixed-length fingerprint of a string whose output is otherwise unpredictable from the input and uniformly distributed -- you can't tell which string the function will produce except by computing the digest, and every string is equally likely. For example, the function we've chosen gives the hexadecimal-string digest `3ce3e909` for 'Chimpanzee' but `07a05f9c` for 'Chimp'. Since all hexadecimal strings are equally likely, one-sixteenth of them will start with a zero.

------
DEFINE Digest  datafu.pig.hash.Hasher('murmur3-32');

keyed_seasons = FOREACH bat_seasons GENERATE Digest(player_id) AS keep_hash, *;

some_seasons  = FOREACH (
    FILTER keyed_seasons BY (SUBSTRING(keep_hash, 0, 1) == '0')
  ) GENERATE $0..;
------

The virtues of this script are that it
To select each row consistently, use the

We'll have a lot more to say about sampling in the Statistics chapter (REF).
==== Retrieve a fixed number of Rows (LIMIT)

A much blunter way to create a smaller dataset is to take the first _K_ records of a collection.  (Please note that we emphasize the _first_ _K_ records, not the _top-ranked_ _K_.  We cover that in the next chapter.)  This is similar to running the `head` command in Unix-like operating systems, or using the `LIMIT` clause in a SQL `SELECT` statement.

Pig offers the `LIMIT` operator for this purpose. To select the first 25 records of our `bat_seasons` data, you would run:

----
some_players = LIMIT player_year_stats 25;
----

In the big data regime, where your data is striped across many machines, there's no intrinsic
notion of a row order.  That means, similar to the `SAMPLE` operator, you have no guarantees of which records `LIMIT` will select.  Changes in the number of mappers or reducers, in the data, and so forth, may change which records are selected. Exerting control over which records are selected requires a much more expensive `ORDER BY` operation -- we'll cover that in the next chapter.

NOTE: If you truly don't care which records to select, just point Pig to a single input file.  For example, invoke `LIMIT` on  `some_data/part-00000`, instead of `some_data/` (which will operate on all files in that directory). Pig effectively does this in the loader, making it very efficient.

==== LIMIT .. DUMP to safely display a handful of records

The main use of a LIMIT statement outside of an `ORDER BY..LIMIT` stanza is for interactive development. We hope that someday the DUMP command will gains an intrinsic LIMIT capability, but until then you can write

------
=> LIMIT player_year_stats 25; DUMP @;
------

Keep in mind that the presence anywhere of a DUMP command has hidden consequences, such as disabling parallel execution of different dataflows in a script.


==== Other Data Elimination Patterns

There are two tools we'll meet in the next chapter that can be viewed as data elimination patterns as well. The `DISTINCT` and related operations are used to identify duplicated or unique records. Doing so requires putting each record in context with its possible duplicates -- meaning they are not pure pipeline operations like the others here. Above, we gave you a few special cases of selecting records against a list of values. We'll see the general case -- selecting records with or without a match in another table (also known as semi-join and anti-join) -- when we learn about all the flavors of `JOIN` operations in the next chapter.
=== Transforming Records
==== Transform Records Individually using `FOREACH`

A `FOREACH` lets you develop simple transformations based on each record.

For example, baseball stats don't list the number of singles, only the number of total hits and the number of non-singles (doubles, triples, home runs). This `FOREACH` statement generates the number of singles as its own field:

----
core_stats = FOREACH bat_season GENERATE
    player_id, year_id, team_id,
    G, PA, AB, H,
    H - h2B - h3B - HR AS h1B,
    h2B, h3B, HR
  ;
----

Here's the corresponding SQL command:

----
SELECT
    player_id, year_id, team_id,
    G, PA, AB, H,
    H - h2B - h3B - HR AS h1B,
    h2B, h3B, HR
  FROM bat_season
  ;
----

A `FOREACH` won't cause a new Hadoop job stage: it's chained onto the end of the preceding operation (and when it's on its own, like this one, there's just a single a mapper-only job). A FOREACH always produces exactly the same count of output records as input records.

Within the GENERATE portion of a normal FOREACH, you can apply arithmetic expressions (as shown); project fields (rearrange, rename and eliminate fields); and apply the FLATTEN operator (see below).

==== A nested `FOREACH` Allows Intermediate Expressions

The above are all "counting stats", and generally the more games the more hits and runs and so forth. For comparing players, it's better to use "rate stats" normalized against plate appearances. You can do quite a reasonable job of estimating players' performance using these three metrics:

* 'On-base percentage' (`OBP`), which indicates how well the player becomes a potential run. It is given as the fraction of plate appearances that are successful: (`(H + BB + HBP) / PA`) footnote:[Although known as percentages, OBP and SLG are always given as fractions to 3 decimal places]. An `OBP` over 0.420 is very good (better than 95% of significant seasons).
* 'Slugging Percentage' (`SLG`), which indicates how well the player converts potential runs into runs. It is given by the rate of total bases gained in hitting (one for a single, two for a double, etc): (`(h1B + 2*h2B + 3*h3B + 4*HR) / AB`). (Due to historical circumstances, SLG and some other stats use a restricted subset of PA called At Bats (`AB`); don't worry about the difference.)  An `SLG` over 0.520 is very good.
* 'On-base-plus-slugging' (`OPS`), which combines on-base and slugging percentages to give a simple and useful estimate of overall offensive contribution. It is simply the sum of those two metrics: (`OBP + SLG`). Anything above 0.900 is very good.

Doing this with the simple form of `FOREACH` we've been using would be annoying and hard to read -- for one thing, the expressions for OBP and SLG would have to be repeated in the expression for OPS, since the full statement is evaluated together.

// TODO: should we demonstrate the big yucky FOREACH? → nah, we've described it already.  If they really want to see it, they can cook it up themselves. ;-)

There's a fancier form of `FOREACH` (a 'nested' `FOREACH`) that allows intermediate expressions:

----
bat_seasons = FILTER bat_seasons BY PA > 0 AND AB > 0;
core_stats  = FOREACH bat_seasons {
  h1B  = H - (h2B + h3B + HR);
  HBP  = (HBP IS NULL ? 0 : HBP);
  TB   = h1B + 2*h2B + 3*h3B + 4*HR;
  OBP  = (H + BB + HBP) / PA;
  SLG  = TB / AB;
  OPS  = SLG + OBP;
  GENERATE
    player_id, year_id, team_id, lg_id,
    G,   PA,  AB,  HBP, SH,  BB,
    H,   h1B, h2B, h3B, HR,  R,  RBI,
    SLG, OBP, OPS;
};
----

This alternative `{` curly braces form of `FOREACH` lets you describe its transformations in smaller pieces, rather than smushing everything into the single `GENERATE` clause. New identifiers within the curly braces (such as `player`) only have meaning within those braces, but they do inform the schema.)


In addition to applying arithmetic expressions, projecting and renaming fields, and FLATTENing records, there are a set of Pig operations you can also apply within a nested FOREACH -- you'll see many examples in the next chapter (REF).

Note: We sneakily did two things with the filter above: eliminated rows where `PA` was equal to zero, and also eliminated rows where `PA` was NULL.  For people coming from a SQL background, Pig's handling of NULL values will be fairly familiar. NULL values generally disappear without notice from operations, and generally compare as false -- NULL is not less than, is not greater than, and is not equal to 5.0. For programmers, however, it can be hard to track all this. The rules are well detailed in the Pig manual and somewhat fiddly, so we won't go deep into them here. We've found the best way to learn what you need is to just see lots of examples, which we endeavor to supply in abundance.

==== Place Values into Categorical Bins With a `FOREACH`

One common task is to prepare a categorical field -- one with a small number of potential values -- from non-categorical fields such as ranges of numbers or strings. As an example, let's find out how common it is to reach various milestones for number of hits -- 50, 100, and so forth.

In mathematical syntax, we would say we're breaking the number of hits into 50-unit categorical ranges: [0,50] , (50,100] , (100,150] , and so on.

The SQL version might look like this:

----
SELECT 100*CEIL(H / 100) AS H_bin, COUNT(*), nameCommon
  FROM bat_career bat
  GROUP BY H_bin;
----

In Pig we'll use a `FOREACH` to determine the bin for each record, and then use a `GROUP BY` as described in the next chapter to prepare its histogram:

----
bat_season = LOAD '...' AS (...);
season_binned = FOREACH bat_season GENERATE
    50*CEIL(H / 50) AS H_bin;
season_grouped = GROUP season_binned BY H_bin;
FOREACH season_grouped GENERATE group AS H_bin, COUNTSTAR(season_binned) AS ct;
----



TODO:

  - Transforming Strings with Regular Expressions

  - Sidebar: Working with NULL Values -- TODO-Flip: we briefly mention NULL earlier; should we skip that and do it all here?


  - As we go along:
	- Transforming Nulls into Real Values
	- Converting the Lettercase of a String
	- Converting a Number to its String Representation (and Back) (cast with (int))
	- Embedding Quotes and Special Characters Within String Literals.



TODO-Flip: remainder of this highlighted section: not sure what you mean?
* Applying a User-Defined Function
  - namely, converting a JSON-encoded string value to the actual string: i.e. "Here is a snowman on its own line:\n\u2603" becomes "Here is a snowman on its own line:
☃"
  - TSV and CSV don't let you have any old character you want, but are so wonderfully simple. In cases where you have a blob of text that you don't always want to pay to decode it, we will sometimes use TSV but JSON-encode the value of internal strings. You can be confident that all special characters have been safely taken out of band, it's compact and ubiquitous to decode

* Assigning a Unique Identifier to Each Record (use `-tagPath` when loading; may require most recent Pig)
  - I guess admit to the `$1..` syntax here (to include the rest of the fields after the first). But don't adopt the disrespectful habit of using this to mean "the rest of the fields that I'm too lazy to type in"; only use this for "the rest of the fields, whatever they happen to be")
  - Call forward to RANK, and to Over, for sequentially numbering records



=== Operations that Expand the number of Rows or Columns

If you count all the letters in a large-enough body of text, you'll generally find that the letter "e" (the most frequent) appears about 12% of the time, while z and q (the least frequent) appear less than 1% of the time. But names of people have a noticeably different distribution of characters, as we can demonstrate using the baseball data. The `people` table has two fields representing city names, a first name field and a last name field. We'll find the frequency distribution for each.

==== Flatten on a Bag Generates Many Records from a Field with Many Elements

===== Demonstration in Pig

This snippet first produces a bag pairing each of the `chararray` values we want with the distribution it belongs to, then flattens it.

----
typed_strings = FOREACH people {
  fields_bag = {('fn', nameFirst), ('ln', nameLast), ('ct', birthCity), ('ct', deathCity)};
  GENERATE FLATTEN(fields_bag) AS (type:chararray, str:chararray);
  };
----

Each single record having a bag turns into four records having a field called 'type' and a field called 'str':

----
fn    Hank
ln    Aaron
ct   San Diego
ct   Inverness
----

==== Flatten on a Tuple Folds it into its Parent

Our next step is to split those string fields into characters. Pig provides a `STRSPLIT` function that _seems_ to do what we want (spoiler alert: for this purpose it doesn't, but we want to prove a point).

----
typed_chars = FOREACH typed_strings {
  chars_bag = STRSPLIT(str, '(?!^)');  -- works, but not as we want
  GENERATE type, FLATTEN(chars_bag) AS token;
  };
----

The output we want would have one record per character in the `str` field, but that isn't what happens:

----
fn   H   a   n   k
ln   A   a   r    o   n
...
----

`STRSPLIT` returns a _tuple_, not a _bag_, and the `FLATTEN` operation applied to a tuple does not produce many records from the tuple field, it lifts the elements of the tuple into its container. This `FLATTEN(STRSPLIT(...))` combination is great for, say, breaking up a comma-delimited string into field, but we want to flatten the characters into multiple records. The pigsy package has the UDF we need:

----
register    '...path/to/pigsy/target/pigsy-2.1.0-SNAPSHOT.jar';
DEFINE STRSPLITBAG         pigsy.text.STRSPLITBAG();
-- ...
typed_chars = FOREACH typed_strings {
  chars_bag = STRSPLITBAG(LOWER(str), '(?!^)');
  GENERATE type, FLATTEN(chars_bag) AS token;
  };
----

===== Results

What remains is to group on the characters for each type to find their overall counts, and then to prepare the final results. We'll jump into all that in the next chapter, but (REF) shows the final results. The letters "k", "j", "b" and "y" are very over-represented in first names. The letter "z" is very over-represented in last names, possibly because of the number of Hispanic and Latin American players.

----
char	% dictionary  	% prose		% first names	% excess
a	  8.49		  8.16		 8.31		 1.01
b	  2.07		  1.49		 3.61		 2.00
c	  4.53		  2.78		 3.67		  .80
d	  3.38		  4.25		 4.42		 1.48
e	 11.16		 12.70		11.03		 1.05
f	  1.81		  2.22		 1.43		 1.27
g	  2.47		  2.01		 2.03		  .96
h	  3.00		  6.09		 3.40		 1.23
i	  7.54		  6.96		 6.85		  .78
j	   .19		  0.15		 3.70		 3.14
k	  1.10		  0.77		 3.07		 4.37
l	  5.48		  4.02		 6.29		 1.07
m	  3.01		  2.40		 3.73		 1.21
n	  6.65		  6.74		 6.46		  .92
o	  7.16		  7.50		 6.81		  .89
p	  3.16		  1.92		 1.08		  .31
q	   .19		  0.09		  . 3		  .19
r	  7.58		  5.98		 8.33		 1.15
s	  5.73		  6.32		 3.06		  .49
t	  6.95		  9.05		 4.00		  .58
u	  3.63		  2.75		 1.91		  .49
v	  1.00		  0.97		 1.15		 1.25
w	  1.28		  2.36		  .82		 1.29
x	   .29		  0.15		  .22		  .73
y	  1.77		  1.97		 3.93		 1.68
z	   .27		  0.07		  .19		  .53
----

(TODO insert actual results, and decide which distribution (prose or dictionary) you'll normalize against)

==== Other Similar Patterns

The chapter on text data (REF) shows how to tokenize free text into a "word bag", using both Pig's simplistic `TOKENIZE` function and a UDF that applies a sophisticated computational linguistics library. In the Event Stream chapter (REF), we'll demonstrate dividing time range into discrete intervals. Lastly, the Statistics chapter (REF) describes a script to take summary statistics of all columns simultaneously, which involves transposing a record into attribute-value pairs.

We have much more to say about FLATTEN, but it's best done the next chapter so that we can illustrate our points well.

==== Generating Data

Generating data in a distributed system requires distributing an assignment of what to generate onto each node, which can be somewhat annoying. The surprisingly useful integers table -- 1, 2, 3, ... each on subsequent rows -- provides one way to get around this. We don't really have a good baseball-based example, but we can demonstrate generating the 11 million combinations of five letters using a map-reduce job (or the similar UDF):

----
C2 = 26**2; C3 = 26**3; C4 = 26**4; C5 = 26**5
ORD_A = 'a'.ord

mapper do |line|
  idx = line.to_i
  offsets = [ line / C5, (line / C4) % 26, (line / C3) % 26, (line / C2) % 26, line % 26 ]
  chars = offsets.map{|offset| (ORD_A + offset).chr }
  yield chars.join
end
----

Another example of this pattern is the poor-man's data loader given in Chapter 3 (REF) -- prepare a mapper input that is a list of filenames or database queries, and have each mapper expand its trivial input into many rows of output.

==== Generating Pairs

is there a way to do the SQL version more elegantly?

SELECT
    IF(home_team_id <= away_team_id, home_team_id, away_team_id) AS team_a,
    IF(home_team_id <= away_team_id, away_team_id, home_team_id) AS team_b,
    COUNT(*)
  FROM events ev
GROUP BY home_team_id, away_team_id
ORDER BY home_team_id, away_team_id
;

(do we want to show the group by or call forward to it)

You'll see a more elaborate version of this

COALESCE requires datafu:
define COALESCE datafu.pig.util.Coalesce();
or use ternary: eg (isEmpty(A) ? 0 : First(A))

  - Generating data using the assignment list as input
	- in particular, using the list of URLs or filenames or whatever -- TODO-Flip: not sure what you mean here?
	- just demonstrate with map-reduce only, no pig (unless we decide to use this to show an inline Ruby UDF?)

=== Operations that Break One Table Into Many

==== Splitting into Multiple Data Flows using `SPLIT`

The careers table gives the number of times each player was elected to the All-Star game (indicating extraordinary performance during a season) and whether they were elected to the Hall of Fame (indicating a truly exceptional career).

===== Demonstration in Pig
Separating those records into different data flows isn't straightforward in map/reduce, but it's very natural using Pig's `SPLIT` operation.

----
SPLIT bat_career
  INTO hof     IF hofYear > 0, -- the '> 0' eliminates both NULLs and 0s
  INTO allstar IF G_allstar > 0,
  INTO neither OTHERWISE
  ;
STORE hof     INTO '/data/out/baseball/hof_careers';
STORE allstar INTO '/data/out/baseball/allstar_careers';
STORE neither INTO '/data/out/baseball/neither_careers';
----

The `SPLIT` operator does not short-circuit: every record is tested against every condition, and so a player who is both a hall-of-famer and an allstar will be written into both files.

==== Splitting into files by key by using a Pig Storefunc UDF

If instead you're looking to partition directly into files named for a key, use the multistorage storefunc from the Piggybank UDF collection. As opposed to SPLIT, each record goes into exactly one file. Here is how to partition player seasons by primary team:

----
    bat_season = LOAD 'bat_season' AS (...);
    STORE bat_season INTO '/data/out/baseball/seasons_by_team' USING MultiStorage('/data/out/baseball/seasons_by_team', '10'); -- team_id, field 10
    STORE ... multistorage;
----

The output has a directory for each key, and within directory that the same `part-NNNNN` files of any map-reduce job.

This means the count of output files is the number of keys times the number of output slots, which can lead to severe many small files problem. As mentioned in Chapter 3 (REF), many small files is Not Good. If you precede the STORE operation by a `GROUP BY` on the key, the reducer guarantee provides that each subdirectory will only have one output file.

==== Splitting a Table into Uniform Chunks

We won't go into much detail, but one final set of patterns is to split a table into uniform chunks. If you don't need the chunks to be exactly sized, you can apply a final `ORDER BY` operation on a uniformly-distributed key -- see the section on "Shuffling the Records in a Table" in the next chapter (REF).

To split into chunks with an exact number of lines, first use `RANK` to number each line, then prepare a chunk key using the line number modulo the chunk size, and store into chunks using MultiStorage. Since the rank operation's reducers number their records sequentially, only a few reducers are involved with each chunk, and so you won't hit the small files problem. Splitting a table into blocks of fixed _size_ is naturally provided by the HDFS block size parameter, but we're not aware of a good way to do so explicitly.

=== Operations that Treat the Union of Several Tables as One

The counterpart to splitting a table into pieces is to treat many pieces as a single table. This really only makes sense when all those pieces have the same schema, so that's the only case we'll handle here.

==== Load Multiple Files as One Table

The easiest way to unify several tables is to simply load them as one. Hadoop will expand a comma-separated list of paths into multiple paths, and perform simple 'glob-style' filename expansion. This snippet will load all the teams whose team_id starts with a "B" or ends with an "N":

===== Demonstration in Pig

----
b_and_n_teams = LOAD '/data/out/baseball/seasons_by_team/B*,/data/out/baseball/seasons_by_team/*N' AS (...);
----

===== Demonstration in map/reduce

----
(show commandline for multiple files)
----

==== Treat Several Pig Relation Tables as a Single Table (Stacking Rowsets)

In Pig, you can rejoin several pipelines using the `UNION` operation. The tables we've been using so far cover only batting stats; there are another set of tables covering stats for pitchers, and in rare cases a player may only appear in one or the other. To find the name and id of all players that appear in either table, we can project the fields we want (earning a uniform schema) and then unify the two streams:

----
bat_career = LOAD '/data/rawd/baseball/sports/bat_career AS (...);
pit_career = LOAD '/data/rawd/baseball/sports/pit_career AS (...);
bat_names = FOREACH bat_career GENERATE player_id, nameFirst, nameLast;
pit_names = FOREACH pit_career GENERATE player_id, nameFirst, nameLast;
names_in_both = UNION bat_names, pit_names;
player_names = DISTINCT names_in_both;
----

The `UNION` operation does not remove duplicate rows as a set-wise union would. It simply tacks one table onto the end of the other, and so the last line eliminates those duplicates -- more on `DISTINCT` in the next chapter (REF). The `UNION` operation also does not provide any guarantees on ordering of rows. Some SQL users may fall into the trap of doing a UNION-then-GROUP to combine multiple tables. This is terrible in several ways, and you should instead use the COGROUP operation -- see the Won-Loss Record example in the next chapter (REF).

==== Clean Up Many Small Files by Merging into Fewer Files

The Many Small Files problem is so pernicious because Hadoop natively assigns each mapper to only one file, and so a normal mapper-only job can only _increase_ the number of files. We know of two ways to reorganize the records of a table into fewer files.

One is to perform a final `ORDER BY` operation footnote:[The tuning chapter (REF) tells you why you might want to increase the HDFS block size for truly huge dataset, and why you might not want to do so]. Since this gives the side benefit of allowing certain optimized join operations, we like to do this for "gold" datasets that will be used by many future jobs.

Sorting is a fairly expensive operation, though; luckily, Pig can do this reasonably well with a mapper-only job by setting the `pig.splitCombination` configuration to true and setting `pig.maxCombinedSplitSize` to the size of the input divided by the number of files you'd like to produce.

----
set pig.splitCombination true;
set pig.maxCombinedSplitSize 2100100100;
----

The `maxCombinedSplitSize` should be much larger than the HDFS block size so that blocks are fully used. Also note the old sailor's trick in the last line -- since there's no essential difference between 2 billion bytes, 2 gigabytes, or a number nearby, the value `2100100100` is much easier to read accurately than `2000000000` or `2147483648`.

The operations in this chapter (except where noted) do not require a reduce on their own, which makes them very efficient. The really interesting applications, however, come when we put data into context, which is the subject of the next chapter.




.A Quick Look into Baseball
****
Nate Silver calls Baseball the "perfect data set".  There are not many human-centered systems for which this comprehensive degree of detail is available, and no richer set of tables for truly demonstrating the full range of analytic patterns.

For readers who are not avid baseball fans, we provide a simple -- some might say "oversimplified" -- description of the sport and its key statistics.  Please refer to Joseph Adler's _Baseball Hacks_ (O'Reilly) or [TODO the one with Baseball and R] for more details.


*Acronyms and terminology*

We use the following acronyms (and, coincidentally, field names) in our baseball dataset:

* `G`, 'Games'
* `PA`: 'Plate Appearances', the number of completed chances to contribute offensively
* `H`: 'Hits', either singles (`h1B`), doubles (`h2B`), triples (`h3B`) or home runs (`HR`)
* `BB`: 'Walks', pitcher presented too many unsuitable pitches
* `HBP`: 'Hit by Pitch', like a walk but more painful
* `OBP`: 'On-base Percentage', indicates effectiveness at becoming a potential run
* `SLG`: 'Slugging Percentage', indicates effectiveness at converting potential runs into runs
* `OPS`: 'On-base-plus-Slugging', a reasonable estimate of overall offensive contribution

For those who consider sporting events to be the dull province of jocks, holding no interest at all: when we say the "On-Base Percentage" is a simple matter of finding `(H + BB + HBP) / AB`, just trust us that (a) it's a useful statistic; (b) that's how you find its value; and then (c) pretend it's the kind of numbers-in-a-table example abstracted from the real world that many books use.


*The rules and goals*

Major League Baseball teams play a game nearly every single day from the start of April to the end of September (currently, 162 per season). The team on offense sends its players to bat in order, with the goal of having its players reach base and advance the full way around the diamond. Each time a player makes it all the way to home, their team scores a run, and at the end of the game, the team with the most runs wins. We count these events as `G` (games), `PA` (plate appearances on offense) and `R` (runs).


The best way to reach base is by hitting the ball back to the fielders and reaching base safely before they can retrieve the ball and chase you down -- a hit (`H`) . You can also reach base on a 'walk' (`BB`) if the pitcher presents too many unsuitable pitches, or from a 'hit by pitch' (`HBP`) which is like a walk but more painful. You advance on the basepaths when your teammates hit the ball or reach base; the reason a hit is valuable is that you can advance as many bases as you can run in time. Most hits are singles (h1B), stopping safely at first base. Even better are doubles (`h2B`: two bases), triples (`h3B`: three bases, which are rare and require very fast running), or home runs (`HR`: reaching all the way home, usually by clobbering the ball out of the park).

So your goal as a batter is both becomes a potential run and helps to convert players on base into runs. If the batter does not reach base it counts as an out, and after three outs, all the players on base lose their chance to score and the other team comes to bat. (This threshold dynamic is what makes a baseball game exciting: a single pitch can swing the score by or squander the offensive efforts


*Additional stats*

The above are all "counting stats", and generally the more games the more hits and runs and so forth. For comparing players, it's better to use "rate stats" normalized against plate appearances.

For historical reasons, some stats use a restricted subset of PA called AB (At Bats). You should generally prefer PA to AB.

'On-base percentage' (`OBP`) indicates how well the player becomes a potential run, given as the fraction of plate appearances that are successful: (`(H + BB + HBP) / PA`) footnote:[Although known as percentages, OBP and SLG are always given as fractions to 3 decimal places]. An `OBP` over 0.420 is very good (better than 95% of significant seasons).

'Slugging Percentage' (`SLG`) indicates how well the player converts potential runs into runs. It is given by the total bases gained in hitting (one for a single, two for a double, etc) divided by the number of at bats: (`(H + h2B + 2*h3B + 3*HR) / AB`). An `SLG` over 0.520 is very good.

'On-base-plus-slugging' (`OPS`) combines on-base and slugging percentages to give a simple and useful estimate of overall offensive contribution: (`OBP + SLG`). Anything above 0.900 is very good.
****


.Pig Gotchas
****

**"dot or colon?"**

Some late night under deadline, Pig will supply you with the absolutely baffling error message "scalar has more than one row in the output". You've gotten confused and used the tuple element operation (`players.year`) when you should have used the disambiguation operator (`players::year`). The dot is used to reference a tuple element, a common task following a `GROUP`. The double-colon is used to clarify which specific field is intended, common following a join of tables sharing a field name.


Where to look to see that Pig is telling you have either nulls, bad fields, numbers larger than your type will hold or a misaligned schema.

Things that used to be gotchas, but aren't, and are preserved here just through the tech review:

* You can rename an alias, and refer to the new name: `B = A;` works. (v10)
* LIMIT is handled in the loader, and LIMIT accepts an expression (v10)
* There is an OTHERWISE (else) statement on SPLIT! v10
* If you kill a Pig job using Ctrl-C or “kill”, Pig will now kill all associated Hadoop jobs currently running. This is applicable to both grunt mode and non-interactive mode.
* In next Pig (post-0.12.0),
  - CONCAT will accept multiple args
  - store can overwrite existing directory (PIG-259)


**"Good Habits of SQL Users That Will Mess You Up in Hadoop"**

* Group/Cogroup is king; Join is a special case
* Window functions are a recent feature -- use but don't overuse Stitch/Over.
* Everything is immutable, so you don't need and can't have transactional behavior



TODO: fill this in with more gotchas

****

. A Foolish Optimization
****
TODO: Make this be more generally "don't use the O(N) algorithm that works locally" -- fisher-yates and top-k-via-heap being two examples
TODO: consider pushing this up, earlier in the chapter, if we find a good spot for it

We will tell you about another "optimization," mostly because we want to illustrate how a naive performance estimation based on theory can lead you astray in practice. In principle, sorting a large table in place takes 'O(N log N)' time. In a single compute node context, you can actually find the top K elements in 'O(N log K)' time -- a big savings since K is much smaller than N. What you do is maintain a heap structure; for every element past the Kth, if it is larger than the smallest element in the heap, remove the smallest member of the heap and add the element to the heap. While it is true that 'O(N log K)' beats 'O(N log N)', this reasoning is flawed in two ways. First, you are not working in a single-node context; Hadoop is going to perform that sort anyway. Second, the fixed costs of I/O almost always dominate the cost of compute (FOOTNOTE:  Unless you are unjustifiably fiddling with a heap in your Mapper.)

The 'O(log N)' portion of Hadoop's log sort shows up in two ways:  The N memory sort that precedes a spill is 'O(N log N)' in compute time but less expensive than the cost of spilling the data. The true 'O(N log N)' cost comes in the reducer: 'O(log N)' merge passes, each of cost 'O(N)'. footnote:[If initial spills have M records, each merge pass combines B spills into one file, and we can skip the last merge pass, the total time is `N (log_B(N/M)-1).` [TODO: double check this]. But K is small, so there should not be multiple merge passes; the actual runtime is 'O(N)' in disk bandwidth. Avoid subtle before-the-facts reasoning about performance; run your job, count the number of merge passes, weigh your salary against the costs of the computers you are running on, and only then decide if it is worth optimizing.
