== Analytic Patterns part 1: Pipeline Operations

Now that you've met the fundamental analytic operations -- in both their map/reduce and table-operation form -- it's time to put them to work in an actual data exploration.

This chapter will equip you to think tactically, to think in terms of the changes you would like to make to the data. Each section introduces a repeatedly-useful data transformation pattern, demonstrated in Pig (and, where we'd like to reinforce the record-by-record action, in Wukong as well).

// The real goal is to learn to think strategically, to be able to look at the data you have and recognize the steps required to produce the data you want. You do not do this, however, by thinking about how to coordinate the fundamental operations you have just learned directly into your solution any more than a general thinks about coordinating the actions of every individual soldier while preparing a battle plan.

Most of the operations in this chapter require no reduce phase of their own. When they stand alone, they lead to map-only jobs. When they are combined with the grouping / sorting operations you'll meet in the next chapter, they become additional pipeline stages of the mapper or reducer (depending on whether they come before or after).

The section on "pattern in use" will become most useful once you're past reading the book and (we hope) using it as your go-to reference. If you find the level of detail here to be a bit intense, skim past them on the first reading. footnote:[The authors' universal experience is that when we do come back, we read past at least problem that we wasted two days figuring out on our own... But of course it was only by figuring out that problem that the other things became intelligible anyway.]

Most of the examples in this chapter will involve our baseball dataset, for which we provide a thorough description of the dataset in [TODO: ref to sidebar]. Even if you're not a baseball fan, we expect you'll learn a lot about data pipelines here.

=== Eliminating Data

For this first round, we'll focus on patterns that somehow shrink your dataset.  This may sound  counterintuitive to the novice ear: isn't the whole point of "Big Data" that we get to work with the entire dataset at once? We finally get to work with the entire population, not a sample thereof, so why should we scale down our data?  

Truth be told, even if you work with every _record_ in a dataset, you may be interested in a subset of _fields_ relevant to your research.  For reasons of memory and computational efficiency, and also your sanity, you'd do yourself a favor to immediately trim a working dataset down to just those records and fields relevant to the task at hand. footnote:[This will certainly simplify debugging.  It also plays to Q's favorite refrain of, _know your data_.  If you're working on a dataset and there are additional fields or records you don't plan to use, can you be certain they won't somehow creep into your model?  The worst-case scenario here is what's called a feature leak, wherein your target variable winds up in your training data. (In essence: imagine saying you can predict today's high temperature, so long as you are first provided today's high temperature.) A feature leak can lead to painful surprises when you deploy this model to the real world.]  Furthermore, you may wish to test some code on a small sample before unleashing it on a long-running job. footnote:[This is generally a good habit to develop, especially if you're one to kick off jobs before leaving the office, going to bed, or boarding a long-haul flight.]  Last, but not least, you may want to draw a random sample just to spot-check a dataset when it's too computationally expensive to inspect every element.

If you're uncomfortable with the notion of _eliminating_ data, then consider these techniques for being _selective_ about your data.

==== Filter Selected Rows Based on an Expression

The first step to eliminating (or being very selective about) data is to remove records that don't match certain criteria. Pig's `FILTER` statement does this for you.  Technically it doesn't remove the data; it creates a new collection that omits certain records from the original dataset.

We've prepared the players' offensive stats in per-season (`bat_season`) and career (`bat_career`) forms. The `people` table gives personal stats such as height and weight, birth year, and so forth. Additionally, there are tables for each team season (`teams`), the stadiums they played in (`park_team_years`), and individual games (`games`).

The stats go back to 1871 (!), but it took a few decades for the game to reach its modern form.  Let's say we're only interested in seasons since 1900.  If we were using a relational database, we'd use the handy `SELECT` statement such as:

----
SELECT bat_season.* FROM bat_season WHERE year_id >= 1900;
----

In Pig, the equivalent `FILTER` syntax would be:

----
modern_stats = FILTER bat_seasons BY (year_id >= 1900);
----

==== Selecting Records that Satisfy Multiple Conditions

You can supply multiple conditions with SQL-like boolean statements footnote:[Note to programmers: `AND`, `OR`, `NOT`; Pig does not support the c-like `&&`/`||`/`!` forms.]. To select player seasons that have a significant number of plate appearances,

------
modsig_stats = FILTER bat_seasons BY
  (PA >= 450) AND (year_id >= 1900) AND ((lg_id == 'AL') OR (lg_id == 'NL'));
------


That example used a mathematical operator to choose records based on a field's numeric value.  It's also possible to employ pattern matching against string values.  SQL provides the the `%` operator for simple string matching, and some database implementations support regular expressions, such as MySQL's `RLIKE`.  This operation uses a regular expression to select players whose names are similar to your authors':

----
SELECT people.* FROM people
  WHERE nameFirst RLIKE "^Q.*" OR nameFirst RLIKE ".*lip.*"; -- TODO maybe a more interesting regexp?
----

Pig supports regular expressions right in the `FILTER` statement. The equivalent Pig call would read as follows:

------
-- Name contains a 'Q', 'Flip', or anything in the Philip/Phillip/... family
-- (?i) means "case insensitive"
namesakes = FILTER people BY (name_first MATCHES '(?i).*(q|flip|phil+ip).*');
------

Java (and therefore Pig) is case sensitive for string operations. It also implicitly anchors regular expressions to match the full string: `"lip"` will only match the string `"lip"`. We used `".*lip.*"`, which will match anything with `lip` as a substring. As an additional caution, we'll remind you to be careful comparing floating-point numbers for equality: use `ABS( (val1 - val2)/val1 ) < tol`, not `val1 = val2`.

This is an important point, so we'll repeat something we said earlier in the chapter: _filter (and otherwise scale down your dataset) as early as possible._  This may sound obvious, but in the next chapter (TODO ref) we'll highlight many non-obvious expressions of this rule.

==== Select a Random Sample of Rows

Another common operation is to extract a _uniform_ sample -- one where every record has an equivalent chance of being selected.  For example, you could use this to test new code before running it against the entire dataset (and possibly having a long-running job fail due to a large number of mis-handled records).  By calling the `SAMPLE`operator, you ask Pig to pluck out some records at random.

The `SAMPLE` operation does so by generating a random number to select records, which means each call to `SAMPLE` should yield a different set of records.  Sometimes this is what you want, or in the very least, you don't mind.  In other cases, you may want to draw a uniform sample once, then repeatedly work through those _same_ records.  (Consider our example of spot-checking new code against a dataset: you'd need to run your code against the same sample in order to confirm your changes work as expected.)

Experienced software developers will invoke a "seeding" function -- such as R's `set.seed()` or Python's `random.seed()` --  to make the randomness a little less so.  To our knowledge, Pig does not have an equivalent function.  You could instead use a "consistent hash digest," which we'll describe in the Statistics chapter (REF).

==== Project Only Chosen Columns by Name

While a `FILTER` selects _rows_ based on an expression, Pig's `FOREACH` selects specific _fields_ chosen by name. footnote:[The fancy word for this simple action is 'projection'. It's useful to have precise terms for choosing columns ('project'), choosing rows ('select'), and specifically choosing rows by expression ('filter')].

The tables we're using come with an overwhelming wealth of stats, but we will be able to do quite sophisticated explorations using only a few of them. 

Major League Baseball teams play a game nearly every single day from the start of April to the end of September (currently, 162 per season). The team on offense sends its players to bat in order, with the goal of having its players reach base and advance the full way around the diamond. Each time a player makes it all the way to home, their team scores a run, and at the end of the game, the team with the most runs wins. We count these events as `G` (games), `PA` (plate appearances on offense) and `R` (runs).

The following Pig code selects our core set of stats for seasons since 1990 where players had more than 60 appearances:

----
TODO: Pig code
----

----
SELECT
    player_id, year_id, team_id,
    G,  PA, AB, R,
    H,  BB, HBP,
    h2B, h3B, HR
  FROM bat_season bat
  WHERE PA > 60 AND year > 1900 ;
----

==== Retrieve a fixed number of Rows (LIMIT)

Another way to create a smaller dataset is to take the first _K_ records of a collection.  (Please note that we emphasize the _first_ _K_ records, not the _top-ranked_ _K_.  We cover that in the next section.)  This is similar to running the `head` command in Unix-like operating systems, or using the `LIMIT` clause in a SQL `SELECT` statement.

Pig offers the `LIMIT` operator for this purpose. To select the first 25 records of our `bat_season` data, you would run:

----
TODO: Pig syntax
----

If your background is in relational databases, that is Pig's way of saying:

----
SELECT bat_season.* FROM bat_season LIMIT 25 ;
----

(TODO: Is there a non-Reduce way to do this?)

In the simplest Map/Reduce equivalent, Mappers emit each record unchanged until they hit the specified limit (or reach the end of their input). Those output records pass through a single Reducer, which itself emits each record unchanged until it has hit the specified limit and does nothing on all subsequent records.

(TODO: Do we want to talk about a non-single Reducer approach?)

In the big data regime, where your data is striped across many machines, there's no intrinsic
notion of a row order.  That means, similar to the `SAMPLE` operator, you have no guarantees of which records `LIMIT` will select.  Changing the number of mappers or reducers, small changes in the data, and so forth can change which records are selected. In the next section, we describe how to better control what records `LIMIT` returns.

NOTE: If you truly don't care which records to select, just point Pig to a single input file.  For example, invoke `LIMIT` on  `some_data/part-00000`, instead of `some_data/` (which will operate on all files in that directory).

A Combiner is helpful here in the predominant case where the specified limit is small, as it will eliminate excess records before they are sent to the Reducer and at each merge/sort pass.

==== Select Rows with the Top-K Values for a Field

On its own, `LIMIT` will return the first records it finds.  What if you want to _rank_ the records -- sort by some criteria -- so you don't just return the first ones, but the _top_ ones?

Use the `ORDER` operator before a `LIMIT` to guarantee this "top _K_" ordering.  This technique also applies a clever optimization (reservoir sampling, see TODO ref) that sharply limits the amount of data sent to the reducers.

Let's say you wanted to select the top 20 seasons by number of hits:

----
TODO: Pig code
----

In SQL, this would be:

----
SELECT H FROM bat_season WHERE PA > 60 AND year_id > 1900 ORDER BY H  DESC LIMIT 10
----

// TODO: not sure what is the second optimization here?
// TODO: remove the term "N" if it is not used elsewhere in this section.


There are two useful optimizations to make when the number of records you will keep (_K_) is much smaller than the number of records in the table (_N_). The first one, which Pig does for you, is to only retain the top K records at each Mapper; this is a great demonstration of where a Combiner is useful:  After each intermediate merge/sort on the Map side and the Reduce side, the Combiner discards all but the top K records.

NOTE: We've cheated on the theme of this chapter (pipeline-only operations) -- sharp eyes will note that `ORDER … LIMIT` will in fact trigger a reduce operation.  We still feel that top-_K_ belongs with the other data elimination pattern, though, so we've included it here.

==== Top K Within a Group

There is a situation where the heap-based top K algorithm is appropriate:  finding the top K elements for a group. Pig's 'top' function accepts a bag and returns a bag with its top K elements.

TODO: needs code example. (Old example used World Cup data; let's find one that fits the baseball dataset)

==== Select Rows using a Limit and Offset

TODO-flip: where do we state that the dataset has 41,040 records (aka, that 2052 is 5% of the dataset?)

A common practice is to express percentiles of your data, that is, to order it and see which records are in the top _K_ percent.  That's another way of saying that those records are better than the remaining 100-_K_ percent, or that they are in the (100-_K_)th percentile.

The first step to calculating percentiles is to determine the number of records in your dataset.  Multiplying that number by 0.01 (that is, 1/100) will show how many records are in one percent of the data.  Multiplying the total by 0.05 (5/100) will show the number of records in five percent of the data, and so on.

For example, our baseball dataset holds 41,040 records.   Five percent of 41,040 is 2,052.  To fetch the top five percent of records -- that is, those records in the 95th percentile -- we would sort the records and extract the top 2,052.

----
TODO: Pig code
----

----
SELECT H FROM bat_season WHERE PA > 60 AND year_id > 1900 ORDER BY H  DESC LIMIT 2052
----

Instead of fetching all of the records in a given percentile, we sometimes just want to know which is the lowest-ranked record of that percentile.  This tells us which record is the boundary between the ranges above and below the percentile marking.  Calculating this requires an additional step, that both Pig and SQL call `OFFSET`.  To find the 95th percentile values for our topline stats -- assuming a post-1900 game, and players with more than 60 plate appearances -- then, we would run:

----
TODO: Pig code
----

----
SELECT H FROM bat_season WHERE PA > 60 AND year_id > 1900 ORDER BY H  DESC LIMIT 1 OFFSET 2052
----

If you repeat those steps for the 75th and 50th percentiles, Pig should return the following:

----
-- %ile	  Row	H	 BB	HBP	h2B	h3B	HR	 G	 PA	OBP	SLG	OPS
-- 95th	 2052	175	75	7	34	9	25	155	669	0.394	0.519	0.895
-- 75th	10260	124	41	3	21	4	9	132	520	0.347	0.422	0.765
-- 50th	20521	66	22	1	11	1	3	93	294	0.313	0.359	0.676
----

WARNING: Be really careful doing this.  As opposed to the `ORDER BY .. LIMIT` pattern, Pig must do a total sort on the full table to calculate percentiles this way.

(TODO call ahead to other "Eliminating Data" things like sparse join and DISTINCT)

==== Selecting Records with a Static Lookup Table

You may want to select rows from a table by whether each record's key matches a one in a separate table. The general case, using a join, is described in the next chapter (REF) under "Selecting Records that Match Records in Another".

If the list is static and of modest size, you might just use a case statement in your code, or match against a set-membership data structure footnote:[By the way, the fastest and cleanest way to load a large-ish table in a dynamic language is often to reformat the table into the language itself. Loading the table is now a one-liner (`require "lookup_table"`), and there's no faster way for the Ruby interpreter to make a data structure than by interpreting Ruby.]. 

In keeping with its "keep the grammar light" philosophy, however, Pig has no equivalent of a case statement -- you must apply the ternary operator (`cond ? t_val : f_val`) repeatedly. The best advice we can give is to use lots of parentheses and whitespace, and to use a UDFs if the case is really severe.

=== Transforming Records

==== Transform Records Individually using `FOREACH`

A `FOREACH` lets you make simple transformations to each record. 

For example, baseball stats don't list the number of singles, only the number of total hits and the number of non-singles (doubles, triples, home runs). This `FOREACH` statement generates the number of singles as its own field:

----
core_stats = FOREACH bat_season GENERATE 
    player_id, year_id, team_id, 
    G, PA, AB, H, 
    H - h2B - h3B - HR AS h1B,
    h2B, h3B, HR
  ;
----

Here's the corresponding SQL command:

----
SELECT
    player_id, year_id, team_id,
    G, PA, AB, H, 
    H - h2B - h3B - HR AS h1B,
    h2B, h3B, HR
  FROM bat_season
  ;
----

A `FOREACH` won't cause a new Hadoop job stage: it's chained onto the end of the preceding operation (and when it's on its own, like this one, there's just a single a mapper-only job). A FOREACH always produces exactly the same count of output records as input records.

Within the GENERATE portion of a normal FOREACH, you can apply arithmetic expressions (as shown); project fields (rearrange, rename and eliminate fields); and apply the FLATTEN operator (see below). 

==== A nested `FOREACH` Allows Intermediate Expressions

The above are all "counting stats", and generally the more games the more hits and runs and so forth. For comparing players, it's better to use "rate stats" normalized against plate appearances. You can do quite a reasonable job of estimating players' performance using these three metrics:

* 'On-base percentage' (`OBP`), which indicates how well the player becomes a potential run. It is given as the fraction of plate appearances that are successful: (`(H + BB + HBP) / PA`) footnote:[Although known as percentages, OBP and SLG are always given as fractions to 3 decimal places]. An `OBP` over 0.420 is very good (better than 95% of significant seasons).
* 'Slugging Percentage' (`SLG`), which indicates how well the player converts potential runs into runs. It is given by the rate of total bases gained in hitting (one for a single, two for a double, etc): (`(h1B + 2*h2B + 3*h3B + 4*HR) / AB`). (Due to historical circumstances, SLG and some other stats use a restricted subset of PA called At Bats (`AB`); don't worry about the difference.)  An `SLG` over 0.520 is very good.
* 'On-base-plus-slugging' (`OPS`), which combines on-base and slugging percentages to give a simple and useful estimate of overall offensive contribution. It is simply the sum of those two metrics: (`OBP + SLG`). Anything above 0.900 is very good.

Doing this with the simple form of `FOREACH` we've been using would be annoying and hard to read -- for one thing, the expressions for OBP and SLG would have to be repeated in the expression for OPS, since the full statement is evaluated together.

// TODO: should we demonstrate the big yucky FOREACH? → nah, we've described it already.  If they really want to see it, they can cook it up themselves. ;-)

There's a fancier form of `FOREACH` (a 'nested' `FOREACH`) that allows intermediate expressions:

----
bat_season = FILTER bat_season BY PA > 0 AND AB > 0;
core_stats = FOREACH bat_season {
    h1B  = H - h2B - h3B - HR;
    HBP = ifNull(HBP) ? 0 : HBP;
    TB  = vh1B + 2*vh2B + 3*vh3B + 4*HR;
    OBP  = (H + BB + HBP) / PA;
    SLG  = TB / AB;
    OPS  = SLG + OBP;
    GENERATE 
      player_id, year_id, team_ids, 
      G, PA, H, HR, SLG, OBP, OPS;
};
----

This alternative `{` curly braces form of `FOREACH` lets you describe its transformations in smaller pieces, rather than smushing everything into the single `GENERATE` clause. New identifiers within the curly braces (such as `player`) only have meaning within those braces, but they do inform the schema.)

In addition to applying arithmetic expressions, projecting and renaming fields, and FLATTENing records, there are a set of Pig operations you can also apply within a nested FOREACH -- you'll see many examples in the next chapter (REF).

Note: We sneakily did two things with the filter above: eliminated rows where `PA` was equal to zero, and also eliminated rows where `PA` was NULL.  For people coming from a SQL background, Pig's handling of NULL values will be fairly familiar. NULL values generally disappear without notice from operations, and generally compare as false -- NULL is not less than, is not greater than, and is not equal to 5.0. For programmers, however, it can be hard to track all this. The rules are well detailed in the Pig manual and somewhat fiddly, so we won't go deep into them here. We've found the best way to learn what you need is to just see lots of examples, which we endeavor to supply in abundance.

==== Place Values into Categorical Bins With a `FOREACH`

// QEM: left off here

TODO-qem: find that word he had in mind for "ranges"

One common task is to prepare a categorical field -- one with a small number of potential values -- from non-categorical fields such as ranges of numbers or strings. As an example, let's find out how common it is to reach various milestones for number of hits -- 50, 100, and so forth. 

The SQL version might look like this:

----
SELECT 100*CEIL(H / 100) AS H_bin, COUNT(*), nameCommon
  FROM bat_career bat
  GROUP BY H_bin;
----

In Pig we'll use a `FOREACH` to determine the bin for each record, and then use a `GROUP BY` as described in the next chapter to prepare its histogram:

----
bat_season = LOAD '...' AS (...);
season_binned = FOREACH bat_season GENERATE 
    50*CEIL(H / 50) AS H_bin;
season_grouped = GROUP season_binned BY H_bin;
FOREACH season_grouped GENERATE group AS H_bin, COUNTSTAR(season_binned) AS ct;
----

==== Generating Data

Generating data in a distributed system requires distributing an assignment of what to generate onto each node, which can be somewhat annoying. The surprisingly useful integers table -- 1, 2, 3, ... each on subsequent rows -- provides one way to get around this. We don't really have a good baseball-based example, but we can demonstrate generating the 11 million combinations of five letters using a map-reduce job (or the similar UDF):

----
C2 = 26**2; C3 = 26**3; C4 = 26**4; C5 = 26**5
ORD_A = 'a'.ord

mapper do |line|
  idx = line.to_i
  offsets = [ line / C5, (line / C4) % 26, (line / C3) % 26, (line / C2) % 26, line % 26 ]
  chars = offsets.map{|offset| (ORD_A + offset).chr }
  yield chars.join
end
----

Another example of this pattern is the poor-mans data loader given in Chapter 3 (REF) -- prepare a mapper input that is a list of filenames or database queries, and have each mapper expand its trivial input into many rows of output.

==== Generating Pairs

is there a way to do the SQL version more elegantly?

SELECT
    IF(home_team_id <= away_team_id, home_team_id, away_team_id) AS team_a,
    IF(home_team_id <= away_team_id, away_team_id, home_team_id) AS team_b,
    COUNT(*)
  FROM events ev
GROUP BY home_team_id, away_team_id
ORDER BY home_team_id, away_team_id
;

(do we want to show the group by or call forward to it)

You'll see a more elaborate version of this

COALESCE requires datafu:
define COALESCE datafu.pig.util.Coalesce();
or use ternary: eg (isEmpty(A) ? 0 : First(A))

==== Concatenate Multiple Strings by Applying a UDF to Records

concatenating bag

DEFINE MULTICONCAT 'pygmalion/udf/RangeBasedStringConcat.java';

https://github.com/jeromatron/pygmalion/blob/master/udf/src/main/java/org/pygmalion/udf/RangeBasedStringConcat.java

TODO describe using, call ahead to acquiring, Piggybank, Datafu, Pigsy, Pygmalion, and Sounder

TODO submodule in Datafu, Pigsy, Pygmalion, and Sounder 

=== Expanding Data

If you count all the letters in a large-enough body of text, you'll generally find that the letter "e" (the most frequent) appears about 12% of the time, while z and q (the least frequent) appear less than 1% of the time. But names of people have a noticeably different distribution of characters, as we can demonstrate using the baseball data. The `people` table has two fields representing city names, a first name field and a last name field. We'll find the frequency distribution for each.

==== Flatten on a Bag Generates Many Records from a Field with Many Elements

===== Demonstration in Pig

This snippet first produces a bag pairing each of the `chararray` values we want with the distribution it belongs to, then flattens it.

----
typed_strings = FOREACH people {
  fields_bag = {('fn', nameFirst), ('ln', nameLast), ('ct', birthCity), ('ct', deathCity)};
  GENERATE FLATTEN(fields_bag) AS (type:chararray, str:chararray);
  };
----

Each single record having a bag turns into four records having a field called 'type' and a field called 'str':

----
fn    Hank
ln    Aaron
ct   San Diego
ct   Inverness
----

==== Flatten on a Tuple Folds it into its Parent

Our next step is to split those string fields into characters. Pig provides a `STRSPLIT` function that _seems_ to do what we want (spoiler alert: for this purpose it doesn't, but we want to prove a point).

----
typed_chars = FOREACH typed_strings {
  chars_bag = STRSPLIT(str, '(?!^)');  -- works, but not as we want
  GENERATE type, FLATTEN(chars_bag) AS token;
  };
----

The output we want would have one record per character in the `str` field, but that isn't what happens:

----
fn   H   a   n   k
ln   A   a   r    o   n
...
----

`STRSPLIT` returns a _tuple_, not a _bag_, and the `FLATTEN` operation applied to a tuple does not produce many records from the tuple field, it lifts the elements of the tuple into its container. This `FLATTEN(STRSPLIT(...))` combination is great for, say, breaking up a comma-delimited string into field, but we want to flatten the characters into multiple records. The pigsy package has the UDF we need:

----
register    '...path/to/pigsy/target/pigsy-2.1.0-SNAPSHOT.jar';
DEFINE STRSPLITBAG         pigsy.text.STRSPLITBAG();
-- ...
typed_chars = FOREACH typed_strings {
  chars_bag = STRSPLITBAG(LOWER(str), '(?!^)');
  GENERATE type, FLATTEN(chars_bag) AS token;
  };
----

===== Results

What remains is to group on the characters for each type to find their overall counts, and then to prepare the final results. We'll jump into all that in the next chapter, but (REF) shows the final results. The letters "k", "j", "b" and "y" are very over-represented in first names. The letter "z" is very over-represented in last names, possibly because of the number of Hispanic and Latin American players.

----
char	% dictionary  	% prose		% first names	% excess
a	  8.49		  8.16		 8.31		 1.01
b	  2.07		  1.49		 3.61		 2.00
c	  4.53		  2.78		 3.67		  .80
d	  3.38		  4.25		 4.42		 1.48
e	 11.16		 12.70		11.03		 1.05
f	  1.81		  2.22		 1.43		 1.27
g	  2.47		  2.01		 2.03		  .96
h	  3.00		  6.09		 3.40		 1.23
i	  7.54		  6.96		 6.85		  .78
j	   .19		  0.15		 3.70		 3.14
k	  1.10		  0.77		 3.07		 4.37
l	  5.48		  4.02		 6.29		 1.07
m	  3.01		  2.40		 3.73		 1.21
n	  6.65		  6.74		 6.46		  .92
o	  7.16		  7.50		 6.81		  .89
p	  3.16		  1.92		 1.08		  .31
q	   .19		  0.09		  . 3		  .19
r	  7.58		  5.98		 8.33		 1.15
s	  5.73		  6.32		 3.06		  .49
t	  6.95		  9.05		 4.00		  .58
u	  3.63		  2.75		 1.91		  .49
v	  1.00		  0.97		 1.15		 1.25
w	  1.28		  2.36		  .82		 1.29
x	   .29		  0.15		  .22		  .73
y	  1.77		  1.97		 3.93		 1.68
z	   .27		  0.07		  .19		  .53
----

(TODO insert actual results, and decide which distribution (prose or dictionary) you'll normalize against)

==== Other Similar Patterns

The chapter on text data (REF) shows how to tokenize free text into a "word bag", using both Pig's simplistic `TOKENIZE` function and a UDF that applies a sophisticated computational linguistics library. In the Event Stream chapter (REF), we'll demonstrate dividing time range into discrete intervals. Lastly, the Statistics chapter (REF) describes a script to take summary statistics of all columns simultaneously, which involves transposing a record into attribute-value pairs.

=== Splitting a Table

==== Splitting into Multiple Data Flows using `SPLIT`

The careers table gives the number of times each player was elected to the All-Star game (indicating extraordinary performance during a season) and whether they were elected to the Hall of Fame (indicating a truly exceptional career).

===== Demonstration in Pig

Separating those records into different data flows isn't straightforward in map/reduce, but it's very natural using Pig's `SPLIT` operation.

----
SPLIT bat_career
  INTO hof     IF hofYear > 0, -- the '> 0' eliminates both NULLs and 0s
  INTO allstar IF G_allstar > 0
  INTO neither IF NOT(hofYear > 0 OR G_allstar > 0)
  ;
STORE hof     INTO '/data/out/baseball/hof_careers';
STORE allstar INTO '/data/out/baseball/allstar_careers';
STORE neither INTO '/data/out/baseball/neither_careers';
----

The `SPLIT` operator does not short-circuit: every record is tested against every condition, and so a player who is both a hall-of-famer and an allstar will be written into both files. There is also no "else" clause to trap records that did not match a condition; you'll have to negate everything as we have here.

==== Splitting into files by key by using a Pig Storefunc UDF

If instead you're looking to write directly into files named for a key, use the multistorage storefunc from the Piggybank UDF collection. For example, we can partition player seasons by primary team:

----
    bat_season = LOAD 'bat_season' AS (...);
    STORE bat_season INTO '/data/out/baseball/seasons_by_team' USING MultiStorage('/data/out/baseball/seasons_by_team', '10'); -- team_id, field 10
    STORE ... multistorage;
----

The output has a directory for each key, and within directory that the same `part-NNNNN` files of any map-reduce job. 

This means the count of output files is the number of keys times the number of output slots, which can lead to severe many small files problem. As mentioned in Chapter 3 (REF), many small files is Not Good. If you precede the STORE operation by a `GROUP BY` on the key, the reducer guarantee provides that each subdirectory will only have one output file. 

==== Splitting a Table into Uniform Chunks

We won't go into much detail, but one final set of patterns is to split a table into uniform chunks. If you don't need the chunks to be exactly sized, you can apply a final `ORDER BY` operation on a uniformly-distributed key -- see the section on "Shuffling the Records in a Table" in the next chapter (REF). 

To split into chunks with an exact number of lines, first use `RANK` to number each line, then prepare a chunk key using the line number modulo the chunk size, and store into chunks using MultiStorage. Since the rank operation's reducers number their records sequentially, only a few reducers are involved with each chunk, and so you won't hit the small files problem. Splitting a table into blocks of fixed _size_ is naturally provided by the HDFS block size parameter, but we're not aware of a good way to do so explicitly. 

=== Treat the Union of Several Tables as a Single Table

The counterpart to splitting a table into pieces is to treat many pieces as a single table. This really only makes sense when all those pieces have the same schema, so that's the only case we'll handle here.

==== Load Multiple Files as One Table

The easiest way to unify several tables is to simply load them as one. Hadoop will expand a comma-separated list of paths into multiple paths, and perform simple 'glob-style' filename expansion. This snippet will load all the teams whose team_id starts with a "B" or ends with an "N":

===== Demonstration in Pig

----
b_and_n_teams = LOAD '/data/out/baseball/seasons_by_team/B*,/data/out/baseball/seasons_by_team/*N' AS (...);
----

===== Demonstration in map/reduce

----
(show commandline for multiple files)
----

==== Treat Several Pig Relation Tables as a Single Table

In Pig, you can rejoin several pipelines using the `UNION` operation. The tables we've been using so far cover only batting stats; there are another set of tables covering stats for pitchers, and in rare cases a player may only appear in one or the other. To find the name and id of all players that appear in either table, we can project the fields we want (earning a uniform schema) and then unify the two streams:

----
bat_career = LOAD '/data/rawd/baseball/sports/bat_career AS (...);
pit_career = LOAD '/data/rawd/baseball/sports/pit_career AS (...);
bat_names = FOREACH bat_career GENERATE player_id, nameFirst, nameLast;
pit_names = FOREACH pit_career GENERATE player_id, nameFirst, nameLast;
names_in_both = UNION bat_names, pit_names;
player_names = DISTINCT names_in_both;
----

The `UNION` operation does not remove duplicate rows as a set-wise union would. It simply tacks one table onto the end of the other, and so the last line eliminates those duplicates -- more on `DISTINCT` in the next chapter (REF). The `UNION` operation also does not provide any guarantees on ordering of rows. Some SQL users may fall into the trap of doing a UNION-then-GROUP to combine multiple tables. This is terrible in several ways, and you should instead use the COGROUP operation -- see the Won-Loss Record example in the next chapter (REF).

==== Clean Up Many Small Files by Merging into Fewer Files

The Many Small Files problem is so pernicious because Hadoop natively assigns each mapper to only one file, and so a normal mapper-only job can only _increase_ the number of files. We know of two ways to reorganize the records of a table into fewer files. 

One is to perform a final `ORDER BY` operation footnote:[The tuning chapter (REF) tells you why you might want to increase the HDFS block size for truly huge dataset, and why you might not want to do so]. Since this gives the side benefit of allowing certain optimized join operations, we like to do this for "gold" datasets that will be used by many future jobs. 

Sorting is a fairly expensive operation, though; luckily, Pig can do this reasonably well with a mapper-only job by setting the `pig.splitCombination` configuration to true and setting `pig.maxCombinedSplitSize` to the size of the input divided by the number of files you'd like to produce.

----
set pig.splitCombination true;
set pig.maxCombinedSplitSize 2100100100;
----

The `maxCombinedSplitSize` should be much larger than the HDFS block size so that blocks are fully used. Also note the old sailor's trick in the last line -- since there's no essential difference between 2 billion bytes, 2 gigabytes, or a number nearby, the value `2100100100` is much easier to read accurately than `2000000000` or `2147483648`.

The operations in this chapter (except where noted) do not require a reduce on their own, which makes them very efficient. The really interesting applications, however, come when we put data into context, which is the subject of the next chapter.


.A Quick Look into Baseball
****
Nate Silver calls Baseball the "perfect data set".  There are not many human-centered systems for which this comprehensive degree of detail is available, and no richer set of tables for truly demonstrating the full range of analytic patterns.

For readers who are not avid baseball fans, we provide a simple -- some might say "oversimplified" -- description of the sport and its key statistics.  Please refer to Joseph Adler's _Baseball Hacks_ (O'Reilly) or [TODO the one with Baseball and R] for more details.


*Acronyms and terminology*

We use the following acronyms (and, coincidentally, field names) in our baseball dataset:

* `G`, 'Games'
* `PA`: 'Plate Appearances', the number of completed chances to contribute offensively
* `H`: 'Hits', either singles (`h1B`), doubles (`h2B`), triples (`h3B`) or home runs (`HR`)
* `BB`: 'Walks', pitcher presented too many unsuitable pitches
* `HBP`: 'Hit by Pitch', like a walk but more painful
* `OBP`: 'On-base Percentage', indicates effectiveness at becoming a potential run
* `SLG`: 'Slugging Percentage', indicates effectiveness at converting potential runs into runs
* `OPS`: 'On-base-plus-Slugging', a reasonable estimate of overall offensive contribution

For those who consider sporting events to be the dull province of jocks, holding no interest at all: when we say the "On-Base Percentage" is a simple matter of finding `(H + BB + HBP) / AB`, just trust us that (a) it's a useful statistic; (b) that's how you find its value; and then (c) pretend it's the kind of numbers-in-a-table example abstracted from the real world that many books use.


*The rules and goals*

Major League Baseball teams play a game nearly every single day from the start of April to the end of September (currently, 162 per season). The team on offense sends its players to bat in order, with the goal of having its players reach base and advance the full way around the diamond. Each time a player makes it all the way to home, their team scores a run, and at the end of the game, the team with the most runs wins. We count these events as `G` (games), `PA` (plate appearances on offense) and `R` (runs).

The best way to reach base is by hitting the ball back to the fielders and reaching base safely before they can retrieve the ball and chase you down -- a hit (`H`) . You can also reach base on a 'walk' (`BB`) if the pitcher presents too many unsuitable pitches, or from a 'hit by pitch' (`HBP`) which is like a walk but more painful. You advance on the basepaths when your teammates hit the ball or reach base; the reason a hit is valuable is that you can advance as many bases as you can run in time. Most hits are singles (h1B), stopping safely at first base. Even better are doubles (`h2B`: two bases), triples (`h3B`: three bases, which are rare and require very fast running), or home runs (`HR`: reaching all the way home, usually by clobbering the ball out of the park).

So your goal as a batter is both becomes a potential run and helps to convert players on base into runs. If the batter does not reach base it counts as an out, and after three outs, all the players on base lose their chance to score and the other team comes to bat. (This threshold dynamic is what makes a baseball game exciting: a single pitch can swing the score by or squander the offensive efforts


*Additional stats*

The above are all "counting stats", and generally the more games the more hits and runs and so forth. For comparing players, it's better to use "rate stats" normalized against plate appearances.

For historical reasons, some stats use a restricted subset of PA called AB (At Bats). You should generally prefer PA to AB.

'On-base percentage' (`OBP`) indicates how well the player becomes a potential run, given as the fraction of plate appearances that are successful: (`(H + BB + HBP) / PA`) footnote:[Although known as percentages, OBP and SLG are always given as fractions to 3 decimal places]. An `OBP` over 0.420 is very good (better than 95% of significant seasons).

'Slugging Percentage' (`SLG`) indicates how well the player converts potential runs into runs. It is given by the total bases gained in hitting (one for a single, two for a double, etc) divided by the number of at bats: (`(H + h2B + 2*h3B + 3*HR) / AB`). An `SLG` over 0.520 is very good.

'On-base-plus-slugging' (`OPS`) combines on-base and slugging percentages to give a simple and useful estimate of overall offensive contribution: (`OBP + SLG`). Anything above 0.900 is very good.
****


.Pig Gotchas
****

"dot or colon?"

Some late night under deadline, Pig will supply you with the absolutely baffling error message "scalar has more than one row in the output". You've gotten confused and used the tuple element operation (`players.year`) when you should have used the disambiguation operator (`players::year`). The dot is used to reference a tuple element, a common task following a `GROUP`. The double-colon is used to clarify which specific field is intended, common following a join of tables sharing a field name.


Where to look to see that Pig is telling you have either nulls, bad fields, numbers larger than your type will hold or a misaligned schema.


TODO: fill this in with more gotchas

****

. A Foolish Optimization
****
TODO: Make this be more generally "don't use the O(N) algorithm that works locally" -- fisher-yates and top-k-via-heap being two examples
TODO: consider pushing this up, earlier in the chapter, if we find a good spot for it

We will tell you about another "optimization," mostly because we want to illustrate how a naive performance estimation based on theory can lead you astray in practice. In principle, sorting a large table in place takes 'O(N log N)' time. In a single compute node context, you can actually find the top K elements in 'O(N log K)' time -- a big savings since K is much smaller than N. What you do is maintain a heap structure; for every element past the Kth, if it is larger than the smallest element in the heap, remove the smallest member of the heap and add the element to the heap. While it is true that 'O(N log K)' beats 'O(N log N)', this reasoning is flawed in two ways. First, you are not working in a single-node context; Hadoop is going to perform that sort anyway. Second, the fixed costs of I/O almost always dominate the cost of compute (FOOTNOTE:  Unless you are unjustifiably fiddling with a heap in your Mapper.)

The 'O(log N)' portion of Hadoop's log sort shows up in two ways:  The N memory sort that precedes a spill is 'O(N log N)' in compute time but less expensive than the cost of spilling the data. The true 'O(N log N)' cost comes in the reducer: 'O(log N)' merge passes, each of cost 'O(N)'. footnote:[If initial spills have M records, each merge pass combines B spills into one file, and we can skip the last merge pass, the total time is `N (log_B(N/M)-1).` [TODO: double check this]. But K is small, so there should not be multiple merge passes; the actual runtime is 'O(N)' in disk bandwidth. Avoid subtle before-the-facts reasoning about performance; run your job, count the number of merge passes, weigh your salary against the costs of the computers you are running on, and only then decide if it is worth optimizing.

****
