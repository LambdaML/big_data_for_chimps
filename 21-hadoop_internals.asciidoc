[[hadoop_internals]]
== Hadoop Internals

For 16 chapters now, we’ve been using Hadoop and Storm+Trident from the outside. The biggest key to writing efficient dataflows is to understand the interface and fundamental patterns of use, not the particulars of how these framework executes the dataflow.  However, even the strongest abstractions pushed far enough can leak, and so at some point, it’s important to understand these internal details.  These next few chapters concentrate on equipping you to understand your jobs’ performance and practical tips for improving it; if you’re looking for more, by far, the best coverage of this material is found in (TODO: Add links Tom White’s Hadoop:  The Definitive Guide and Eric Sammer’s Hadoop Operations).

Let’s first focus on the internals of Hadoop.

=== HDFS (NameNode and DataNode)

It’s time to learn how the HDFS stores your data; how the Map/Reduce framework launches and coordinates job attempts; and what happens within the framework as your Map/Reduce process executes.

The HDFS provides three remarkable guarantees:  durability (your data is never lost or corrupted), availability (you can always access it) and efficiency (you can consume the data at high rate especially from Map/Reduce jobs and other clients).  The center of action, as you might guess, is the NameNode.  The NameNode is a permanently running daemon process that tracks the location of every block on the network of DataNodes, along with its name and other essential metadata.  (If you’re familiar with the File Allocation Table (FAT) of a traditional file system, it’s like a more active version of that.  FOOTNOTE:  [If you’re not familiar with what an FAT is, then it’s like the system you’re reading about but for a file system.])

(NOTE:  check something … appending)

When a client wishes to create a file, it contacts the NameNode with the desired path and high-level metadata.  The NameNode records that information in an internal table and identifies the DataNodes that will hold the data.  The NameNode then replies with that set of DataNodes, identifying one of them as the initial point of contact.  (When we say "client", that’s anything accessing the NameNode, whether it’s a Map/Reduce job, one of the Hadoop filesystem commands or any other program.)  The file is now exclusively available to the client for writing but will remain invisible to anybody else until the write has concluded (TODO: Is it when the first block completes or when the initial write completes?).

Within the client’s request, it may independently prescribe a replication factor, file permissions, and block size _____________ (TODO: fill in)

The client now connects to the indicated DataNode and begins sending data.  At the point you’ve written a full block’s worth of data, the DataNode transparently finalizes that block and begins another (TODO: check that it’s the DataNode that does this).  As it finishes each block or at the end of the file, it independently prepares a checksum of that block, radioing it back to the NameNode and begins replicating its contents to the other DataNodes.  (TODO: Is there an essential endoffile ritual?)  This is all transparent to the client, who is able to send data as fast as it can cram it through the network pipe.

Once you’ve created a file, its blocks are immutable -- as opposed to a traditional file system, there is no mechanism for modifying its internal contents.  This is not a limitation; it’s a feature.  Making the file system immutable not only radically simplifies its implementation, it makes the system more predictable operationally and simplifies client access.  For example, you can have multiple jobs and clients access the same file knowing that a client in California hasn’t modified a block being read in Tokyo (or even worse, simultaneously modified by someone in Berlin).  (TODO: When does append become a thing?)

The end of the file means the end of its data but not the end of the story.  At all times, the DataNode periodically reads a subset of its blocks to find their checksums and sends a "heartbeat" back to the DataNode with the (hopefully) happy news.  ____________ (TODO: fill in).  There are several reasons a NameNode will begin replicating a block.  If a DataNode’s heartbeat reports an incorrect block checksum, the NameNode will remove that DataNode from the list of replica holders for that block, triggering its replication from one of the remaining DataNodes from that block.  If the NameNode has not received a heartbeat from a given DataNode within the configured timeout, it will begin replicating all of that DataNode’s blocks; if that DataNode comes back online, the NameNode calmly welcomes it back into the cluster, cancelling replication of the valid blocks that DataNode holds.  Furthermore, if the amount of data on the most populated and least populated DataNodes becomes larger than a certain threshold or the replication factor for a file is increased, it will rebalance; you can optionally trigger one earlier using the `hadoop balancer` command.

However it’s triggered, there is no real magic; one of the valid replica-holder DataNodes sends the block contents to the new replica holder, which heartbeats back the block once received.  (TODO: Check details)

As you can see, the NameNode and its metadata are at the heart of every HDFS story.  This is why the new HighAvailability (HA) NameNode feature in recent versions is so important and should be used in any production installation.  It’s even more important, as well, to protect and backup the NameNode’s metadata, which, unfortunately, many people don’t know to do.  (TODO: Insert notes on NameNode metadata hygiene previously written).

The NameNode selects the recipient DataNodes with some intelligence.  Information travels faster among machines on the same switch, switches within the same data center and so forth.  However, if a switch fails, all its machines are unavailable and if a data center fails, all its switches are unavailable and so forth.  When you configure Hadoop, there is a setting that allows you to tell it about your network hierarchy.  If you do so, the NameNode will ensure the first replica lies within the most distant part of the hierarchy -- durability is important above all else.  All further replicas will be stored within the same rack -- providing the most efficient replication.  (TODO: Mention the phrase "rack aware.")   As permitted within that scheme, it also tries to ensure that the cluster is balanced -- preferring DataNodes with less data under management.  (TODO: Check that it’s amount of data not percent free)

(TODO: Does it make contact on every block or at the start of a file?)

The most important feature of the HDFS is that it is highly resilient against failure of its underlying components.  Any system achieves resiliency through four mechanisms:  Act to prevent failures; insulate against failure through redundancy; isolate failure within independent fault zones; and lastly, detect and remediate failures rapidly.  (FOOTNOTE: This list is due to James Hamilton, TODO: link whose blocks and papers are essential reading).  The HDFS is largely insulated from failure by using file system-based access (it does not go behind the back of the operating system), by being open source (ensuring code is reviewed by thousands of eyes and run at extremely high scale by thousands of users), and so forth.  Failure above the hardware level is virtually unheard of.  The redundancy is provided by replicating the data across multiple DataNodes and, in recent versions, using the Zookeeper-backed HighAvailability NameNode implementation.

The rack awareness, described above, isolates failure using your defined network hierarchy, and at the semantic level, independently for each HDFS block.  Lastly, the heartbeat and checksum mechanisms along with its active replication and monitoring hooks allow it and its Operators to detect intermediate faults.

==== S3 File System

The Amazon EC2 Cloud provides an important alternative to the HDFS, its S3 object store.  S3 transparently provides multi-region replication far exceeding even HDFS’ at exceptionally low cost (at time of writing, about $80 per terabyte per month, and decreasing at petabyte and higher scale).  What’s more, its archival datastore solution, Glacier, will hold rarely-accessed data at one-tenth that price and even higher durability.  (FOOTNOTE: The quoted durability figure puts the engineering risk below, say, the risk of violent overthrow of our government).  For machines in the Amazon Cloud with a provisioned connection, the throughput to and from S3 is quite acceptable for Map/Reduce use.

Hadoop has a built-in facade for the S3 file system, and you can do more or less all the things you do with an HDFS:  list, put and get files; run Map/Reduce jobs to and from any combination of HDFS and S3; read and create files transparently using Hadoop’s standard file system API.  There are actually two facades.  The `s3hdfs`  facade (confusingly labeled as plain `s3` by Hadoop but we will refer to it here as `s3hdfs`) stores blocks in individual files using the same checksum format as on a DataNode and stores the Name Node-like metadata separately in a reserved area.  The `s3n` facade, instead, stores a file as it appears to the Hadoop client, entirely in an `s3` object with a corresponding path.  When you visit S3 using Amazon’s console or any other standard S3 client, you’ll see a file called `/my/data.txt` as an object called `datadoc.txt` in `MyContainer` and its contents are immediately available to any such client; that file, written `s3hdfs` will appear in objects named for 64-bit identifiers like `0DA37f...` and with uninterpretable contents.  However, `s3n` cannot store an individual file larger than 5 terabytes.  The `s3hdfs` blocks minorly improve Map/Reduce efficiency and can store files of arbitrary size.  All in all, we prefer the `s3n` facade; the efficiency improvement for the robots does not make up for the impact on convenience on the humans using the system and that it’s a best-practice to not make individual files larger than 1 terabyte any way.

The universal availability of client libraries makes S3 a great hand-off point to other systems or other people looking to use the data.  We typically use a combination of S3, HDFS and Glacier in practice.  Gold data -- anything one project produces that another might use -- is stored on S3.  In production runs, jobs read their initial data from S3 and write their final data to S3 but use an HDFS local to all its compute nodes for any intermediate checkpoints.

When developing a job, we run an initial `distcp` from S3 onto the HDFS and do all further development using the cluster-local HDFS.  The cluster-local HDFS provides better (but not earth-shakingly better) Map/Reduce performance.  It is, however, noticeably faster in interactive use (file system commands, launching jobs, etc).  Applying the "robots are cheap, humans are important" rule easily justifies the maintenance of the cluster-local HDFS.

If you use a cluster-local HDFS in the way described, that is, it holds no gold data, only development and checkpoint artifacts, _______________ (TODO: fill in).  Provision your HDFS to use EBS volumes, not the local (ephemeral) ones.  EBS volumes surprisingly offer the same or better throughput as local ones and allow you to snapshot a volume in use, or even kill all the compute instances attached to those volumes then reattach them to a later incarnation of the cluster.  (FOOTNOTE: This does require careful coordination.  Our open-source Iron-Fan framework has all the code required to do so.)  Since the EBS volumes have significant internal redundancy, it then becomes safe to run a replication factor of 2 or even 1.  For many jobs, the portion of the commit stage waiting for all DataNodes to acknowledge replication can become a sizable portion of the time it takes a Map/Reduce stage to complete.  Do this only if you’re an amateur with low stakes or a professional whose colleagues embrace these tradeoffs; nobody ever got fired for using a replication factor of 3.

As your S3 usage grows --- certainly if you find you have more than, say, a dozen terabytes of data not in monthly use -- it’s worth marking that data for storage in Glacier, not S3 (you can only do this, of course, if you’re using the `s3n` facade).  There’s a charge for migrating data and, of course, your time is valuable, but the savings can be enormous.

