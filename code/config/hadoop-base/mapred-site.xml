<configuration>
  <!-- <property><name>mapreduce.framework.name</name>                           <value>yarn</value>                             <description>The runtime framework for executing MapReduce jobs. Can be one of local, classic or yarn. </description>   </property> -->

  <property><name>mapreduce.cluster.local.dir</name>                        <value>/data/hadoop/local/mapred</value>     <description>Location(s) for intermediate data files. Must be local to the machine. Comma-separated.</description> </property>
  <property><name>mapreduce.jobtracker.persist.jobstatus.dir</name>         <value>/data/hadoop/mapred/jobstatus</value>    <description>Location to store job status information</description> </property>
  <!-- <property><name>yarn.app.mapreduce.am.staging-dir</name>             <value>/data/hadoop/yarn/staging</value>        <description></description> </property> -->

  <!-- <property><name></name>                                              <value></value>    <description></description> </property> -->


  <!-- ===================================================================== ~ -->
  <!--                                                                       ~ -->
  <!-- task properties                                                       ~ -->

  <!-- <property><name>mapreduce.job.maps</name>                                 <value>2</value>                                <description>The default number of map tasks per job. Ignored when mapreduce.framework.name is "local". </description>  </property> -->
  <!-- <property><name>mapreduce.job.reduces</name>                              <value>2</value>                                <description>The default number of reduce tasks per job. Typically set to 99% of the cluster's reduce capacity, so that if a node fails the reduces can  still be executed in a single wave. Ignored when mapreduce.framework.name is "local". </description>   </property> -->
  <!-- <property><name>mapreduce.job.reduce.slowstart.completedmaps</name>       <value>0.1</value>                              <description>Fraction of the number of maps in the job which should be  complete before reduces are scheduled for the job.  </description>      </property> -->
  <!-- <property><name>mapreduce.input.fileinputformat.split.minsize</name> <value>0</value>                                <description>The minimum size chunk that map input should be split into.  Note that some file formats may have minimum split sizes that take priority over this setting.</description></property> -->
  <!-- <property><name>mapreduce.job.ubertask.enable</name>                 <value>true</value>                             <description>Whether to enable the small-jobs "ubertask" optimization, which runs "sufficiently small" jobs sequentially within a single JVM. "Small" is defined by the following maxmaps, maxreduces, and maxbytes settings.  Users may override this value. </description>    </property>  -->
  <!-- <property><name>mapreduce.job.ubertask.maxmaps</name>                <value>4</value>                                <description>Threshold for number of maps, beyond which job is considered too big for the ubertasking optimization.  Users may override this value, but only downward. </description>   </property>  -->
  <!-- <property><name>mapreduce.job.ubertask.maxreduces</name>             <value>1</value>                                <description>Threshold for number of reduces, beyond which job is considered too big for the ubertasking optimization.  CURRENTLY THE CODE CANNOT SUPPORT MORE THAN ONE REDUCE and will ignore larger values.  (Zero is a valid max, however.)  Users may override this value, but only downward. </description>        </property>  -->
  <!-- <property> <name>mapred.job.reuse.jvm.num.tasks</name>               <value>10</value></property> -->

  <!-- ===================================================================== ~ -->
  <!--                                                                       ~ -->
  <!-- memory and properties                                                 ~ -->

  <!-- <property><name>mapreduce.map.memory.mb</name>                       <value>1500</value> </property> -->
  <!-- <property><name>mapreduce.reduce.memory.mb</name>                    <value>2800</value> </property> -->
  <!-- <property><name>mapreduce.map.java.opts</name>                       <value>-Xmx1400m -Xss256k -XX:+UseCompressedOops -server -XX:+AggressiveOpts -XX:+UseConcMarkSweepGC -XX:+UseParNewGC -XX:+CMSParallelRemarkEnabled -XX:CMSInitiatingOccupancyFraction=75 -XX:+UseCMSInitiatingOccupancyOnly -XX:NewSize=400m -XX:MaxNewSize=400m -XX:SurvivorRatio=10 -XX:MaxTenuringThreshold=2 -verbose:gc -XX:+PrintGCDetails -XX:+PrintHeapAtGC -XX:+PrintGCTimeStamps -XX:+PrintClassHistogram -XX:+PrintTenuringDistribution -XX:+PrintGCApplicationStoppedTime -Xloggc:/tmp/hadoop-flip/gc-@taskid@.log -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false -Djava.awt.headless=true</value></property> -->
  <!-- <property><name>mapreduce.reduce.java.opts</name>                    <value>-Xmx2200m -Xss256k -XX:+UseCompressedOops -server -XX:+AggressiveOpts -XX:+UseConcMarkSweepGC -XX:+UseParNewGC -XX:+CMSParallelRemarkEnabled -XX:CMSInitiatingOccupancyFraction=75 -XX:+UseCMSInitiatingOccupancyOnly -XX:NewSize=400m -XX:MaxNewSize=500m -XX:SurvivorRatio=10 -XX:MaxTenuringThreshold=2 -verbose:gc -XX:+PrintGCDetails -XX:+PrintHeapAtGC -XX:+PrintGCTimeStamps -XX:+PrintClassHistogram -XX:+PrintTenuringDistribution -XX:+PrintGCApplicationStoppedTime -Xloggc:/tmp/hadoop-flip/gc-@taskid@.log -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false -Djava.awt.headless=true</value></property> -->
  <!-- <property><name>mapreduce.task.io.sort.factor</name>                 <value>30</value>                               <description>The number of streams to merge at once while sorting files.  This determines the number of open file handles.</description>        </property> -->
  <!-- <property><name>mapreduce.task.io.sort.mb</name>                     <value>40</value>                               <description>The total amount of buffer memory to use while sorting  files, in megabytes.  By default, gives each merge stream 1MB, which should minimize seeks.</description>  </property> -->
  <!-- <property><name>mapreduce.task.io.sort.spill.percent</name>          <value>0.9</value>                              <description>The total amount of buffer memory to use while sorting  files, in megabytes.  By default, gives each merge stream 1MB, which should minimize seeks.</description>  </property> -->

  <!-- <property><name>mapreduce.reduce.shuffle.parallelcopies</name>       <value>10</value>                               <description>The default number of parallel transfers run by reduce during the copy(shuffle) phase. </description>      </property> -->
  <!-- <property><name>mapreduce.reduce.merge.inmem.threshold</name>        <value>0</value>                                <description>The threshold, in terms of the number of files  for the in-memory merge process. When we accumulate threshold number of files we initiate the in-memory merge and spill to disk. A value of 0 or less than 0 indicates we want to DON'T have any threshold and instead depend only on the ramfs's memory consumption to trigger the merge. </description>  </property> -->
  <!-- <property><name>mapreduce.reduce.shuffle.merge.percent</name>        <value>0.66</value>                             <description>The usage threshold at which an in-memory merge will be initiated, expressed as a percentage of the total memory allocated to storing in-memory map outputs, as defined by mapreduce.reduce.shuffle.input.buffer.percent. </description>   </property>  -->
  <!-- <property><name>mapreduce.reduce.shuffle.input.buffer.percent</name> <value>0.70</value>                             <description>The percentage of memory to be allocated from the maximum heap size to storing map outputs during the shuffle. </description>      </property>  -->
  <!-- <property><name>mapreduce.reduce.input.buffer.percent</name>         <value>0.35</value>                             <description>The percentage of memory- relative to the maximum heap size- to retain map outputs during the reduce. When the shuffle is concluded, any remaining map outputs in memory must consume less than this threshold before the reduce can begin. </description> </property> -->
  <!-- <property><name>mapreduce.reduce.markreset.buffer.percent</name>     <value>0.0</value>                              <description>The percentage of memory -relative to the maximum heap size- to be used for caching values when using the mark-reset functionality. </description> </property>  -->

  <!-- <property><name>mapreduce.map.output.compress</name>                      <value>true</value>                             <description>Should the outputs of the maps be compressed before being sent across the network. Uses SequenceFile compression. </description>   </property> -->
  <!-- <property><name>mapreduce.map.output.compress.codec</name>           <value>org.apache.hadoop.io.compress.SnappyCodec</value> </property> -->

  <!-- <property><name>mapreduce.reduce.merge.memtomem.enabled</name>       <value>true</value>                             <description></description>   </property> -->


  <!-- ===================================================================== ~ -->
  <!--                                                                       ~ -->
  <!-- Logging and Profiling                                                 ~ -->

  <!-- <property><name>mapreduce.task.profile</name>                        <value>true</value>                             <description>To set whether the system should collect profiler information for some of the tasks in this job? The information is stored in the user log directory. The value is "true" if task profiling is enabled.</description>      </property>  -->
  <!-- <property><name>mapreduce.task.profile.maps</name>                   <value>0-1</value>                              <description> To set the ranges of map tasks to profile. mapreduce.task.profile has to be set to true for the value to be accounted. </description>     </property>  -->
  <!-- <property><name>mapreduce.task.profile.reduces</name>                <value>0-1</value>                              <description> To set the ranges of reduce tasks to profile. mapreduce.task.profile has to be set to true for the value to be accounted. </description>  </property>  -->
  <!-- <property><name>mapreduce.task.profile.params</name>                 <value></value>                                 <description>JVM profiler parameters used to profile map and reduce task attempts. This string may contain a single format specifier %s that will be replaced by the path to profile.out in the task attempt log directory. To specify different profiling options for map tasks and reduce tasks, more specific parameters mapreduce.task.profile.map.params and mapreduce.task.profile.reduce.params should be used.</description>    </property>  -->
  <!-- <property><name>mapreduce.task.profile.map.params</name>             <value>${mapreduce.task.profile.params}</value> <description>Map-task-specific JVM profiler parameters. See mapreduce.task.profile.params</description> </property>  -->
  <!-- <property><name>mapreduce.task.profile.reduce.params</name>          <value>${mapreduce.task.profile.params}</value> <description>Reduce-task-specific JVM profiler parameters. See mapreduce.task.profile.params</description>      </property>  -->

  <!-- ===================================================================== ~ -->
  <!--                                                                       ~ -->
  <!-- jobhistory properties                                                 ~ -->

  <!-- <property><name>mapreduce.jobhistory.cleaner.enable</name>           <value>true</value>                             <description></description>     </property> -->
  <!-- <property><name>mapreduce.jobhistory.cleaner.interval-ms</name>      <value>86400000</value>                         <description>How often the job history cleaner checks for files to delete,  in milliseconds. Defaults to 86400000 (one day). Files are only deleted if they are older than mapreduce.jobhistory.max-age-ms. </description>     </property>  -->
  <!-- <property><name>mapreduce.jobhistory.max-age-ms</name>               <value>604800000</value>                        <description>Job history files older than this many milliseconds will be deleted when the history cleaner runs. Defaults to 604800000 (1 week). </description> </property>  -->

  <!-- ===================================================================== ~ -->
  <!--                                                                       ~ -->
  <!-- Because we're single-node                                             ~ -->

  <property><name>mapreduce.map.maxattempts</name>                          <value>1</value>                                <description>Expert: The maximum number of attempts per map task. In other words, framework will try to execute a map task these many number of times before giving up on it. </description>    </property>
  <property><name>mapreduce.reduce.maxattempts</name>                       <value>1</value>                                <description>Expert: The maximum number of attempts per reduce task. In other words, framework will try to execute a reduce task these many number of times before giving up on it. </description>      </property>
  <property><name>mapreduce.am.max-attempts</name>                          <value>1</value>                                <description>The maximum number of application attempts. It is a application-specific setting. It should not be larger than the global number set by resourcemanager. Otherwise, it will be override. The default number is set to 2, to allow at least one retry for AM.</description> </property>
  <property><name>mapreduce.map.speculative</name>                          <value>false</value>                            <description>If true, then multiple instances of some map tasks  may be executed in parallel.</description>     </property>
  <property><name>mapreduce.reduce.speculative</name>                       <value>false</value>                            <description>If true, then multiple instances of some reduce tasks  may be executed in parallel.</description>  </property>     <property><name>mapreduce.job.speculative.speculativecap</name>          <value>0.1</value>      <description>The max percent (0-1) of running tasks that can be speculatively re-executed at any time.</description>    </property>
  <!-- <property><name>mapreduce.task.timeout</name>                        <value>60100</value>                            <description>The number of milliseconds before a task will be terminated if it neither reads an input, writes an output, nor updates its status string.  A value of 0 disables the timeout. </description>      </property> -->
  <!-- <property><name>mapreduce.task.merge.progress.records</name>         <value>1000</value>                             <description>The number of records to process during merge before sending a progress notification to the MR ApplicationMaster. </description>  </property> -->
  <!-- <property><name>mapreduce.client.completion.pollinterval</name>      <value>500</value>                              <description>The interval (in milliseconds) between which the JobClient polls the MapReduce ApplicationMaster for updates about job status. You may want to set this to a lower value to make tests run faster on a single node system. Adjusting this value in production may lead to unwanted client-server traffic.</description></property> -->
  <!-- <property><name>mapreduce.tasktracker.outofband.heartbeat</name>     <value>true</value></property> -->

  <!-- <property><name>yarn.app.mapreduce.am.scheduler.heartbeat.interval-ms</name> <value>500</value>                              <description>The interval in ms at which the MR AppMaster should send heartbeats to the ResourceManager</description></property> -->

</configuration>
