
=== How a group works

------
team_n_parks = FOREACH (GROUP park_teams BY (team_id,year_id)) GENERATE
  group.team_id, COUNT_STAR(park_teams) AS n_parks;
vagabonds = FILTER team_n_parks BY n_parks >= 3;

DUMP vagabonds;
(CL4,7)
(CLE,5)
(WS3,4)
(CLE,3)
(DET,3)
...
------

------
mapper(array_fields_of: ParkTeamYear) do |park_id, team_id, year_id, beg_date, end_date, n_games|
 yield [team_id, year_id]
end

# In effect, what is happening in Java:
reducer do |(team_id, year_id), stream|
  n_parks = 0
  stream.each do |*_|
    n_parks += 1
  end
  yield [team_id, year_id, n_parks] if n_parks > 1
end
------

(In actual practice, the ruby version would just call `n_parks = stream.size` rather than iterating)


TODO in part on groups note As Jon Covent says, "Bags are what makes Pig Awesome". SQL doesn't have them, and they bring extraordinary power. They can be of arbitrarily large size, present an ad-hoc object representation, and within limits can themselves be limited, transformed, ordered, threaded, and joined.
They can't be indexed into, and unless you explicitly say so are not ordered.

TODO add diagram showing inner bag like the ThinkBig demo (and reference it)

TODO: a JOIN is used for: direct foreign key join; matching records on a criterion, possibly sparsely; set intersection.

The core operation you will use to put records from one table into context with data from another table is the JOIN. A common application of the JOIN is to reunite data that has been normalized -- that is to say, where the database tables are organized to eliminate any redundancy. For example, each Retrosheet game log lists the ballpark in which it was played but, of course, it does not repeat the full information about that park within every record. Later in the book, (TODO:  REF) we will want to label each game with its geo-coordinates so we can augment each with official weather data measurements.

To join the game_logs table with the parks table, extracting the game time and park geocoordinates, run the following Pig command:

------
gls_with_parks_j = JOIN
   parks     BY (park_id),
   game_logs BY (park_id);
explain gls_with_parks_j;
gls_with_parks = FOREACH gls_with_parks_j GENERATE
 (game_id, gamelogs.park_id, game_time, park_lng, statium_lat);
explain gls_with_parks;
(TODO output of explain command)
------

The output schema of the new `gls_with_parks` table has all the fields from the `parks` table first (because it's first in the join statement), stapled to all the fields from the `game_logs` table. We only want some of the fields, so immediately following the JOIN is a FOREACH to extract what we're interested in. Note there are now two 'park_id' columns, one from each dataset, so in the subsequent FOREACH, we need to dereference the column name with the table from which it came. (TODO: check that Pig does push the projection of fields up above the JOIN). If you run the script, 'examples/geo/baseball_weather/geolocate_games.pig' you will see that its output has example as many records as there are 'game_logs' because there is exactly one entry in the 'parks' table for each park.

In the general case, though, a JOIN can be many to many. Suppose we wanted to build a table listing all the home ballparks for every player over their career. The 'player_seasons' table has a row for each year and team over their career. If a player changed teams mid year, there will be two rows for that player. The 'park_years' table, meanwhile, has rows by season for every team and year it was used as a home stadium. Some ballparks have served as home for multiple teams within a season and in other cases (construction or special circumstances), teams have had multiple home ballparks within a season.

The Pig script (TODO: write script) includes the following JOIN:

------
JOIN
player_park_years=JOIN
 parks(year,team_ID),
 players(year,team_ID);
explain_player_park_year;
------

First notice that the JOIN expression has multiple columns in this case separated by commas; you can actually enter complex expressions here -- almost all (but not all) the things you do within a FOREACH. If you examine the output file (TODO: name of output file), you will notice it has appreciably more lines than the input 'player' file. For example (TODO: find an example of a player with multiple teams having multiple parks), in year x player x played for the x and the y and y played in stadiums p and q. The one line in the 'players' table has turned into three lines in the 'players_parks_years' table.

The examples we have given so far are joining on hard IDs within closely-related datasets, so every row was guaranteed to have a match. It is frequently the case, however, you will join tables having records in one or both tables that will fail to find a match. The 'parks_info' datasets from Retrosheet only lists the city name of each ballpark, not its location. In this case we found a separate human-curated list of ballpark geolocations, but geolocating records -- that is, using a human-readable location name such as "Austin, Texas" to find its nominal geocoordinates (-97.7,30.2) -- is a common task; it is also far more difficult than it has any right to be, but a useful first step is match the location names directly against a gazette of populated place names such as the open source Geonames dataset.

Run the script (TODO: name of script) that includes the following JOIN:

------
park_places = JOIN
 parks BY (location) LEFT OUTER,
 places BY (concatenate(city, ", ", state);
DESCRIBE park_places;
------

In this example, there will be some parks that have no direct match to location names and, of course, there will be many, many places that do not match a park. The first two JOINs we did were "inner" JOINs -- the output contains only rows that found a match. In this case, we want to keep all the parks, even if no places matched but we do not want to keep any places that lack a park. Since all rows from the left (first most dataset) will be retained, this is called a "left outer" JOIN. If, instead, we were trying to annotate all places with such parks as could be matched -- producing exactly one output row per place -- we would use a "right outer" JOIN instead. If we wanted to do the latter but (somewhat inefficiently) flag parks that failed to find a match, you would use a "full outer" JOIN. (Full JOINs are pretty rare.)

TODO: discuss use of left join for set intersection.

In a Pig JOIN it is important to order the tables by size -- putting the smallest table first and the largest table last. (You'll learn why in the "Map/Reduce Patterns" (TODO:  REF) chapter.) So while a right join is not terribly common in traditional SQL, it's quite valuable in Pig. If you look back at the previous examples, you will see we took care to always put the smaller table first. For small tables or tables of similar size, it is not a big deal -- but in some cases, it can have a huge impact, so get in the habit of always following this best practice.

------
NOTE
A Pig join is outwardly similar to the join portion of a SQL SELECT statement, but notice that  although you can place simple expressions in the join expression, you can make no further manipulations to the data whatsoever in that statement. Pig's design philosophy is that each statement corresponds to a specific data transformation, making it very easy to reason about how the script will run; this makes the typical Pig script more long-winded than corresponding SQL statements but clearer for both human and robot to understand.
------

==== Reassemble a Vertically Partitioned Table

Another reason to split data across tables is 'vertical partitioning': storing fields that are very large or seldom used in context within different tables. That's the case with the Wikipedia article tables -- the geolocation information is only relevant for geodata analysis; the article text is both large and not always relevant.



Every stadium a player has played in. (We're going to cheat on the detail of
multiple stints and credit every player with all stadiums visited by the team
of his first stint in a season

------
  -- there are only a few many-to-many cases, so the 89583 seasons in batting
  -- table expands to only 91904 player-park-years. But it's a cross product, so
  -- beware.
SELECT COUNT(*) FROM batting bat WHERE bat.stint = 1;
SELECT bat.player_id, bat.team_id, bat.year_id, pty.park_id
  FROM       batting bat
  INNER JOIN park_team_years pty
    ON bat.year_id = pty.year_id AND bat.team_id = pty.team_id
  WHERE bat.stint = 1
  ORDER BY player_id
  ;
------

What if you only want the distinct player-team-years?
You might naively do a join and then a group by,
or a join and then distinct. Don't do that.

------
  -- DON'T DO THE (pig equivalent) OF THIS to find the distinct teams, years and parks;
  -- it's an extra reduce.
SELECT bat.player_id, bat.nameCommon,
    GROUP_CONCAT(DISTINCT pty.park_id) AS park_ids, COUNT(DISTINCT pty.park_id) AS n_parks,
    GROUP_CONCAT(DISTINCT bat.team_id) AS team_ids,
    MIN(bat.year_id) AS begYear, MAX(bat.year_id) AS endYear
  FROM       bat_war bat
  INNER JOIN park_team_years pty
    ON bat.year_id = pty.year_id AND bat.team_id = pty.team_id
  WHERE bat.stint = 1 AND player_id IS NOT NULL
  GROUP BY player_id
  HAVING begYear > 1900
  ORDER BY n_parks DESC, player_id ASC
  ;
  
  Join bat_yr on (team_id, year_id), pty by (team_id, year_id);
  FOREACH @ GENERATE bat_years::player_id, park_id;
  Group by player_id
  Distinct parks
  
  Cogroup baty by (team_id, year_id), pty by (team_id, year_id);
   distinct park_id, 
------

So now we disclose the most important thing that SQL experts need to break
their brains of:

In SQL, the JOIN is supreme.
In Pig, the GROUP is supreme

A JOIN is, for the most part, just sugar around a COGROUP-and-FLATTEN.
Very often you'll find the simplest path is through COGROUP not JOIN.

In this case, if you start by thinking of the group, you'll see you can eliminate a whole reduce.

(show pig, including a DISTINCT in the fancy-style FOREACH)

==== Join Practicalities

(add note) Joins on null values are dropped even when both are null. Filter nulls. (I can't come up with a good example of this)
(add note) in contrast, all elements with null in a group _will_ be grouped as null. This can be dangerous when large number of nulls: all go to same reducer


=== SQL-to-Pig-to-Hive Cheatsheet

* SELECT..WHERE
* SELECT...LIMit
* GROUP BY...HAVING
* SELECT WHERE... ORDER BY
* SELECT WHERE... SORT BY (just use reducer sort) ~~ (does reducer in Pig guarantee this?)
* SELECT … DISTRIBUTE BY … SORT BY ...
* SELECT ... CLUSTER BY (equiv of distribute by X sort by X)
* Indexing tips
* CASE...when...then
* Block Sampling / Input pruning
* SELECT country_name, indicator_name, `2011` AS trade_2011 FROM wdi WHERE (indicator_name = 'Trade (% of GDP)' OR indicator_name = 'Broad money (% of GDP)') AND `2011` IS NOT NULL CLUSTER BY indicator_name;

SELECT columns or computations FROM table WHERE condition GROUP BY columns HAVING condition ORDER BY column  [ASC | DESC] LIMIT offset,count;

==== Ready Reckoner: How fast should your Pig fly? --> not sure what this is

TODO: move to the first tuning chapter.

The idea is to have you run through a set of pig scripts with datasets of defined size, measuring the throughput of the core operations. The result is a ready reckoner that lets you estimate how long your job _should_ take (and how many map-reduce stages it will use).
