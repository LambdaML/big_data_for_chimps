=== Grouping Operations

TODO in part on groups note As Jon Covent says, "Bags are what makes Pig Awesome". SQL doesn't have them, and they bring extraordinary power. They can be of arbitrarily large size, present an ad-hoc object representation, and within limits can themselves be limited, transformed, ordered, threaded, and joined.
They can't be indexed into, and unless you explicitly say so are not ordered.

TODO add diagram showing inner bag like the ThinkBig demo (and reference it)
TODO add flatten example that crosses the data.



=== Grouping Operations
==== Group Records into a Collection
==== Nested `FOREACH`
==== Distribution of Values Using a Histogram
==== Re-injecting global totals
=== Putting tables in context with JOIN and friends
==== Direct Join: Extend Records with Uniquely Matching Records from Another Table
==== Fill in Holes in a List with a Join on an integer table
==== Join a table with itself (self-join)
==== Enumerating a Many-to-Many Relationship
==== Join Against Another Table Without Discarding Non-Matches
==== Find rows with a match in another table (semi-join)
==== Counting on multiple levels
==== Cube and rollup
=== Handling duplicates
==== Eliminating Duplicates from a Table
==== Eliminating Duplicates from a Query Result:
==== Identifying unique records for a key
==== Identifying duplicated records for a key
==== Eliminating rows that have a duplicated value
=== Set Operations
==== Structural Group Operations (ie non aggregating)
=== Group Elements From Multiple Tables On A Common Attribute (COGROUP)
==== GROUP/COGROUP To Restructure Tables
==== Group flatten regroup
==== Generate a won-loss record
==== Ungrouping operations (FOREACH..FLATTEN) expand records
=== Sorting Operations
==== Season leaders
==== Transpose record into attribute-value pairs
==== Sorting (ORDER BY, RANK) places all records in total order
==== Sorting Records by Key
==== Numbering Records by Sorted Rank
==== Rank records in a group using Stitch/Over
==== Finding Records Associated with Maximum Values
==== Top K Records within a table using ORDER..LIMIT
====  Shuffle a set of records
=== SQL-to-Pig-to-Hive Cheatsheet



==== TOC


* Analytic Patterns part 1: Pipeline Operations
* Eliminating Data
  - Filter Selected Rows Based on an Expression
  - Select a Random Sample of Rows
  - Project Only Chosen Columns by Name
  - Retrieve a fixed number of Rows (LIMIT)
  - Select Rows with the Top-K Values for a Field (move?)
  - Top K Within a Group
  - Select Rows using a Limit and Offset
* Transforming Records
  - Transform Records Individually using `FOREACH`
  - A nested `FOREACH` Allows Intermediate Expressions
  - Place Values into Categorical Bins With a `FOREACH` (move?)
  - Generating Data
  - (Concatenate Multiple Strings?) by Applying a User-Defined Function
    -
* Expanding Data
  - Flatten on a Bag Generates Many Records from a Field with Many Elements
  - Flatten on a Tuple Folds it into its Parent
  - Other Similar Patterns
* Splitting a Table
* Splitting into Multiple Data Flows using `SPLIT`
  - Splitting into files by key by using a Pig Storefunc UDF
  - Splitting a Table into Uniform Chunks
* Treat the Union of Several Tables as a Single Table
  - Load Multiple Files as One Table
  - Treat Several Pig Relation Tables as a Single Table
  - Clean Up Many Small Files by Merging into Fewer Files
* A Quick Look into Baseball


* Group
  - group for groups sake
  - Teams a player player for by year
  - inner-bag operations -- distinct teams in order by year(?)
  - Use the games table for this?

* Group and aggregate:
  - group & summarize number (H by season) and string (names)
  - group all (H)
  - nested FOREACH (obp, slg, ops from counting stats)
  - Move content from cogrpup at bottom describing how group works up here
  - Histogram:
    - Games
    - binned games
    - multiple fields, (?reinject global totals)
  - summing trick:
    - win-loss record
    - ?? HoF standards test: 1pt batting over .300, 1-10 pts for each 0.025 pts of SLG above .300; 1-10 pts for each 0.010 of OBP over 0.300; 1-5 pts for each 200 walks over 300; 1 pt for each 200 HR. (And about a dozen more)

* Join:
  - Direct join on foreign key -- ages for each player season
  - vertical partitioned (Wikipedia articles and metadata; just call this out as an example)
  - sparse join (left join) --
  - filling holes in a list -- histogram of career hits
  - many-to-many join --  ballparks a player has played in
  - self join -- teammates -- team-year pla-plb

* Cogroup:
  - semi-join/anti-join --
  - (move some stuff from below distinct to here);
  - cogroup for cogroups sake?
* Distinct:
  - eliminate dupes;
  - (eliminate dupes in-bag);
  - find unique / duplicated elements;
  - keep one record from many distinct'ed by a field
* Sorting:
  - sort;
  - (inner bag sort);
  - rank;
  - top-k overall, in-bag;
  - tuple with maximum element;
  - shuffling
* Cogroup-decorate-regroup;
  - ?Cube?
  - Rollup Summary Statistics at Multiple Levels
    - Show m/r job doing both summaries at same time, and using summaries directly.
    - Introduce notion of holistic?
    - (but not cube)
* Set operations summary
* SQL-hive-pig cheatsheet

TODO Check against MySQL cookbook patterns




==== Exact median using RANK

Well, we've met another operation with this problem, namely the sort (ORDER BY) operation. It does a first pass to sample the global distribution of keys, then a full map-reduce to place ordered values on the same reducer. Its numerate younger brother, RANK, will do what we need. The quartiles -- the boundaries of the four bins bins each holding 25% of the values -- ...

(Show using RANK and then filter; use the "pre-inject and assert global values" trick for the bin size. Handle the detail of needing to average two values when boundary splits an index, eg median of a table with even number of rows)

==== Approximate median & quantiles using DataFu
 (get better title)


==== Find Aggregate Statistics of a Group

Pretty much every data exploration you perform will involve summarizing datasets using statistical aggregations -- counts, averages and so forth. You have already seen an example of this when we helped the reindeer count UFO visit frequency by month and later in the book, we will devote a whole chapter to statistical summaries and aggregation.

----
SELECT
  team_id, COUNT(*) AS n_seasons, MIN(year_id) as yearBeg, MAX(year_id) as yearEnd
  FROM teams tm
  GROUP BY team_id
  ORDER BY n_seasons DESC, team_id ASC
;
----

-- Group on year; find COUNT(), count distinct, MIN(), MAX(), SUM(), AVG(), STDEV(), byte size


bat_all  = GROUP bats ALL;
hr_stats = FOREACH bat_all {
  hrs_distinct = DISTINCT bats.HR;
  GENERATE
    MIN(bats.HR)        AS hr_min,
    MAX(bats.HR)        AS hr_max,
    AVG(bats.HR)        AS hr_avg,
    SQRT(VAR(bats.HR))  AS hr_stddev,
    SUM(bats.HR)        AS hr_sum,
    COUNT_STAR(bats)    AS n_recs,
    COUNT_STAR(bats) - COUNT(bats.HR) AS hr_n_nulls,
    COUNT(hrs_distinct) AS hr_card,
    hrs_distinct
    ;
  }



SELECT
    MIN(HR)              AS hr_min,
    MAX(HR)              AS hr_max,
    AVG(HR)              AS hr_avg,
    STDDEV_POP(HR)       AS hr_stddev,
    SUM(HR)              AS hr_sum,
    COUNT(*)             AS n_recs,
    COUNT(*) - COUNT(HR) AS hr_n_nulls,
    COUNT(DISTINCT HR)   AS hr_n_distinct -- doesn't count NULL
  FROM bat_season bat
;

SELECT
    MIN(nameFirst)                     AS nameFirst_min,
    MAX(nameFirst)                     AS nameFirst_max,
    --
    MIN(CHAR_LENGTH(nameFirst))        AS nameFirst_strlen_min,
    MAX(CHAR_LENGTH(nameFirst))        AS nameFirst_strlen_max,
    MIN(OCTET_LENGTH(nameFirst))       AS nameFirst_bytesize_max,
    MAX(OCTET_LENGTH(nameFirst))       AS nameFirst_bytesize_max,
    AVG(CHAR_LENGTH(nameFirst))        AS nameFirst_strlen_avg,
    STDDEV_POP(CHAR_LENGTH(nameFirst)) AS nameFirst_strlen_stddev,
    LEFT(GROUP_CONCAT(nameFirst),25)   AS nameFirst_examples,
    SUM(CHAR_LENGTH(nameFirst))        AS nameFirst_strlen_sum,
    --
    COUNT(*)                           AS n_recs,
    COUNT(*) - COUNT(nameFirst)        AS nameFirst_n_nulls,
    COUNT(DISTINCT nameFirst)          AS nameFirst_n_distinct
  FROM bat_career bat
;

SELECT
  player_id,
  MIN(year_id) AS yearBeg,
  MAX(year_id) AS yearEnd,
  COUNT(*)    AS n_years,
    MIN(HR)              AS hr_min,
    MAX(HR)              AS hr_max,
    AVG(HR)              AS hr_avg,
    STDDEV_POP(HR)       AS hr_stddev,
    SUM(HR)              AS hr_sum,
    COUNT(*)             AS n_recs,
    COUNT(*) - COUNT(HR) AS hr_n_nulls,
    COUNT(DISTINCT HR)   AS hr_n_distinct -- doesn't count NULL
  FROM bat_season bat
  GROUP BY player_id
  ORDER BY hr_max DESC
;

-- Count seasons per team
SELECT
  team_id, COUNT(*) AS n_seasons, MIN(year_id) as yearBeg, MAX(year_id) as yearEnd
  FROM teams tm
  GROUP BY team_id
  ORDER BY n_seasons DESC, team_id ASC
;

Finding a median (or other quantiles) is quite difficult at large scale; we'll discuss why and what to do about it in the Statistics chapter (REF).

==== Group Records into a Collection

The fundamental Map/Reduce operation is to group a set of records and operate on that group. In fact, it’s a one-liner in Pig:

We can use `GROUP` to assemble an inline list of the stadiums each team played for by year:

----
team_parks_g = Group team_park_years BY team_id;
team_parks     = FOREACH team_parks_g GENERATE
  group as team_id, team_park_years.(year_id, park_id);
----

The result is always a tuple whose first field is named “Group” -- holding the individual group keys in order. The next field has the full input record with all its keys, even the group key. Here’s a Wukong script that illustrates what is going on:

----
(TODO: Wukong script)
----
==== Nested `FOREACH`

Let's continue our example of finding the list of home ballparks for each player over their career.

(Yikes just skip this section for now)

----
parks = LOAD '.../parks.tsv' AS (...);
player_seasons = LOAD '.../player_seasons.tsv' AS (...);
team_seasons = LOAD '.../team_seasons.tsv' AS (...);

park_seasons = JOIN parks BY park_id, team_seasons BY park_id;
park_seasons = FOREACH park_seasons GENERATE
   team_seasons.team_id, team_seasons.year, parks.park_id, parks.name AS park_name;

player_seasons = FOREACH player_seasons GENERATE
   player_id, name AS player_name, year, team_id;
player_season_parks = JOIN
   parks           BY (year, team_id),
   player_seasons BY (year, team_id);
player_season_parks = FOREACH player_season_parks GENERATE player_id, player_name, parks::year AS year, parks::team_id AS team_id, parks::park_id AS park_id;

player_all_parks = GROUP player_season_parks BY (player_id);
describe player_all_parks;
Player_parks = FOREACH player_all_parks {
   player = FirstFromBag(players);
   home_parks = DISTINCT(parks.park_id);
   GENERATE group AS player_id,
       FLATTEN(player.name),
       MIN(players.year) AS beg_year, MAX(players.year) AS end_year,
       home_parks; -- TODO ensure this is still tuple-ized
}
----

Whoa! There are a few new tricks here.

We would like our output to have one row per player, whose fields have these different flavors:

* Aggregated fields (`beg_year`, `end_year`) come from functions that turn a bag into a simple type (`MIN`, `MAX`).
* The `player_id` is pulled from the `group` field, whose value applies uniformly to the the whole group by definition. Note that it's also in each tuple of the bagged `player_park_seasons`, but then you'd have to turn many repeated values into the one you want...
* ... which we have to do for uniform fields (like `name`) that are not part of the group key, but are the same for all elements of the bag. The awareness that those values are uniform comes from our understanding of the data -- Pig doesn't know that the name will always be the same. The FirstFromBag (TODO fix name) function from the Datafu package grabs just first one of those values
* Inline bag fields (`home_parks`), which continue to have multiple values.

We've applied the `DISTINCT` operation so that each home park for a player appears only once. `DISTINCT` is one of a few operations that can act as a top-level table operation, and can also act on bags within a foreach -- we'll pick this up again in the next chapter (TODO ref). For most people, the biggest barrier to mastery of Pig is to understand how the name and type of each field changes through restructuring operations, so let's walk through the schema evolution.

Nested FOREACH allows CROSS, DISTINCT, FILTER, FOREACH, LIMIT, and ORDER BY (as of Pig 0.12).

We `JOIN`ed player seasons and team seasons on `(year, team_id)`. The resulting schema has those fields twice. To select the name, we use two colons (the disambiguate operator): `players::year`.

After the `GROUP BY` operation, the schema is `group:int, player_season_parks:bag{tuple(player_id, player_name, year, team_id, park_id, park_name)}`. The schema of the new `group` field matches that of the `BY` clause: since `park_id` has type chararray, so does the group field. (If we had supplied multiple fields to the `BY` clause, the `group` field would have been of type `tuple`). The second field, `player_season_parks`, is a bag of size-6 tuples. Be clear about what the names mean here: grouping on the `player_season_parks` _table_ (whose schema has six fields) produced the `player_parks` table. The second field of the `player_parks` table is a tuple of size six (the six fields in the corresponding table) named `player_season_parks` (the name of the corresponding table).

So within the `FOREACH`, the expression `player_season_parks.park_id` is _also_ a bag of tuples (remember, bags only hold tuples!), now size-1 tuples holding only the park_id. That schema is preserved through the `DISTINCT` operation, so `home_parks` is also a bag of size-1 tuples.

----
   team_park_seasons = LOAD '/tmp/team_parks.tsv' AS (
       team_id:chararray,
       park_years: bag{tuple(year:int, park_id:chararray)},
       park_ids_lookup: map[chararray]
       );
   team_parks = FOREACH team_park_seasons { distinct_park_ids = DISTINCT park_years.park_id; GENERATE team_id, FLATTEN(distinct_park_ids) AS park_id; }
   DUMP team_parks;
----

==== Distribution of Values Using a Histogram

One of the most common uses of a group-and-aggregate is to create a histogram showing how often each value (or range of values) of a field occur. We can prepare a histogram of how many times each home-run total was met:

----
G_vals = FOREACH pl_yr_stats GENERATE G;
G_hist = FOREACH (GROUP G_vals BY G) GENERATE
  group AS G, COUNT(G_vals) AS n_seasons;
----

----
SELECT G, COUNT(*) AS n_seasons
  FROM bat_season bat GROUP BY G;
----

A team starts 9 players but has 25 roster spots so most players see very few games. There are cutoff points at 154 (the length of a full season until 1961) and 162 (the current length of a full season), and in the 30's (starting pitchers typically only play every fifth day).

So the pattern here is to

* project only the values,
* Group by the values,
* Produce the group as key and the count as value.


----
H_vals = FOREACH pl_yr_stats GENERATE 10 * CEIL(H/10) AS H_bin;
H_hist = FOREACH (GROUP H_vals BY H_bin) GENERATE
  group AS H_bin, COUNT(H_vals) AS n_seasons;
----

In this case, we prescribed the bins in advance and each bin had uniform width -- answering the question ""How many records fell into each bin?". Another approach is to find an 'equal-height' histogram, answering the question "How should we size the bins so that each has the same values?" (Effectively the same question as finding quantiles.) Do you see why this is fiendishly hard? You can find out the answer to why it's hard, and what to do about it, in the Statistics chapter (REF)

==== Histogram on Multiple Fields Simultaneously

(Pick up the chars count from previous chapter)
==== Re-injecting global totals

To calculate a relative frequency
Requires total count of records,
a global statistic.


. This brings up one of the more annoying things about Hadoop programming. The global_term_info result is two lousy values, needed to turn the global _counts_ for each term into the global _frequency_ for each term. But a pig script just orchestrates the top-level motion of data: there's no intrinsic way to bring the result of a step into the declaration of following steps. The proper recourse is to split the script into two parts, and run it within a workflow tool like Rake, Drake or Oozie. The workflow layer can fish those values out of the HDFS and inject them as runtime parameters into the next stage of the script.

If the global statistic is relatively static, we prefer to cheat. We instead ran a version of the script that found the global count of terms and usages, then copy/pasted their values as static parameters at the top of the script. This also lets us calculate the ppm frequency of each term and the other term statistics in a single pass. To ensure our time-traveling shenanigans remain valid, we add an `ASSERT` statement which compares the memoized values to the actual totals

==== Cogroup regroup, maybe

Move this below to be with cogroup-regroup section.

An exceptionally useful table for understanding baseball is the run expectancy table.

That's the same analysis used to determine whether to go for it on fourth down in American football, and a useful model for predicting asset prices and other "Bayesian" analysis (TECH am I using the right term): given a discrete assessment of the current state, what future outcomes result?

To do this, we need to first determine the final inning and final game outcome for each event, and then determine the distribution of outcomes across all events for each game state. The first requires placing all events into context by inning and game; the second requires placing them into context by event type.

(Shoot this won't work for demonstrating the cogroup-regroup I think)

TODO for geographic count example use the Datafu udf to do the document counts

=== Putting tables in context with JOIN and friends

==== Direct Join: Extend Records with Uniquely Matching Records from Another Table

Using a join to extend the records in one table with the fields from one matching record in another is a very common pattern. Datasets are commonly stored as tables in 'normalized' form -- that is, having tables structured to minimize redundancy and dependency.

(Replace with the 'people' table)

The global hourly weather dataset has one table giving the metadata for every weather station: identifiers, geocoordinates, elevation, country and so on. The giant tables listing the hourly observations from each weather station are normalized to not repeat the station metadata on each line, only the weather station id. However, later in the book (REF) we'll do geographic analysis of the weather data -- and one of the first tasks will be to denormalize the geocoordinates of each weather station with its observations, letting us group nearby observations.

hang weight, height and BMI off of their OPS (overall hitting); ISO ("isolated power");
and number of stolen bases per time on base (loosely tied to speed)

SELECT bat.player_id, peep.nameCommon, begYear,
    peep.weight, peep.height,
    703*peep.weight/(peep.height*peep.height) AS BMI, -- measure of body type
    PA, OPS, ISO
  FROM bat_career bat
  JOIN people peep ON bat.player_id = peep.player_id
  WHERE PA > 500 AND begYear > 1910
  ORDER BY BMI DESC
  ;

(add note) Joins on null values are dropped even when both are null. Filter nulls. (I can't come up with a good example of this)
(add note) in contrast, all elements with null in a group _will_ be grouped as null. This can be dangerous when large number of nulls: all go to same reducer

-- don't do this (needs two group-bys):
SELECT n_seasons, COUNT(*), COUNT(*)/n_seasons
  FROM (SELECT COUNT(*) AS n_seasons FROM batting) t1,
  (SELECT COUNT(*) AS n_stints FROM batting GROUP BY player_id, year_id HAVING n_stints > 1) stintful
  ;
-- instead use the summing trick (needs only one group-by):
SELECT COUNT(*), (COUNT(*)-SUM(IF(stint = 1, 1, 0)))/COUNT(*), COUNT(*) FROM batting WHERE stint <= 2;

==== Reassemble a Vertically Partitioned Table

Another reason to split data across tables is 'vertical partitioning': storing fields that are very large or seldom used in context within different tables. That's the case with the Wikipedia article tables -- the geolocation information is only relevant for geodata analysis; the article text is both large and not always relevant.

Use the pitchers and batters table

Call forward to the merge join
==== Fill in Holes in a List with a Join on an integer table

If we prepare a histogram of career hits, similar to the one above for seasons, you'll find that Pete Rose (4256 hits) and Ty Cobb (4189 hits) have so many more hits than the third-most player (Hank Aaron, 3771 hits) there are gaps in the output bins. To make it so that every bin has an entry, do an outer join on the integer table. (See, we told you the integers table was surprisingly useful.)

----
SET @H_binsize = 10;
SELECT bin, H, IFNULL(n_H,0)
  FROM      (SELECT @H_binsize * idx AS bin FROM numbers WHERE idx <= 430) nums
  LEFT JOIN (SELECT @H_binsize*CEIL(H/@H_binsize) AS H, COUNT(*) AS n_H
    FROM bat_career bat GROUP BY H) hist
  ON hist.H = nums.bin
  ORDER BY bin DESC
;
----
==== Join a table with itself (self-join)

-- teammates (played for same team same season, discarding second and later
-- stints; players half table?)  note that we're cheating a bit: players may
-- change teams during the season (happens in about 7% of player seasons).

-- note the explosion: 90k player-seasons lead to 3,104,324 teammate-year pairs.
-- the distinct pairing is 2 million

SELECT DISTINCT b1.player_id, b2.player_id
  FROM bat_season b1, bat_season b2
  WHERE b1.team_id = b2.team_id          -- same team
    AND b1.year_id = b2.year_id          -- same season
    AND b1.player_id != b2.player_id     -- reject self-teammates
  GROUP BY b1.player_id
  ;
==== Enumerating a Many-to-Many Relationship

-- Every stadium a player has played in. (We're going to cheat on the detail of
-- multiple stints and credit every player with all stadiums visited by the team
-- of his first stint in a season
--

-- there are only a few many-to-many cases, so the 89583 seasons in batting
-- table expands to only 91904 player-park-years. But it's a cross product, so
-- beware.
SELECT COUNT(*) FROM batting bat WHERE bat.stint = 1;
SELECT bat.player_id, bat.team_id, bat.year_id, pty.park_id
  FROM       batting bat
  INNER JOIN park_team_years pty
    ON bat.year_id = pty.year_id AND bat.team_id = pty.team_id
  WHERE bat.stint = 1
  ORDER BY player_id
  ;

--
-- What if you only want the distinct player-team-years?
-- You might naively do a join and then a group by,
-- or a join and then distinct. Don't do that.

-- DON'T DO THE (pig equivalent) OF THIS to find the distinct teams, years and parks;
-- it's an extra reduce.
SELECT bat.player_id, bat.nameCommon,
    GROUP_CONCAT(DISTINCT pty.park_id) AS park_ids, COUNT(DISTINCT pty.park_id) AS n_parks,
    GROUP_CONCAT(DISTINCT bat.team_id) AS team_ids,
    MIN(bat.year_id) AS begYear, MAX(bat.year_id) AS endYear
  FROM       bat_war bat
  INNER JOIN park_team_years pty
    ON bat.year_id = pty.year_id AND bat.team_id = pty.team_id
  WHERE bat.stint = 1 AND player_id IS NOT NULL
  GROUP BY player_id
  HAVING begYear > 1900
  ORDER BY n_parks DESC, player_id ASC
  ;

--
-- So now we disclose the most important thing that SQL experts need to break
-- their brains of:
--
-- In SQL, the JOIN is supreme.
-- In Pig, the GROUP is supreme
--
-- A JOIN is, for the most part, just sugar around a COGROUP-and-FLATTEN.
-- Very often you'll find the simplest path is through COGROUP not JOIN.
--
-- In this case, if you start by thinkingn of the group, you'll see you can eliminate a whole reduce.
--
-- (show pig, including a DISTINCT in the fancy-style FOREACH)

Now introduce cogroup
==== Join Against Another Table Without Discarding Non-Matches

using a left join so you can fix up remnants
note: haven't actually run this, need to load geonames

----
SELECT pk.*
  FROM      parks pk
  LEFT JOIN geonames.places gn
    ON (pk.city = gn.city AND pk.state = gn.region1)
    OR (pk.parkname = gn.placename)
;
----

-- See advanced joins: bag left outer join from DataFu
-- See advanced joins: Left outer join on three tables: http://datafu.incubator.apache.org/docs/datafu/guide/more-tips-and-tricks.html
-- See Time-series: Range query using cross
-- See Time-series: Range query using prefix and UDFs
-- See advanced joins: Sparse joins for filtering, with a HashMap (replicated)
-- Out of scope: Bitmap index
-- Out of scope: Bloom filter joins
-- See time-series: Self-join for successive row differences

==== Find rows with a match in another table (semi-join)

Semi-join: just care about the match, don't keep joined table; anti-join is where you keep the non-matches and also don't keep the joined table. Again, use left or right so that the small table occurs first in the list. Note that a semi-join has only one row per row in dominant table -- so needs to be a cogroup and sum or a join to distinct'ed table (extra reduce, but lets you do a fragment replicate join.)

Select player seasons where they made the all-star team.
You might think you could do this with a join:

----
-- Don't do this... produces duplicates!
bats_g    = JOIN allstar BY (player_id, year_id), bats BY (player_id, year_id);
bats_as   = FOREACH bats_g GENERATE bats::player_id .. bats::HR;
----

The result is wrong, and even a diligent spot-check will probably fail to notice. You see, from 1959-1962 there were multiple All-Star games (!), and so each singular row in the `bat_season` table became two rows in the result for players in those years.

Instead, use a `COGROUP` and filter:

----
ast     = FOREACH allstar GENERATE player_id, year_id;
bats_g  = COGROUP ast     BY (player_id, year_id), bats BY (player_id, year_id);
bats_f  = FILTER  bats_g  BY NOT IsEmpty(ast);
bats_as = FOREACH bats_f  GENERATE FLATTEN(bats);
----

In our case there was only one row per player/year, but in the general case where the dominant table has more than one row for a key, the `FLATTEN` operation will generate just that many rows in the output.

To finding rows with no match in another table -- known as an anti-join -- simply use `FILTER BY IsEmpty()` instead of `FILTER BY NOT IsEmpty()`

==== Counting on multiple levels

-- fraction of people with multiple stints per year (about 7%)

-- don't do this (needs two group-bys):
SELECT n_seasons, COUNT(*), COUNT(*)/n_seasons
  FROM (SELECT COUNT(*) AS n_seasons FROM batting) t1,
  (SELECT COUNT(*) AS n_stints FROM batting GROUP BY player_id, year_id HAVING n_stints > 1) stintful
  ;
-- instead use the summing trick (needs only one group-by):
SELECT COUNT(*), (COUNT(*)-SUM(IF(stint = 1, 1, 0)))/COUNT(*), SUM(IF(stint = 1, 1, 0)) FROM batting WHERE stint <= 2;
==== Cube and rollup

-- stats by team, division and league

http://joshualande.com/cube-rollup-pig-data-science/
https://cwiki.apache.org/confluence/display/Hive/Enhanced+Aggregation,+Cube,+Grouping+and+Rollup#EnhancedAggregation,Cube,GroupingandRollup-CubesandRollups

From manual: "Handling null values in dimensions
Since null values are used to represent subtotals in cube and rollup operation, in order to differentiate the legitimate null values that already exists as dimension values, CUBE operator converts any null values in dimensions to "unknown" value before performing cube or rollup operation. For example, for CUBE(product,location) with a sample tuple (car,null) the output will be
{(car,unknown), (car,null), (null,unknown), (null,null)}"

http://labs.opendns.com/2013/04/08/pig-jruby/?referred=1 -- pairs_r = FOREACH (GROUP raw BY client_ip) {
  client_queries = FOREACH raw GENERATE ts, name;
  client_queries = ORDER client_queries BY ts, name;
  GENERATE client_queries;
};
=== Handling duplicates

==== Eliminating Duplicates from a Table

-- Every team a player has played for
SELECT DISTINCT player_id, team_id from batting;

==== Eliminating Duplicates from a Query Result:

--
-- All parks a team has played in
--
SELECT team_id, GROUP_CONCAT(DISTINCT park_id ORDER BY park_id) AS park_ids
  FROM park_team_years
  GROUP BY team_id
  ORDER BY team_id, park_id DESC
  ;

==== Identifying unique records for a key

-- * Distinct: players with a unique first name (once again we urge you: crawl through your data. Big data is a collection of stories; the power of its unusual effectiveness mode comes from the comprehensiveness of those stories. even if you aren't into baseball this celebration of the diversity of our human race and the exuberance of identity should fill you with wonder.)
--
-- But have you heard recounted the storied diamond exploits of Firpo Mayberry,
-- Zoilo Versalles, Pi Schwert or Bevo LeBourveau?  OK, then how about
-- Mysterious Walker, The Only Nolan, or Phenomenal Smith?  Mul Holland, Sixto
-- Lezcano, Welcome Gaston or Mox McQuery?  Try asking your spouse to that your
-- next child be named for Urban Shocker, Twink Twining, Pussy Tebeau, Bris Lord, Boob
-- Fowler, Crazy Schmit, Creepy Crespi, Cuddles Marshall, Vinegar Bend Mizell,
-- or Buttercup Dickerson.
--

SELECT nameFirst, nameLast, COUNT(*) AS n_usages
  FROM bat_career
  WHERE    nameFirst IS NOT NULL
  GROUP BY nameFirst
  HAVING   n_usages = 1
  ORDER BY nameFirst
  ;
* Counting Missing Values
* Counting and Identifying Duplicates
* Determining Whether Values are Unique

==== Identifying duplicated records for a key

-- group by, then emit bags with more than one size; call back to the won-loss example

Once again, what starts out looking like one of the high-level operations turns into a GROUP BY.

Up above, the allstar table almost led us astray due to the little-known fact that some years featured multiple All-Star games. We can pull out the rows matching those fields:


-- Teams who played in more than one stadium in a year
SELECT COUNT(*) AS n_parks, pty.*
  FROM park_team_years pty
  GROUP BY team_id, year_id
  HAVING n_parks > 1

(Do this with games table?)
==== Eliminating rows that have a duplicated value

(ie the whole row isn't distinct,
-- just the field you're distinct-ing on.
-- Note: this chooses an arbitrary value from each group
SELECT COUNT(*) AS n_asg, ast.*
  FROM allstarfull ast
  GROUP BY year_id, player_id
  HAVING n_asg > 1
  ;
=== Set Operations

-- Partition a Set into Subsets: SPLIT, but keep in mind that the SPLIT operation doesn't short-circuit.
-- Find the Union of Sets UNION-then-DISTINCT
--    (note that it doesn't dedupe, doesn't order, and doesn't check for same schema)
--    * don't combine the career stats tables by union-group; do it with cogroup.
-- Prepare a Distinct Set from a Collection of Records: DISTINCT
-- Intersect: semi-join (allstars)
-- * Difference (in a but not in b): cogroup keep only empty (non-allstars)
-- * Equality (use symmetric difference): result should be empty
-- * Symmetric difference: in A or B but not in A intersect B -- do this with aggregation: count 0 or 1 and only keep 1
-- * http://datafu.incubator.apache.org/docs/datafu/guide/set-operations.html
-- * http://www.cs.tufts.edu/comp/150CPA/notes/Advanced_Pig.pdf

==== Structural Group Operations (ie non aggregating)

--
-- * GROUP/COGROUP To Restructure Tables
-- * Group Elements From Multiple Tables On A Common Attribute (COGROUP)
-- * Denormalize Normalized
--   - roll up stints
--   - Normalize Denormalized (flatten)

You can group more than one dataset at the same time. In weather data, there is one table listing the location and other essentials of each weather station and a set of tables listing, for each hour, the weather at each station. Here’s one way to combine them into a new table, giving the explicit latitude and longitude of every observation:

----
G1=GROUP WSTNS BY (ID1,ID2), WOBS BY (ID1,ID2);
G2=FLATTEN G1…
G3=FOR EACH G2 …
----

This is equivalent to the following Wukong job:

----
(TODO: Wukong job)
----

(TODO: replace with an example where you would use a pure code group).

=== Group Elements From Multiple Tables On A Common Attribute (COGROUP)

The fundamental structural operation in Map/Reduce is the COGROUP:  assembling records from multiple tables into groups based on a common field; this is a one-liner in Pig, using, you guessed it, the COGROUP operation. This script returns, for every world map grid cell, all UFO sightings and all airport locations within that grid cell footnote:[We've used the `quadkey` function to map geocoordinates into grid cells; you'll learn about in the Geodata Chapter (REF)]:

----
sightings = LOAD('/data/gold/geo/ufo_sightings/us_ufo_sightings.tsv') AS (...);
airports     = LOAD('/data/gold/geo/airflights/us_airports.tsv') AS (...);
cell_sightings_airports = COGROUP
   sightings by quadkey(lng, lat),
   airports  by quadkey(lng, lat);
STORE cell_sightings_locations INTO '...';
----

In the equivalent Map/Reduce algorithm, you label each record by both the indicated key and a number based on its spot in the COGROUP statement (here, records from sightings would be labeled 0 and records from airports would be labeled 1). Have Hadoop then PARTITION and GROUP on the COGROUP key with a secondary sort on the table index. Here is how the previous Pig script would be done in Wukong:

----
mapper(partition_keys: 1, sort_keys: 2) do
 recordize_by_filename(/sightings/ => Wu::Geo::UfoSighting, /airport/ => Wu::Geo::Airport)
 TABLE_INDEXES = { Wu::Geo::UfoSighting => 0, Wu::Geo::Airport => 1 }
 def process(record)
   table_index = TABLE_INDEXES[record.class] or raise("Don't know how to handle records of type '{record.class}'")
   yield( [Wu::Geo.quadkey(record.lng, record.lat), table_index, record.to_wire] )
 end
end

reducer do
 def recordize(quadkey, table_index, jsonized_record) ; ...; end
 def start(key, *)
   @group_key = key ;
   @groups = [ [], [] ]
 end
 def accumulate(quadkey, table_index, record)
   @groups[table_index.to_i] << record
 end
 def finalize
   yield(@group_key, *groups)
 end
end
----

The Mapper loads each record as an object (using the file name to recognize which class to use) and then emits the quadkey, the table index (0 for sightings, 1 for airports) and the original record's fields. Declaring partition keys 1, sort keys 2 insures all records with the same quadkey are grouped together on the same Reducer and all records with the same table index arrive together. The body of the Reducer makes temporary note of the GROUP key, then accumulates each record into an array based on its type.

The result of the COGROUP statement always has the GROUP key as the first field. Next comes the set of elements from the table named first in the COGROUP statement -- in Pig, this is a bag of tuples, in Wukong, an array of objects. After that comes the set of elements from the next table in the GROUP BY statement and so on.

While a standalone COGROUP like this is occasionally interesting, it is also the basis for many other common patterns, as you'll see over the next chapters.


==== GROUP/COGROUP To Restructure Tables

This next pattern is one of the more difficult to picture but also one of the most important to master. Once you can confidently recognize and apply this pattern, you can consider yourself a black belt in the martial art of Map/Reduce.

(TODO: describe this pattern)

==== Group flatten regroup


--     * OPS+ -- group on season, normalize, reflatten
--     * player's highest OPS+: regroup on player, top

Words/tiles:

(Word tile wd_doc_ct doc_tot)
Group on word find total word count, total doc count
(Word tile
    doc-usg:val(wd,doc)
    doc-tot_usgs:sum(u|*,doc)   doc-n_wds:count(w|*,doc)
    wd-tot_usgs:sum(u|wd,*)                                                wd-n_docs:count(d|wd,*)
    tot-usgs:sum(*,*)                  n_wds:count(w|*,*)            ct-docs:count(d|*,*)

   usgs    tile-ct-wds     tile-ct-docs

    pl-yr-ops:val(pl,yr)
    yr-tot-ops:sum(ops|*,yr)            yr-n-pl:count(pl|*,yr)   yr-avg-ops:avg(ops|*,yr)
    pl-yr-oz:(pl-yr-ops/yr-avg-ops)
    pl-max-oz:max(pl-yr-oz|p,*)

    yr-g:(*,y)
    te-yr-g:(*,te,y)

Name tables for dominating primary keys. If a value is subsumed, omit. Keys are x_id always
              pl-yr[te,ops]  pk-te-yr[]
              pl-info[...] -- vertical partition on any other func(pl)
If Non unique key, assumed that table xx has id xx_id





 Do not get join happy: find year averages, join all on year, group on player
Just group on year then flatten with records.


Style: n_H, ct_H, H_ct? n_H because the n_* have same schema, and because ^^^

==== Generate a won-loss record

Using the summing trick footnote:[we're skipping some details such as forfeited games, so the numbers won't agree precisely with the combined team numbers.]

----
-- generate a summable value for each game, once for home and once for away:
home_games = FOREACH games GENERATE
  home_team_id AS team_id, year_id,
  IF (home_runs_ct > away_runs_ct, 1,0) AS win,
  IF (home_runs_ct < away_runs_ct, 1,0) AS loss,
  If (forfeit == ...) as forf_w, ...
  ;
away_games = FOREACH games GENERATE
  away_team_id AS team_id, year_id,
  IF (home_runs_ct < away_runs_ct, 1,0) AS win,
  IF (home_runs_ct > away_runs_ct, 1,0) AS loss
  ;
----

Now you might be tempted (especially if you are coming from SQL land) to follow this with a UNION of `home_games` and `away_games`. Don't! Instead, use a COGROUP. Once you've wrapped your head around it, it's simpler and more efficient.

----
team_games = COGROUP home_games BY (team_id, year_id), away_games BY (team_id, year_id);
----

Each combination of team and year creates one row with the following fields:

* `group`, a tuple with the `team_id` and `year_id`
* `home_games`, a bag holding tuples with `team_id`, `year_id`, `win` and `loss`
* `away_games`, a bag holding tuples with `team_id`, `year_id`, `win` and `loss`

----
team_games:
((BOS,2004),  {(BOS,2004,1,0),(BOS,2004,1,0),...}, {(BOS,2004,0,1),(BOS,2004,1,0),...})
...
----

You should notice a few things:

* The group values go in a single field (the first one) called `group`.
* Since we grouped on two fields, the group value is a tuple; if we had grouped on one field it would have the same schema as that field
* The name of the _table_ in the COGROUP BY statement became the name of the _field_ in the result
* The group values appear redundantly in each tuple of the bag. That's OK, we're about to project them out.

This is one of those things to think back on when you're looking at a script and saying "man, I just have this feeling this script has more reduce steps than it deserves".

The next step is to calculate the answer:

----
...
team_games = COGROUP home_games BY....
winloss_record = FOREACH team_games {
  wins   = SUM(home_games.win)    + SUM(away_games.win);
  losses = SUM(home_games.loss)   + SUM(away_games.loss);
  G      = COUNT_STAR(home_games) + COUNT_STAR(away_games);
  G_home = COUNT_STAR(home_games);
  ties   = G - (wins + losses);
  GENERATE group.team_id, group.year_id, G, G_home, wins, losses, ties;
};
----

Exercise: Do this instead with a single GROUP. Hint: the first FOREACH should have a FLATTEN.

==== Ungrouping operations (FOREACH..FLATTEN) expand records

So far, we've seen using a group to aggregate records and (in the form of `JOIN’) to match records between tables.
Another frequent pattern is restructuring data (possibly performing aggregation at the same time). We used this several times in the first exploration (TODO ref): we regrouped wordbags (labelled with quadkey) for quadtiles containing composite wordbags; then regrouping on the words themselves to find their geographic distribution.

The baseball data is closer at hand, though, so l

----
team_player_years = GROUP player_years BY (team,year);
FOREACH team_player_years GENERATE
   FLATTEN(player_years.player_id), group.team, group.year, player_years.player_id;
----

In this case, since we grouped on two fields, `group` is a tuple; earlier, when we grouped on just the `player_id` field, `group` was just the simple value.

The contextify / reflatten pattern can be applied even within one table. This script will find the career list of teammates for each player -- all other players with a team and year in common footnote:[yes, this will have some false positives for players who were traded mid-year. A nice exercise would be to rewrite the above script using the game log data, now defining teammate to mean "all other players they took the field with over their career".].

----
GROUP player_years BY (team,year);
FOREACH
   cross all players, flatten each playerA/playerB pair AS (player_a
FILTER coplayers BY (player_a != player_b);
GROUP by playerA
FOREACH {
   DISTINCT player B
}
----

Here's another

The result of the cross operation will include pairing each player with themselves, but since we don't consider a player to be their own teammate we must eliminate player pairs of the form `(Aaronha, Aaronha)`. We did this with a FILTER immediate before the second GROUP (the best practice of removing data before a restructure), but a defensible alternative would be to `SUBTRACT` playerA from the bag right after the `DISTINCT` operation.


=== Sorting Operations


* RANK: Dense, not dense
* Number records with a serial or unique index
  - use rank with (the dense that give each a number)
  - use file name index and row number in mapper (ruby UDF)
* Sorting Subsets of a Table (order inside cogroup)
* Controlling Summary Display Order
* Sorting and NULL Values; Controlling Case Sensitivity of String Sorts
*
Note: ORDER BY is NOT stable; can't guarantee that records with same keys will keep same order
Note about ORDER BY and keys across reducers -- for example, you can't do the sort | uniq trick


==== Season leaders

-- * Selecting top-k Records within Group
-- GROUP...FOREACH GENERATE TOP
-- most hr season-by-season

==== Transpose record into attribute-value pairs

Group by season, transpose, and take the top 10 for each season, attribute pair


==== Sorting (ORDER BY, RANK) places all records in total order

To put all records in a table in order, it's not sufficient to use the sorting that each reducer applies to its input. If you sorted names from a phonebook, file `part-00000` will have names that start with A, then B, up to Z; `part-00001` will also have names from A-Z; and so on. The collection has a _partial_ order, but we want the 'total order' that Pig's `ORDER BY` operation provides. In a total sort, each record in `part-00000` is in order and precedes every records in `part-00001`; records in `part-00001` are in order and precede every record in `part-00002`; and so forth. From our earlier example to prepare topline batting statistics for players, let's sort the players in descending order by the "OPS" stat (slugging average plus offensive percent, the simplest reasonable estimator of a player's offensive contribution).

----
player_seasons = LOAD `player_seasons` AS (...);
qual_player_seasons = FILTER player_years BY plapp > what it should be;
player_season_stats = FOREACH qual_player_seasons GENERATE
   player_id, name, games,
   hits/ab AS batting_avg,
   whatever AS slugging_avg,
   whatever AS offensive_pct
   ;
player_season_stats_ordered = ORDER player_season_stats BY (slugging_avg + offensive_pct) DESC;
STORE player_season_stats INTO '/tmp/baseball/player_season_stats';
----

This script will run _two_ Hadoop jobs. One pass is a light mapper-only job to sample the sort key, necessary for Pig to balance the amount of data each reducer receives (we'll learn more about this in the next chapter (TODO ref). The next pass is the map/reduce job that actually sorts the data: output file `part-r-00000` has the earliest-ordered records, followed by `part-r-00001`, and so forth.

NOTE: The custom partitioner of an `ORDER` statement subtly breaks the reducer contract: it may send records having the same key to different reducers. This will cause them to be in different output (`part-xxxxx`) files, so make sure anything using the sorted data doesn't assume keys uniquely correspond to files.

==== Sorting Records by Key

Sorting records by key

==== Numbering Records by Sorted Rank

--   - ORDER by multiple fields: sort on OPS to three places then use games then playerid
--   - note value of stabilizing list
-- - (how do NULLs sort?)
-- - ASC / DESC: fewest strikeouts per plate appearance

==== Rank records in a group using Stitch/Over

==== Finding Records Associated with Maximum Values

For each player, find their best significant season by OPS:

----


-- For each season by a player, select the team they played the most games for.
-- In SQL, this is fairly clumsy (involving a self-join and then elimination of
-- ties) In Pig, we can ORDER BY within a foreach and then pluck the first
-- element of the bag.

SELECT bat.player_id, bat.year_id, bat.team_id, MAX(batmax.Gmax), MAX(batmax.stints), MAX(team_ids), MAX(Gs)
  FROM       batting bat
  INNER JOIN (SELECT player_id, year_id, COUNT(*) AS stints, MAX(G) AS Gmax, GROUP_CONCAT(team_id) AS team_ids, GROUP_CONCAT(G) AS Gs FROM batting bat GROUP BY player_id, year_id) batmax
  ON bat.player_id = batmax.player_id AND bat.year_id = batmax.year_id AND bat.G = batmax.Gmax
  GROUP BY player_id, year_id
  -- WHERE stints > 1
  ;

-- About 7% of seasons have more than one stint; only about 2% of seasons have
-- more than one stint and more than a half-season's worth of games
SELECT COUNT(*), SUM(mt1stint), SUM(mt1stint)/COUNT(*) FROM (SELECT player_id, year_id, IF(COUNT(*) > 1 AND SUM(G) > 77, 1, 0) AS mt1stint FROM batting GROUP BY player_id, year_id) bat

-- TOP(topN, sort_column_idx, bag_of_tuples)
-- must have an explicit field -- can't use an expression

Leaderboard By Season-and-league

-- GROUP BY year_id, lg_id

-- There is no good way to find the tuples associated with the minimum value.
-- EXERCISE: make a "BTM" UDF, having the same signature as the "TOP" operation,
-- to return the lowest-n tuples from a bag.

==== Top K Records within a table using ORDER..LIMIT

--      Most hr in a season
--      Describe pigs optimization of order..limit

* Pulling a Section from the Middle of a Result Set: rank and filter? Modify the quantile/median code?

* Hard in SQL but easy in Pig: Finding Rows Containing Per-Group Minimum or Maximum Value, Displaying One Set of Values While Sorting by Another:
--  - can only ORDER BY an explicit field. In SQL you can omit the sort expression from the table (use expression to sort by)
* Sorting a Result Set (when can you count on reducer order?)

====  Shuffle a set of records

See notes on random numbers.


You might also enjoy the random number table, holding 350 million 64-bit numbers directly from random.org (7 GB of 20-digit decimal numbers)
* 160-bit numbers in hexadecimal form
* 32 64-bit numbers (2048-bits per row)

cogroup events by team_id
... there's a way to do this in one less reduce in M/R -- can you in Pig?

=== SQL-to-Pig-to-Hive Cheatsheet

* SELECT..WHERE
* SELECT...LIMit
* GROUP BY...HAVING
* SELECT WHERE... ORDER BY
* SELECT WHERE... SORT BY (just use reducer sort) ~~ (does reducer in Pig guarantee this?)
* SELECT … DISTRIBUTE BY … SORT BY ...
* SELECT ... CLUSTER BY (equiv of distribute by X sort by X)
* Indexing tips
* CASE...when...then
* Block Sampling / Input pruning
* SELECT country_name, indicator_name, `2011` AS trade_2011 FROM wdi WHERE (indicator_name = 'Trade (% of GDP)' OR indicator_name = 'Broad money (% of GDP)') AND `2011` IS NOT NULL CLUSTER BY indicator_name;

SELECT columns or computations FROM table WHERE condition GROUP BY columns HAVING condition ORDER BY column  [ASC | DESC] LIMIT offset,count;



// ------------- CRUFT -------------------------
// ------------- CRUFT -------------------------
// ------------- CRUFT -------------------------
// ------------- CRUFT -------------------------

// Ignore below.

=== In statistics Chapter

==== Cube and rollup
stats by team, division and league

cogroup events by team_id
... there's a way to do this in one less reduce in M/R -- can you in Pig?

=== in Time-series chapter

* Running total http://en.wikipedia.org/wiki/Prefix_sum
* prefix sum value; by combining list ranking, prefix sums, and Euler tours, many important problems on trees may be solved by efficient parallel algorithms.[3]
* Self join of table on its next row (eg timeseries at regular sample)

=== Don't know how to do these

* Computing Team Standings
* Producing Master-Detail Lists and Summaries
* Find Overlapping Rows
* Find Gaps in Time-Series
* Find Missing Rows in Series / Count all Values
* Calculating Differences Between Successive Rows
* Finding Cumulative Sums and Running Averages

==== Tables

* `games`

* `events`: the amazing Retrosheet project has _play-by-play_ information for
  nearly every game since the 1970s. By the time

* `pitchfx`: a true reminder that we live in the future, Major League
  Baseball makes available the trajectory of every pitch from every game with
  full game state since 2007.

* `allstarfull` table: About halfway through a season, players with a particularly strong
  performance (or fanbase) are elected to the All-Star game.

* `halloffame` table: Players with exceptionally strong careers (or particularly strong fanbase
  among old white journalists) are elected to the Hall of Fame (hof).


* player_id: unique identifier for each player, built from their name and an ascending index
* team_id: three-letter unique identifier for a team
* park_id: five-letter unique identifier for a park (stadium)
* G (Games): the number of
