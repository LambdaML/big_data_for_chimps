
== Structural Operations

=== Introduction

The overriding focus here is to equip you with the toolkit of analytic patterns.
The most meaningful way to introduce these patterns is to demonstrate their use in service of a question of real interest.
the main storyline of these chapters will be to find quantitative indicators of exceptional performance, and we'll pick that thread up repeatedly.
But where a pattern has no natural demonstration in service of that primary story, we non-sequitur into questions that could form a necessary piece of some other investigation:
"here's how you'd track changes in each team's roster over time", "is the stereotypical picture of the big brawny home-run hitter true." (TODO-qem please replace with what you found to be the most interesting one-offs (ie side-roads we didn't explore)).
And at several points, immediately on peeking down a side road the data comes forth with a story of its own, and so there are also a few brief side trips to follow such a tale.
But as we revisit the player-performance exploration, you should recognize not just a way for fantasy baseball players to get an edge, but strategies for quantifying the behavior of any sort of outlier. Here, it's baseball players, but similar questions will apply when examining agents posing security threats, factors causing manufacturing defects, cell strains with a significantly positive response, and many other topics of importance.

=== Grouping Records into a Bag by Key

The GROUP BY operation is at the heart of every structural operation. It's a
one-liner in Pig to collect all the stadiums each team has played for:

------
park_teams_g = GROUP park_teams BY team_id;
------

The result of a group is always a field called 'group', having the schema of
the key (atom) or keys (tuple); and then one field per grouped table, each
named for the table it came from. Notice that the name we used to refer to
the _table_ is now also the name for a _field_. This will confuse you at
first, but soon become natural. Until then, use `DESCRIBE` liberally.

------
DESCRIBE park_teams_g;
-- park_teams_g: {
--    group: chararray,
--    park_teams: {
--        ( park_id: chararray, team_id: chararray, year_id: long,
--          beg_date: chararray, end_date: chararray, n_games: long ) } }
------

Notice that the _full record_ is kept, even including the keys:

------
(ALT,{(ALT01,ALT,1884,1884-04-30,1884-05-31,18)})
(ANA,{(ANA01,ANA,2001,2001-04-10,2001-10-07,81),(ANA01,ANA,2010,2010-04-05,2010-09-29,81),...})
------

This means it's pretty common to immediately project using a FOREACH, and we
can even put the `GROUP BY` statement inline:

------
team_pkyr_pairs = FOREACH (GROUP park_teams BY team_id) GENERATE
  group AS team_id, park_teams.(park_id,year_id);
-- (ALT,{(ALT01,1884)})
-- (ANA,{(ANA01,2001),(ANA01,2010),(ANA01,2002),...})
------

Notice the `park_teams.(park_id,year_id)` form, which gives us a bag of
(park_id,year_id) pairs. Using `park_teams.park_id, park_teams.year_id`
instead gives two bags, one with park_id tuples and one with year_id tuples:

------
team_pkyr_bags = FOREACH (GROUP park_teams BY team_id) GENERATE
  group AS team_id, park_teams.park_id, park_teams.year_id;
-- (ALT, {(ALT01)}, {(1884)})
-- (ANA, {(ANA01),(ANA01),(ANA01),...}, {(2001),(2010),(2002),...})

DESCRIBE team_pkyr_pairs;
-- team_parks: { team_id: chararray, { (park_id: chararray, year_id: long) } }

DESCRIBE team_pkyr_bags;
-- team_parks: { team_id: chararray, { (park_id: chararray) }, { (year_id: long) } }
------

You can group on multiple fields.  For each park and team, find all the years
that the park hosted that team:

------
park_team_g = GROUP park_teams BY (park_id, team_id);
------

The first field is still called 'group', but it's now a tuple

------
DESCRIBE park_team_g;
-- park_team_g: {
--   group: (park_id: chararray, team_id: chararray),
--   park_teams: { (park_id: chararray, team_id: chararray, year_id: long, ...) } }
------

and so our `FOREACH` statement looks a bit different:

------
park_team_occupied = FOREACH(GROUP park_teams BY (park_id, team_id)) GENERATE
  group.park_id, group.team_id, park_teams.year_id;

=> LIMIT park_team_occupied 3 ; DUMP @;
-- (ALB01,TRN,{(1882),(1880),(1881)})
-- (ALT01,ALT,{(1884)})
-- (ANA01,ANA,{(2009),(2008),(1997)...})
------

==== Counting Occurrences of a Key

The typical reason to group records is to operate on the bag of values it
forms, and that's how we'll spend much of this chapter -- the data bag is a
very powerful concept. Let's take a quickie tour of what we can do to a
group; afterwards we'll see the internals of how a group works before moving
on to its broader applications.

As you can see, sometimes a team has more than one "home" stadium in a
season. Typically this is due to stadium repairs or late-season makeups for
cancelled games. A few other times MLB has opened the season with a series in
Japan or Mexico for publicity. In any case, it's rare enough that it's
difficult to conceive how a team could end up with more than a couple home
stadiums in a year. Let's confirm our feel for the data using `COUNT_STAR`,
which counts all elements of a bag:

------
team_n_parks = FOREACH (GROUP park_teams BY (team_id,year_id)) GENERATE
  group.team_id, COUNT_STAR(park_teams) AS n_parks;
vagabonds = FILTER team_n_parks BY n_parks >= 3;
DUMP vagabonds;
(CL4,7)
(CLE,5)
(WS3,4)
(CLE,3)
(DET,3)
...
------

Always, always look through the data and seek 'second stories'. In this case
you'll notice that the 1898 Cleveland Spiders called seven (!) different
stadiums home during the season. Is this an anomaly in the data? Could we
possibly have messed up this three-line script? Or could teams really have had
three, four, even seven home stadiums? This demands a closer look.

=== Representing a Collection of Values with a Delimited String


Let's keep the count of parks, but also list the parks themselves for
inspection.  We could keep dumping the values in Pig's oddball output format,
but this is a good opportunity to introduce a very useful pattern:
denormalizing a collection of values into a single delimited field.

The format Pig uses to dump bags and tuples to disk wastes characters and is not safe to use in
general: any string containing a comma or bracket will cause its record to be mis-interpreted. For
simple data structures such as a list, we are better off concatenating the values together using a
delimiter: a character with no other meaning that does not appear in any of the values. This
preserves the rows-and-columns representation of the table that Pig handles best. It also lets us
keep using the oh-so-simple TSV format for interchange with Excel, `cut` and other commandline
tools, and later runs of Pig itself. Storing data this way means we do have to pack and unpack the
value ourselves, which is an added burden when we need access to the array members. But if accessing
the list contents is less frequent this can act as a positive feature: we can move the field around
as a simple string and only pay the cost of constructing the full data structure when necessary.

The BagToString function will serialize a bag of values into a single
delimited field as follows:

------
team_year_w_parks = FOREACH (GROUP park_teams BY (team_id, year_id)) GENERATE
  group.team_id,
  COUNT_STAR(park_teams) AS n_parks,
  BagToString(park_teams.park_id, '^') AS park_ids;
-- (ALT,1,ALT01)
-- (ANA,1,ANA01)
-- ...
-- (CL4,7,CHI08^CLE05^CLL01^PHI09^ROC02^ROC03^STL05)
------

This script ouputs four fields -- park_id, year, count of stadiums, and the
names of the stadiums used separated by a `^` caret delimiter. Like colon
':', comma `,`, and slash '/', it doesn't need to be escaped at the
commandline; like those and semicolon `;`, pipe `|`, and bang `!`, it is
visually lightweight and can be avoided within a value.  Don't use the wrong
delimiter for addresses ("Fargo, ND"), dates ("2014-08-08T12:34:56+00:00"),
paths (`/tmp/foo`) or unsanitized free text (`It's a girl! ^_^ \m/ |:-)`).

Since the park ids are formed from the first characters of the city name, we
can recognize that the Spiders' home fields include two stadiums in Cleveland
plus "home" stadiums in Philadelphia, Rochester, St. Louis, and Chicago.
These aren't close enough to be alternatives in case of repairs, and 1898
baseball did not call for publicity tours. Were they rotating among these
fields, or just spending a day or so at each? Let's see how many were played
at each stadium.

==== Representing a Complex Data Structure with a Delimited String

Instead of a simple list of park ids, we'd now like to serialize a collection
of (park id, number of games) pairs. We can handle this case, and the case
where we want to serialize an object with simple attribute-value pairs, by
using two delimiters: one for separating list elements and one for delimiting
its contents.

------
team_year_w_pkgms = FOREACH (GROUP park_teams BY (team_id,year_id)) {
  pty_ordered     = ORDER park_teams BY n_games DESC;
  pk_ng_pairs     = FOREACH pty_ordered GENERATE
    CONCAT(park_id, ':', (chararray)n_games) AS pk_ng_pair;
  --
  GENERATE group.team_id, group.year_id,
    COUNT_STAR(park_teams) AS n_parks,
    BagToString(pk_ng_pairs,'|') AS pk_ngs;
  };
------

Whoa, a few new things going on here. We've snuck the ORDER BY statement into
a few previous examples even though it won't be covered until later in the
chapter (REF), but always as a full-table operator. Here we're using it
within the body of a FOREACH to sort each bag locally, rather than a total
sort of the table as a whole. In most cases an ORDER BY in the first slot (as
the above) has the nice property of being "free": we just ask Hadoop to do a
secondary-sort on the data as it lands on the reducer.

After the `ORDER BY` statement, we use a _nested_ `FOREACH` to staple each
park onto the number of games at that park, delimited with a colon. (Along
the way you'll see we also typecast the n_games value, since the CONCAT
method expects a `chararray`.) The final GENERATE line creates records naming
the team, the count of parks, and the list of park-usages pairs:

------
-- ALT  1   ALT01:18
-- ANA  1   ANA01:82
-- ...
-- CL4  7   CLE05:40|PHI09:9|STL05:2|ROC02:2|CLL01:2|CHI08:1|ROC03:1
------


Out of http://www.baseball-reference.com/teams/CLV/1898.shtml[156 games] that
season, the Spiders played only 42 in Cleveland. They held 15 "home games" in
other cities, and played _ninety-nine_ away games -- in all, nearly
three-quarters of their season on the road.

The http://www.baseballlibrary.com/chronology/byyear.php?year=1898[Baseball
Library Chronology] sheds some light. It turns out that labor problems
prevented play at their home or any other stadium in Cleveland for a stretch
of time, and so they relocated to Philadelphia while that went on. What's
more, on June 19th police arrested the entire team _during_ footnote:[The
Baseball Library Chronology does note that "not so coincidentally‚ the
Spiders had just scored to go ahead 4-3‚ so the arrests assured Cleveland of
a victory."  Sounds like the officers, not devoid of hometown pride, might
have enjoyed a few innings of the game first.] a home game for violating the
Sunday "blue laws" footnote:[As late as 1967, selling a 'Corning Ware dish
with lid' in Ohio was still enough to get you convicted of "Engaging in
common labor on Sunday":
www.leagle.com/decision/19675410OhioApp2d44_148]. Little wonder the Spiders
decided to take their talents elsewhere than Cleveland! The following year
they played 50 straight on the road, won fewer than 13% of their games
overall (20-134, the worst single-season record ever) and immediately
disbanded at season's end.


==== Representing a Complex Data Structure with a JSON-encoded String

So their extreme position is not a mistake; is it an anomaly? The first three
characters of the park id mirror the city name, so we can identify not just
alternative parks but season spent in alternative cities. And since an 1898
season is quite pre-modern, let's also keep around the year_id field to see
what it says.

------
-- Prepare the city field
pktm_city     = FOREACH park_teams GENERATE
  team_id, year_id, park_id, n_games,
  SUBSTRING(park_id, 0,3) AS city;
------

------
-- First grouping: stats about each city of residence
pktm_stats = FOREACH (GROUP pktm_city BY (team_id, year_id, city)) {
  pty_ordered   = ORDER   pktm_city BY n_games DESC;
  pk_ct_pairs   = FOREACH pty_ordered GENERATE CONCAT(park_id, ':', (chararray)n_games);
  GENERATE
    group.team_id,
    group.year_id,
    group.city                   AS city,
    COUNT_STAR(pktm_city)        AS n_parks,
    SUM(pktm_city.n_games)       AS n_city_games,
    MAX(pktm_city.n_games)       AS max_in_city,
    BagToString(pk_ct_pairs,'|') AS parks
    ;
};
------

The records we're forming are significantly more complex this time. With
fields of numbers or constrained categorical values, stapling together
delimited values is a fine approach. But when fields become this complex, or
when there's any danger of stray delimiters sneaking into the record, if
you're going to stick with TSV you are better off using JSON encoding to
serialize the field. It's a bit more heavyweight but nearly as portable, and
it happy bundles complex structures and special characters to hide within TSV
files. footnote:[And if nether JSON nor simple-delimiter is appropriate, use
Parquet or Trevni, big-data optimized formats that support complex data
structures. As we'll explain in chapter (REF), those are your three choices:
TSV with delimited fields; TSV with JSON fields or JSON lines on their own;
or Parquet/Trevni. We don't recommend anything further.]

TODO: make the code better match the story here, make the record a bit less
byzantine.

------
-- Next, assemble full picture:
farhome_gms = FOREACH (GROUP pktm_stats BY (team_id, year_id)) {
  pty_ordered   = ORDER   pktm_stats BY n_city_games DESC;
  city_pairs    = FOREACH pty_ordered GENERATE CONCAT(city, ':', (chararray)n_city_games);
  n_home_gms    = SUM(pktm_stats.n_city_games);
  n_main_city   = MAX(pktm_stats.n_city_games);
  n_main_park   = MAX(pktm_stats.max_in_city);
  -- a nice trick to make the modern-ness easily visible while scanning the data:
  is_modern     = (group.year_id >= 1905 ? 'mod' : NULL);
  --
  GENERATE group.team_id, group.year_id,
    is_modern                      AS is_modern,
    n_home_gms                     AS n_home_gms,
    n_home_gms - n_main_city       AS n_farhome_gms,
    n_home_gms - n_main_park       AS n_althome_games,
    COUNT_STAR(pktm_stats)         AS n_cities,
    BagToString(city_pairs,'|')    AS cities,
    BagToString(pktm_stats.parks,'|')    AS parks
    ;
};
farhome_gms = ORDER farhome_gms BY n_cities DESC, n_farhome_gms DESC;
--
-- CL4	1898	   	57	17	17	6	CLE:40|PHI:9|ROC:3|STL:2|CLL:2|CHI:1	CLE05:40|PHI09:9|ROC02:2|ROC03:1|STL05:2|CLL01:2|CHI08:1
-- CLE	1902	   	65	5 	5 	5	CLE:60|FOR:2|COL:1|CAN:1|DAY:1      	CLE05:60|FOR03:2|COL03:1|CAN01:1|DAY01:1
-- ...
-- MON	2003	mod	81	22	22	2	MON:59|SJU:22                       	MON02:59|SJU01:22
-- MON	2004	mod	80	21	21	2	MON:59|SJU:21                       	MON02:59|SJU01:21
-- ...
-- CHA	1969	mod	81	11	11	2	CHI:70|MIL:11                       	CHI10:70|MIL05:11
-- CHA	1968	mod	81	9 	9 	2	CHI:72|MIL:9                        	CHI10:72|MIL05:9
-- BRO	1957	mod	77	8 	8 	2	NYC:69|JER:8                        	NYC15:69|JER02:8
------

Inspecting the data answers the question of whether the Spiders were an
outlier: no. Considered against the teams of their era, they look much more
normal. In the early days baseball was still literally getting its act together
and teams hopped around frequently. Since 1905, no team has seen home bases
in three cities, and the three cases where a team spent any significant time
in an alternate city each tell a notable story.

In 2003 and 2004, les pauvres Montreal Expos were sentenced to play 22 "home"
games in San Juan (Puerto Rico) and only 59 back in Montreal. The rudderless
franchise had been sold back to the league itself and was being shopped
around in preparation for a move to Washington, DC. With no real stars, no
home-town enthusiasm, and no future in Montreal, MLB took the opportunity to
build its burgeoning fanbase in Latin America and so deployed the team to
Puerto Rico part-time. The 1968-1969 Chicago White Sox (CHA) were similarly
nation-building in Milwaukee; the owner of the 1956-1957 Brooklyn Dodgers
slipped them away for a stint in New Jersey in order to pressure Brooklyn for
a new stadium.

You won't always want to read a second story to the end as we have here, but
it's important to at least identify unusual features of your data set -- they
may turn out to explain more than you'd think.

NOTE: In traditional analysis with sampled data, edge cases undermine the
data, presenting the spectre of a non-representative sample or biased
result. In big data analysis on comprehensive data, the edge cases _prove_
the data. Here's what we mean. Since 1904, only a very few teams have
multiple home stadiums, and no team has had more than two home stadiums in a
season. Home-field advantage gives a significant edge: the home team plays
the deciding half of the final inning, their roster is constructed to take
advantage of the ballpark's layout, and players get to eat home-cooked meals,
enjoy the cheers of encouraging fans, and spend a stretch of time in one
location. The Spiders and Les Expos and a few others enjoyed only part of
those advantages. XX % of our dataset is pre-modern and Y% had six or more
home games in multiple cities.

With a data set this small there's no good way to control for these unusual
circumstances, and so they represent outliers that taint our results. With a
large and comprehensive data set those small fractions would represent
analyzable populations of their own. With millions of seasons, we could
conceivably baseline the jet-powered computer-optimized schedules of the
present against the night-train wanderjahr of Cleveland Spiders and other
early teams.



=== Group and Aggregate

Some of the happiest moments you can have analyzing a massive data set come
when you are able to make it a slightly less-massive data set.  Aggregate
functions -- ones that turn the whole of a group into a scalar value -- are
the best path to this joy.

==== Aggregate Statistics of a Group

In the previous chapter, we used each player's seasonal counting stats --
hits, home runs, and so forth -- to estimate seasonal rate stats -- how well
they get on base (OPS), how well they clear the bases (SLG) and an overall
estimate of offensive performance (OBP). But since we were focused on
pipeline operations, we only did so on a season-by-season basis.

A group-and-aggregate on the seasonal stats starts us on the path to
characterizing each player's career:

bat_careers = FOREACH (GROUP bat_seasons BY player_id) {
  team_ids = DISTINCT bat_seasons.team_id;
  totG   = SUM(bat_seasons.G);   totPA  = SUM(bat_seasons.PA);  totAB  = SUM(bat_seasons.AB);
  totH   = SUM(bat_seasons.H);   totBB  = SUM(bat_seasons.BB);  totHBP = SUM(bat_seasons.HBP); totR   = SUM(bat_seasons.R);
  toth1B = SUM(bat_seasons.h1B); toth2B = SUM(bat_seasons.h2B); toth3B = SUM(bat_seasons.h3B); totHR  = SUM(bat_seasons.HR);
  OBP    = 1.0*(totH + totBB + totHBP) / totPA;
  SLG    = 1.0*(toth1B + 2*toth2B + 3*toth3B + 4*totHR) / totAB;
  GENERATE
    group                          AS player_id,
    COUNT_STAR(bat_seasons)        AS n_seasons,
    COUNT_STAR(team_ids)           AS n_distinct_teams,
    MIN(bat_seasons.year_id)	     AS beg_year,
    MAX(bat_seasons.year_id)       AS end_year,
    totG   AS G,   totPA  AS PA,  totAB  AS AB,
    totH   AS H,   totBB  AS BB,  totHBP AS HBP,
    toth1B AS h1B, toth2B AS h2B, toth3B AS h3B, totHR AS HR,
    OBP AS OBP, SLG AS SLG, (OBP + SLG) AS OPS
    ;
};

==== Completely Summarizing a Field


In the preceding case, the aggregate functions were used to create an output
table with similar structure to the input table, but at a coarser-grained
relational level: career rather than season. The result was a new table to
analyze, not a conceptual report.

Statistical aggregations also let you summarize groups and tables with
well-understood descriptive statistics. By sketching their essential
characteristics at dramatically smaller size, we make the data easier to work
with but more importantly we make it possible to comprehend.

The following functions are built in to Pig:

* Count of all values: `COUNT_STAR(bag)`
* Count of non-Null values: `COUNT(bag)`
* Minimum / Maximum non-Null value: `MIN(bag)` / `MAX(bag)`
* Sum of non-Null values: `SUM(bag)`
* Average of non-Null values: `AVG(bag)`

There are a few additional summary functions that aren't native features of Pig, but are offered by
Linkedin's might-as-well-be-native DataFu package. footnote:[If you've forgotten/never quite learned
what those functions mean, hang on for just a bit and we'll demonstrate them in context. If that
still doesn't do it, set a copy of http://www.amazon.com/dp/039334777X[Naked Statistics] or
http://www.amazon.com/Head-First-Statistics-Dawn-Griffiths/dp/0596527586[Head First Statistics] next
to this book. Both do a good job of efficiently imparting what these functions mean and how to use
them without assuming prior expertise or interest in mathematics. This is important material
though. Every painter of landscapes must know how to convey the essence of a
https://www.youtube.com/watch?v=YLO7tCdBVrA[happy little tree] using a few deft strokes and not the
prickly minutae of its 500 branches; the above functions are your brushes footnote:[Artist/Educator
Bob Ross: "Anyone can paint, all you need is a dream in your heart and a little bit of practice" --
hopefully you're feeling the same way about Big Data analysis.].

* Cardinality (i.e. the count of distinct values): combine the `DISTINCT` operation and the `COUNT_STAR` function as demonstrated below, or use the DataFu `HyperLogLogPlusPlus` UDF
* Variance of non-Null values: `VAR(bag)`, using the `datafu.pig.stats.VAR` UDF
* Standard Deviation of non-Null values: `SQRT(VAR(bag))`
* Quantiles: `Quantile(bag)` or `StreamingQuantile(bag)`
* Median (50th Percentile Value) of a Bag: `Median(bag)` or `StreamingMedian(bag)`

The previous chapter (REF) has details on how to use UDFs, and so we're going to leave the details
of that to the sample code. You'll also notice we list two functions for quantile and for median.
Finding the exact median or other quantiles (as the Median/Quantile UDFs do) is costly at large
scale, and so a good approximate algorithm (StreamingMedian/StreamingQuantile) is well
appreciated. Since the point of this stanza is to characterize the values for our own sense-making,
the approximate algorithms are appropriate. We'll have much more to say about why finding quantiles
is costly, why finding averages isn't, and what to do about it in the Statistics chapter (REF).

------
weight_yr_stats = FOREACH (GROUP bat_seasons BY year_id) {
  dist         = DISTINCT bat_seasons.weight;
  sorted_a     = FILTER   bat_seasons.weight BY weight IS NOT NULL;
  sorted       = ORDER    sorted_a BY weight;
  some         = LIMIT    dist.weight 5;
  n_recs       = COUNT_STAR(bat_seasons);
  n_notnulls   = COUNT(bat_seasons.weight);
  GENERATE
    group,
    AVG(bat_seasons.weight)        AS avg_val,
    SQRT(VAR(bat_seasons.weight))  AS stddev_val,
    MIN(bat_seasons.weight)        AS min_val,
    FLATTEN(ApproxEdgeile(sorted)) AS (p01, p05, p50, p95, p99),
    MAX(bat_seasons.weight)        AS max_val,
    --
    n_recs                         AS n_recs,
    n_recs - n_notnulls            AS n_nulls,
    COUNT_STAR(dist)               AS cardinality,
    SUM(bat_seasons.weight)        AS sum_val,
    BagToString(some, '^')         AS some_vals
    ;
};
------






=== Summarizing Aggregate Statistics of a Full Table



To summarize the statistics of a full table, we use a `GROUP ALL` statement.
That is, instead of `GROUP [table] BY [key]`, write `GROUP [table]
ALL`. Everything else is as usual:


------
weight_summary = FOREACH (GROUP bat_seasons ALL) {
  dist         = DISTINCT bat_seasons.weight;
  sorted_a     = FILTER   bat_seasons.weight BY weight IS NOT NULL;
  sorted       = ORDER    sorted_a BY weight;
  some         = LIMIT    dist.weight 5;
  n_recs       = COUNT_STAR(bat_seasons);
  n_notnulls   = COUNT(bat_seasons.weight);
  GENERATE
    group,
    AVG(bat_seasons.weight)             AS avg_val,
    SQRT(VAR(bat_seasons.weight))       AS stddev_val,
    MIN(bat_seasons.weight)             AS min_val,
    FLATTEN(ApproxEdgeile(sorted))  AS (p01, p05, p50, p95, p99),
    MAX(bat_seasons.weight)             AS max_val,
    --
    n_recs                          AS n_recs,
    n_recs - n_notnulls             AS n_nulls,
    COUNT_STAR(dist)                AS cardinality,
    SUM(bat_seasons.weight)         AS sum_val,
    BagToString(some, '^')          AS some_vals
    ;
};
------

As we hope you readily recognize, using the `GROUP ALL` operation can be
dangerous, as it requires bringing all the data onto a single reducer.

We're safe here, even on larger datasets, because all but one of the
functions we supplied above are efficiently 'algebraic': they can be
significantly performed in the map phase and combiner'ed. This eliminates
most of the data before the reducer. The cardinality calculation, done here
with a nested DISTINCT operation, is the only real contributor to
reducer-side data size. For this dataset its size is manageable, and if it
weren't there is a good approximate cardinality function. We'll explain the
why and the how of algebraic functions and these approximate methods in the
Statistics chapter.  But you'll get a good feel for what is and isn't
efficient through the examples in this chapter.)
    
NOTE: Note the syntax of the full-table group statement. There's no I in
TEAM, and no BY in GROUP ALL.

=== Summarizing the Length of a String Field

We showed how to examine the constituents of a string field in the preceding
chapter, under "Tokenizing a String" (REF). But for forensic purposes similar
to the prior example, it's useful to summarize their length distribution.

------
name_first_summary_0 = FOREACH (GROUP bat_seasons ALL) {
  dist       = DISTINCT bat_seasons.name_first;
  lens       = FOREACH  bat_seasons GENERATE SIZE(name_first) AS len;
  --
  n_recs     = COUNT_STAR(bat_seasons);
  n_notnulls = COUNT(bat_seasons.name_first);
  --
  examples   = LIMIT    dist.name_first 5;
  snippets   = FOREACH  examples GENERATE (SIZE(name_first) > 15 ? CONCAT(SUBSTRING(name_first, 0, 15),'…') : name_first) AS val;
  GENERATE
    group,
    'name_first'                   AS var:chararray,
    MIN(lens.len)                  AS minlen,
    MAX(lens.len)                  AS maxlen,
    --
    AVG(lens.len)                  AS avglen,
    SQRT(VAR(lens.len))            AS stdvlen,
    SUM(lens.len)                  AS sumlen,
    --
    n_recs                         AS n_recs,
    n_recs - n_notnulls            AS n_nulls,
    COUNT_STAR(dist)               AS cardinality,
    MIN(bat_seasons.name_first)    AS minval,
    MAX(bat_seasons.name_first)    AS maxval,
    BagToString(snippets, '^')     AS examples,
    lens  AS lens
    ;
};

name_first_summary = FOREACH name_first_summary_0 {
  sortlens   = ORDER lens  BY len;
  pctiles    = ApproxEdgeile(sortlens);
  GENERATE
    var,
    minlen, FLATTEN(pctiles) AS (p01, p05, p10, p50, p90, p95, p99), maxlen,
    avglen, stdvlen, sumlen,
    n_recs, n_nulls, cardinality,
    minval, maxval, examples
    ;
};
------

=== Calculating the Distribution of Numeric Values with a Histogram

One of the most common uses of a group-and-aggregate is to create a histogram
showing how often each value (or range of values) of a field occur. This
calculates the distribution of seasons played -- that is, it counts the
number of players whose career lasted only a single season; who played for
two seasons; and so forth, up

------
vals = FOREACH bat_careers GENERATE n_seasons AS bin;
seasons_hist = FOREACH (GROUP vals BY bin) GENERATE
  group AS bin, COUNT_STAR(vals) AS ct;

vals = FOREACH (GROUP bat_seasons BY (player_id, name_first, name_last)) GENERATE
  COUNT_STAR(bat_seasons) AS bin, flatten(group);
seasons_hist = FOREACH (GROUP vals BY bin) {
  some_vals = LIMIT vals 3;
  GENERATE group AS bin, COUNT_STAR(vals) AS ct, BagToString(some_vals, '|');
};
------

So the pattern here is to

* project only the values,
* Group by the values,
* Produce the group as key and the count as value.

==== Binning Data for a Histogram

------
H_vals = FOREACH bat_seasons GENERATE H;
H_hist = FOREACH (GROUP H_vals BY H) GENERATE
  group AS val, COUNT_STAR(H_vals) AS ct;
------

What binsize? These zoom in on the tail -- more than 2000 games played. A bin size of 200 is too coarse; it washes out the legitimate gaps. The bin size of 2 is too fine -- the counts are small and there are many trivial gaps. We chose a bin size of 50 games; it's meaningful (50 games represents about 1/3 of a season), it gives meaty counts per bin even when the population starts to become sparse, while preserving the gaps that demonstrate the epic scope of the career of Pete Rose (our 3,562-game outlier).

==== Interpreting Histograms and Quantiles

Different underlying mechanics will give different distributions.

------
DEFINE histogram(table, key) RETURNS dist {
  vals = FOREACH $table GENERATE $key;
  $dist = FOREACH (GROUP vals BY $key) GENERATE
    group AS val, COUNT_STAR(vals) AS ct;
};

DEFINE binned_histogram(table, key, binsize, maxval) RETURNS dist {
  numbers = load_numbers_10k();
  vals = FOREACH $table GENERATE (ROUND($key / $binsize) * $binsize) AS bin;
  all_bins = FOREACH numbers GENERATE (num0 * $binsize) AS bin;
  all_bins = FILTER  all_bins BY (bin <= $maxval);
  $dist = FOREACH (COGROUP vals BY bin, all_bins BY bin) GENERATE
    group AS bin, (COUNT_STAR(vals) == 0L ? Null : COUNT_STAR(vals)) AS ct;
};
------

===== Distribution of Games Played

------
season_G_hist = histogram(bat_seasons, 'G');
career_G_hist = binned_histogram(bat_careers, 'G', 50, 3600);

career_G_hist_2   = binned_histogram(bat_careers, 'G', 2, 3600);
career_G_hist_200 = binned_histogram(bat_careers, 'G', 200, 3600);

career_HR_hist = binned_histogram(bat_careers, 'HR', 10, 800);
------

===== Distribution of Birth and Death day of year

------
vitals = FOREACH peeps GENERATE
  height_in,
  10*CEIL(weight_lb/10.0) AS weight_lb,
  birth_month,
  death_month;

birth_month_hist = histogram(vitals, 'birth_month');
death_month_hist = histogram(vitals, 'death_month');
height_hist = histogram(vitals, 'height_in');
weight_hist = histogram(vitals, 'weight_lb');

attr_vals = FOREACH vitals GENERATE
  FLATTEN(Transpose(height, weight, birth_month, death_month)) AS (attr, val);

attr_vals_nn = FILTER attr_vals BY val IS NOT NULL;

peep_stats   = FOREACH (GROUP attr_vals_nn BY attr) GENERATE
group                        AS attr,
COUNT_STAR(attr_vals_nn)     AS ct_all,
COUNT_STAR(attr_vals_nn.val) AS ct;

peep_stats = FOREACH (GROUP attr_vals_nn ALL) GENERATE
  BagToMap(CountVals(attr_vals_nn.attr)) AS cts:map[long];

peep_hist = FOREACH (GROUP attr_vals BY (attr, val)) {
  ct = COUNT_STAR(attr_vals);
  GENERATE
    FLATTEN(group) AS (attr, val),
    ct             AS ct
    -- , (float)ct / ((float)peep_stats.ct) AS freq
    ;
};
peep_hist = ORDER peep_hist BY attr, val;

one = LOAD '$data_dir/stats/numbers/one.tsv' AS (num:int);
ht = FOREACH one GENERATE peep_stats.cts#'height';
------

==== Extreme Populations and Confounding Factors

To reach the major leagues, a player must possess multiple extreme
attributes: ones that are easy to measure, like being tall or being born in a
country where baseball is popular; and ones that are not, like field vision,
clutch performance, the drive to put in outlandishly many hours practicing
skills. Any time you are working with extremes as we are, you must be very
careful to assume their characteristics resemble the overall population's.

Here again are the graphs for players' height and weight, but now graphed
against (in light blue) the distribution of height/weight for US males aged
20-29 footnote:[US Census Department, Statistical Abstract of the United States.
Tables 206 and 209, Cumulative Percent Distribution of Population by
(Weight/Height) and Sex, 2007-2008; uses data from the U.S. National Center
for Health Statistics].

The overall-population distribution is shown with light blue bars, overlaid
with a normal distribution curve for illustrative purposes. The population of
baseball players deviates predictably from the overall population: it's an
advantage to The distribution of player weights, meanwhile, is shifted
somewhat but with a dramatically smaller spread.

Surely at least baseball players are born and die like the rest of us, though?

A lot of big data analyses explore population extremes: manufacturing
defects, security threats, disease carriers, peak performers.  Elements
arrive into these extremes exactly because multiple causative features drive
them there (such as an advantageous height or birth month); and a host of
other conflated features follow from those deviations (such as those stemming
from the level of fitness athletes maintain).

So whenever you are examining populations of outliers, you cannot depend on
their behavior resembling the universal population. Normal distributions may
not remain normal and may not even retain a central tendency; independent
features in the general population may become tightly coupled in the outlier
group; and a host of other easy assumptions become invalid. Stay alert.


=== Calculating a Relative Distribution Histogram

==== Calculating Percent Relative to Total

The histograms we've calculated have results in terms of counts. The results do a better general job of enforcing comparisons if express them as relative frequencies: as fractions of the total count. You know how to find the total:

------
HR_stats = FOREACH (GROUP bats BY ALL) GENERATE COUNT_STAR(bats) AS n_players;
------

The problem is that HR_stats is a single-row table, and so not something we can use directly in a FOREACH expression. Pig gives you a piece of syntactic sugar for this specific case of a one-row table footnote:[called 'scalar projection' in Pig terminology]: project the value as tablename.field as if it were an inner bag, but slap the field's type (in parentheses) in front of it like a typecast expression:

------
HR_stats = FOREACH (GROUP bats BY ALL) GENERATE COUNT_STAR(bats) AS n_total;
HR_hist  = FOREACH (GROUP bats BY HR) {
  ct = COUNT_STAR(bats);
  GENERATE HR as val,
    ct/( (long)HR_stats.n_total ) AS freq,
    ct;
};
------

Typecasting the projected field as if you were simply converting the schema of a field from one scalar type to another acts as a promise to Pig  that what looks like column of possibly many values will turn out to have only row. In return, Pig will understand that you want a sort of über-typecast of the projected column into what is effectively its literal value.

=== Re-injecting Global Values

==== Re-injecting global totals

Sometimes things are more complicated, and what you'd like to do is perform light synthesis of the results of some initial Hadoop jobs, then bring them back into your script as if they were some sort of "global variable". But a pig script just orchestrates the top-level motion of data: there's no good intrinsic ways to bring the result of a step into the declaration of following steps. You can use a backhoe to tear open the trunk of your car, but it's not really set up to push the trunk latch button. The proper recourse is to split the script into two parts, and run it within a workflow tool like Rake, Drake or Oozie. The workflow layer can fish those values out of the HDFS and inject them as runtime parameters into the next stage of the script.

In the case of global counts, it would be so much faster if we could sum the group counts to get the global totals; but that would mean a job to get the counts, a job to get the totals, and a job to get the relative frequencies. Ugh.

If the global statistic is relatively static, there are occasions where we prefer to cheat. Write the portion of the script that finds the global count and stores it, then comment that part out and inject the values statically -- the sample code shows you how to do it with with a templating runner, as runtime parameters, by copy/pasting, or using the `cat` Grunt shell statement. Then, to ensure your time-traveling shenanigans remain valid, add an `ASSERT` statement comparing the memoized values to the actual totals. Pig will not only run the little checkup stage in parallel if possible, it will realize that the data size is small enough to run as a local mode job -- cutting the turnaround time of a tiny job like that in half or better.

------
-- cheat mode:
-- HR_stats = FOREACH (GROUP bats BY ALL) GENERATE COUNT_STAR(bats) AS n_total;
SET HR_stats_n_total = `cat $out_dir/HR_stats_n_total`;

HR_hist  = FOREACH (GROUP bats BY HR) {
ct = COUNT_STAR(bats);
GENERATE HR as val, ct AS ct,
-- ct/( (long)HR_stats.n_total ) AS freq,
ct/( (long)HR_stats_n_total) AS freq,
ct;
};
-- the much-much-smaller histogram is used to find the total after the fact
--
ASSERT (GROUP HR_hist ALL)
IsEqualish( SUM(freq), 1.0 ),
(HR_stats_n_total == SUM(ct);
------

As we said, this is a cheat-to-win scenario: using it to knock three minutes off an eight minute job is canny when used to make better use of a human data scientist's time, foolish when applied as a production performance optimization.

=== Calculating a Histogram Within a Group

As long as the groups in question do not rival the available memory, counting how often each value occurs within a group is easily done using the DataFu `CountEach` UDF. There's been a trend over baseball's history for increased specialization

http://datafu.incubator.apache.org/docs/datafu/guide/bag-operations.html

You'll see the

------
DEFINE CountVals              datafu.pig.bags.CountEach('flatten');
binned = FOREACH sig_seasons GENERATE
  ( 5 * ROUND(year_id/ 5.0f)) AS year_bin,
  (20 * ROUND(H      /20.0f)) AS H_bin;

hist_by_year_bags = FOREACH (GROUP binned BY year_bin) {
H_hist_cts = CountVals(binned.H_bin);
GENERATE group AS year_bin, H_hist_cts AS H_hist_cts;
};
------

We want to normalize this to be a relative-fraction histogram, so that we can
make comparisons across eras even as the number of active players grows.
Finding the total count to divide by is a straightforward COUNT_STAR on the
group, but a peccadillo of Pig's syntax makes using it a bit frustrating.
Annoyingly, a nested FOREACH can only "see" values from the bag it's
operating on, so there's no natural way to reference the calculated total
from the FOREACH statement.

------
-- Won't work:
hist_by_year_bags = FOREACH (GROUP binned BY year_bin) {
H_hist_cts = CountVals(binned.H_bin);
tot        = 1.0f*COUNT_STAR(binned);
H_hist_rel = FOREACH H_hist_cts GENERATE H_bin, (float)count/tot;
GENERATE group AS year_bin, H_hist_cts AS H_hist_cts, tot AS tot;
};
------

The best current workaround is to generate the whole-group total in the form
of a bag having just that one value. Then we use the CROSS operator to graft
it onto each (bin,count) tuple, giving us a bag with (bin,count,total) tuples
-- yes, every tuple in the bag will have the same group-wide value. Finally,
This lets us iterate across those tuples to find the relative frequency.

It's more verbose than we'd like, but the performance hit is limited to the
CPU and GC overhead of creating three bags (`{(result,count)}`,
`{(result,count,total)}`, `{(result,count,freq)}`) in quick order.

------
hist_by_year_bags = FOREACH (GROUP binned BY year_bin) {
  H_hist_cts = CountVals(binned.H_bin);
  tot        = COUNT_STAR(binned);
  GENERATE
    group      AS year_bin,
    H_hist_cts AS H_hist,
    {(tot)}    AS info:bag{(tot:long)}; -- single-tuple bag we can feed to CROSS
};
hist_by_year = FOREACH hist_by_year_bags {
  -- Combines H_hist bag {(100,93),(120,198)...} and dummy tot bag {(882.0)}
  -- to make new (bin,count,total) bag: {(100,93,882.0),(120,198,882.0)...}
  H_hist_with_tot = CROSS   H_hist, info;
  -- Then turn the (bin,count,total) bag into the (bin,count,freq) bag we want
  H_hist_rel      = FOREACH H_hist_with_tot
    GENERATE H_bin, count AS ct, count/((float)tot) AS freq;
  GENERATE year_bin, H_hist_rel;
};
------

=== The Summing Trick

There's a pattern-of-patterns we like to call the "Summing trick", a frequently useful way to act on
subsets of a group without having to perform multiple GROUP BY or FILTER operations. Call it to mind
every time you find yourself thinking "gosh, this sure seems like a lot of reduce steps on the same
key". Before we describe its generic nature, it will help to see an example

=== Counting Conditional Subsets of a Group -- The Summing Trick

Whenever you are exploring a dataset, you should determine figures of merit
for each of the key statistics -- easy-to-remember values that separate
qualitatively distinct behaviors. You probably have a feel for the way that
30 C / 85 deg F reasonably divides a "warm" day from a "hot" one; and if I
tell you that a sub-three-hour marathon distinguishes "really impress your
friends" from "really impress other runners", you are equipped to recognize
how ludicrously fast a 2:15 (the pace of a world-class runner) marathon is.

For our purposes, we can adopt 180 hits (H), 30 home runs (HR), 100 runs
batted in (RBI), a 0.400 on-base percentage (OBP) and a 0.500 slugging
percentage (SLG) each as the dividing line between a good and a great
performance.

One reasonable way to define a great career is to ask how many great seasons
a player had. We can answer that by counting how often a player's season
totals exceeded each figure of merit. The obvious tactic would seem to
involve filtering and counting each bag of seasonal stats for a player's
career; that is cumbersome to write, brings most of the data down to the
reducer, and exerts GC pressure materializing multiple bags.

------
-- Create indicator fields on each figure of merit for the season
standards = FOREACH mod_seasons {
  OBP    = 1.0*(H + BB + HBP) / PA;
  SLG    = 1.0*(h1B + 2*h2B + 3*h3B + 4*HR) / AB;
  GENERATE
    player_id,
    (H   >=   180 ? 1 : 0) AS hi_H,
    (HR  >=    30 ? 1 : 0) AS hi_HR,
    (RBI >=   100 ? 1 : 0) AS hi_RBI,
    (OBP >= 0.400 ? 1 : 0) AS hi_OBP,
    (SLG >= 0.500 ? 1 : 0) AS hi_SLG
    ;
};
-- Count the seasons that pass the threshold by summing the indicator value
career_standards = FOREACH (GROUP standards BY player_id) GENERATE
    group AS player_id,
    COUNT_STAR(standards) AS n_seasons,
    SUM(standards.hi_H)   AS hi_H,
    SUM(standards.hi_HR)  AS hi_HR,
    SUM(standards.hi_RBI) AS hi_RBI,
    SUM(standards.hi_OBP) AS hi_OBP,
    SUM(standards.hi_SLG) AS hi_SLG
    ;
------

The summing trick involves projecting a new field whose value is based on
whether it's in the desired set, forming the desired groups, and aggregating
on those new fields. Irrelevant records are assigned a value that will be
ignored by the aggregate function (typically zero or NULL), and so although
we operate on the group as a whole, only the relevant records contribute.

In this case, instead of sending all the hit, home run, etc figures directly
to the reducer to be bagged and filtered, we send a `1` for seasons above the
threshold and `0` otherwise. After the group, we find the _count_ of values
meeting our condition by simply _summing_ the values in the indicator
field. This approach allows Pig to use combiners (and so less data to the
reducer); and more importantly it doesn't cause a bag of values to be
collected, only a running sum (and so way less garbage-collector pressure).

Another example will help you see what we mean -- next, we'll use one GROUP
operation to summarize multiple subsets of a table at the same time.

First, though, a side note on these figures of merit. As it stands, this isn't a terribly
sophisticated analysis: the numbers were chosen to be easy-to-remember, and not based on the
data. For actual conclusion-drawing, we should use the z-score (REF) or quantile (REF) figures
(we'll describe both later on, and use them for our performance analysis instead). And yet, for the
exploratory phase we prefer the ad-hoc figures. A 0.400 OBP is a number you can hold in your hand
and your head; you can go click around
http://espn.go.com/mlb/stats/batting/_/sort/onBasePct/order/true[ESPN] and see that it selects about
the top 10-15 players in most seasons; you can use paper-and-pencil to feed it to the run expectancy
table (REF) we'll develop later and see what it says a 0.400-on-base hitter would produce. We've
shown you how useful it is to identify exemplar records; learn to identify these touchstone values
as well.

=== Summarizing Multiple Subsets of a Group Simultaneously

We can use the summing trick to apply even more sophisticated aggregations to
conditional subsets. How did each player's career evolve -- a brief brilliant
flame? a rise to greatness? sustained quality? Let's classify a player's
seasons by whether they are "young" (age 21 and below), "prime" (22-29
inclusive) or "older" (30 and older). We can then tell the story of their
career by finding their OPS (our overall performance metric) both overall and
for the subsets of seasons in each age range footnote:[these breakpoints are
based on where www.fangraphs.com/blogs/how-do-star-hitters-age research by
fangraphs.com showed a performance drop-off by 10% from peak.].

The complication here over the previous exercise is that we are forming
compound aggregates on the group. To apply the formula `career SLG = (career
TB) / (career AB)`, we need to separately determine the career values for
`TB` and `AB` and then form the combined `SLG` statistic.

Project the numerator and denominator of each offensive stat into the field
for that age bucket. Only one of the subset fields will be filled in; as an
example, an age-25 season will have values for PA_all and PA_prime and zeros
for PA_young and PA_older.

------
age_seasons = FOREACH mod_seasons {
  young = (age <= 21               ? true : false);
  prime = (age >= 22 AND age <= 29 ? true : false);
  older = (age >= 30               ? true : false);
  OB = H + BB + HBP;
  TB = h1B + 2*h2B + 3*h3B + 4*HR;
  GENERATE
    player_id, year_id,
    PA AS PA_all, AB AS AB_all, OB AS OB_all, TB AS TB_all,
    (young ? 1 : 0) AS is_young,
      (young ? PA : 0) AS PA_young, (young ? AB : 0) AS AB_young,
      (young ? OB : 0) AS OB_young, (young ? TB : 0) AS TB_young,
    (prime ? 1 : 0) AS is_prime,
      (prime ? PA : 0) AS PA_prime, (prime ? AB : 0) AS AB_prime,
      (prime ? OB : 0) AS OB_prime, (prime ? TB : 0) AS TB_prime,
    (older ? 1 : 0) AS is_older,
      (older ? PA : 0) AS PA_older, (older ? AB : 0) AS AB_older,
      (older ? OB : 0) AS OB_older, (older ? TB : 0) AS TB_older
    ;
};
------

After the group, we can sum across all the records to find the
plate-appearances-in-prime-seasons even though only some of the records
belong to the prime-seasons subset. The irrelevant seasons show a zero value
in the projected field and so don't contribute to the total.

------
career_epochs = FOREACH (GROUP age_seasons BY player_id) {
  PA_all    = SUM(age_seasons.PA_all  );
  PA_young  = SUM(age_seasons.PA_young);
  PA_prime  = SUM(age_seasons.PA_prime);
  PA_older  = SUM(age_seasons.PA_older);
  -- OBP = (H + BB + HBP) / PA
  OBP_all   = 1.0f*SUM(age_seasons.OB_all)   / PA_all  ;
  OBP_young = 1.0f*SUM(age_seasons.OB_young) / PA_young;
  OBP_prime = 1.0f*SUM(age_seasons.OB_prime) / PA_prime;
  OBP_older = 1.0f*SUM(age_seasons.OB_older) / PA_older;
  -- SLG = TB / AB
  SLG_all   = 1.0f*SUM(age_seasons.TB_all)   / SUM(age_seasons.AB_all);
  SLG_prime = 1.0f*SUM(age_seasons.TB_prime) / SUM(age_seasons.AB_prime);
  SLG_older = 1.0f*SUM(age_seasons.TB_older) / SUM(age_seasons.AB_older);
  SLG_young = 1.0f*SUM(age_seasons.TB_young) / SUM(age_seasons.AB_young);
  --
  GENERATE
    group AS player_id,
    MIN(age_seasons.year_id)  AS beg_year,
    MAX(age_seasons.year_id)  AS end_year,
    --
    OBP_all   + SLG_all       AS OPS_all:float,
    (PA_young >= 700 ? OBP_young + SLG_young : Null) AS OPS_young:float,
    (PA_prime >= 700 ? OBP_prime + SLG_prime : Null) AS OPS_prime:float,
    (PA_older >= 700 ? OBP_older + SLG_older : Null) AS OPS_older:float,
    --
    COUNT_STAR(age_seasons)   AS n_seasons,
    SUM(age_seasons.is_young) AS n_young,
    SUM(age_seasons.is_prime) AS n_prime,
    SUM(age_seasons.is_older) AS n_older
    ;
};
------

If you do a sort on the different OPS fields, you'll spot Ted Williams
(player ID willite01) as one of the top three young players, top three prime
players, and top three old players. He's pretty awesome.


=== Testing for Absence of a Value Within a Group


We don't need a trick to answer "which players have ever played for the Red
Sox" -- just select seasons with team id `BOS` and eliminate duplicate player
ids:

------
-- Players who were on the Red Sox at some time
onetime_sox_ids = FOREACH (FILTER bat_seasons BY (team_id == 'BOS')) GENERATE player_id;
onetime_sox     = DISTINCT onetime_sox_ids;
------

The summing trick is useful for the complement, "which players have _never_
played for the Red Sox?" You might think to repeat the above but filter for
`team_id != 'BOS'` instead, but what that gives you is "which players have
ever played for a non-Red Sox team?". The right approach is to generate a
field with the value `1` for a Red Sox season and the irrelevant value `0`
otherwise. The never-Sox are those with zeroes for every year.

------
player_soxness   = FOREACH bat_seasons GENERATE
  player_id, (team_id == 'BOS' ? 1 : 0) AS is_soxy;

player_soxness_g = FILTER (GROUP player_soxness BY player_id)
  BY MAX(is_soxy) == 0;

never_sox = FOREACH player_soxness_g GENERATE group AS player_id;
------
