== Analytic Patterns: Ordering Operations

=== Sorting All Records in Total Order

We're only going to look at players able to make solid contributions over
several years, which we'll define as playing for five or more seasons and
2000 or more plate appearances (enough to show statistical significance), and
a OPS of 0.650 (an acceptable-but-not-allstar level) or better.

.Career Epochs
------
career_epochs = FILTER career_epochs BY
  ((PA_all >= 2000) AND (n_seasons >= 5) AND (OPS_all >= 0.650));

career_young = ORDER career_epochs BY OPS_young DESC;
career_prime = ORDER career_epochs BY OPS_prime DESC;
career_older = ORDER career_epochs BY OPS_older DESC;
------

You'll spot Ted Williams (willite01) as one of the top three young players,
top three prime players, and top three old players. Ted Williams was pretty
awesome.

// To put all records in a table in order, it's not sufficient to use the sorting that each reducer applies to its input. If you sorted names from a phonebook, file `part-00000` will have names that start with A, then B, up to Z; `part-00001` will also have names from A-Z; and so on. The collection has a _partial_ order, but we want the 'total order' that Pig's `ORDER BY` operation provides. In a total sort, each record in `part-00000` is in order and precedes every records in `part-00001`; records in `part-00001` are in order and precede every record in `part-00002`; and so forth. From our earlier example to prepare topline batting statistics for players, let's sort the players in descending order by the "OPS" stat (slugging average plus offensive percent, the simplest reasonable estimator of a player's offensive contribution).
//
// ------
// player_seasons = LOAD `player_seasons` AS (...);
// qual_player_seasons = FILTER player_years BY plapp > what it should be;
// player_season_stats = FOREACH qual_player_seasons GENERATE
//    player_id, name, games,
//    hits/ab AS batting_avg,
//    whatever AS slugging_avg,
//    whatever AS offensive_pct
//    ;
// player_season_stats_ordered = ORDER player_season_stats BY (slugging_avg + offensive_pct) DESC;
// STORE player_season_stats INTO '/tmp/baseball/player_season_stats';
// ------
//
// This script will run _two_ Hadoop jobs. One pass is a light mapper-only job to sample the sort key, necessary for Pig to balance the amount of data each reducer receives (we'll learn more about this in the next chapter (TODO ref). The next pass is the map/reduce job that actually sorts the data: output file `part-r-00000` has the earliest-ordered records, followed by `part-r-00001`, and so forth.
//
// NOTE: The custom partitioner of an `ORDER` statement subtly breaks the reducer contract: it may send records having the same key to different reducers. This will cause them to be in different output (`part-xxxxx`) files, so make sure anything using the sorted data doesn't assume keys uniquely correspond to files.


==== Sorting by Multiple Fields

Sorting on Multiple fields is as easy as adding them in order with commas.
Sort by number of older seasons, breaking ties by number of prime seasons:

------
career_older = ORDER career_epochs
  BY n_older DESC, n_prime DESC;
------

Whereever reasonable, "stabilize" your sorts by adding enough columns to make
the ordering unique. This ensures the output will remain the same from run to
run, a best practice for testing and maintainability.

------
career_older = ORDER career_epochs
  BY n_older DESC, n_prime DESC, player_id ASC; -- makes sure that ties are always broken the same way.
------

==== Sorting on an Expression (You Can't)


Which players have aged the best -- made the biggest leap in performance from
their prime years to their older years? You might thing the following would
work, but you cannot use an expression in an `ORDER..BY` statement:

------
by_diff_older = ORDER career_epochs BY (OPS_older-OPS_prime) DESC; -- fails!
------

Instead, generate a new field, sort on it, then project it away. Though it's
cumbersome to type, there's no significant performance impact.

------
by_diff_older = FOREACH career_epochs
  GENERATE OPS_older - OPS_prime AS diff, player_id..;
by_diff_older = FOREACH (ORDER by_diff_older BY diff DESC, player_id)
  GENERATE player_id..;
------

If you browse through that table, you'll get a sense that current-era players
seem to be over-represented. This is just a simple whiff of a question, but
http://j.mp/bd4c-baseball_age_vs_performance[more nuanced analyses] do show
an increase in longevity of peak performance.  Part of that is due to better
training, nutrition, and medical care -- and part of that is likely due to
systemic abuse of performance-enhancing drugs.

==== Sorting Case-insensitive Strings

There's no intrinsic way to sort case-insensitive; instead, just force a
lower-case field to sort with:

------
dict        = LOAD '/usr/share/dict/words' AS (word:chararray);
sortable    = FOREACH dict GENERATE LOWER(word) AS key, *;
dict_nocase = FOREACH (ORDER sortable BY key DESC, word) GENERATE word;
zzz_nocase  = LIMIT dict_nocase 200;
dict_case   = ORDER dict BY word DESC;
zzz_case    = LIMIT dict_case   200;
------

==== Dealing with Nulls When Sorting


When the sort field has nulls, Pig sorts them as least-most by default: they
will appear as the first rows for `DESC` order and as the last rows for `ASC`
order. To float Nulls to the front or back, project a dummy field having the
favoritism you want to impose, and name it first in the `ORDER..BY` clause.

.Handling Nulls When Sorting
------
nulls_sort_demo = FOREACH career_epochs
  GENERATE (OPS_older IS NULL ? 0 : 1) AS has_older_epoch, player_id..;
nulls_then_vals = FOREACH (ORDER nulls_sort_demo BY
  has_older_epoch ASC,  OPS_all DESC, player_id)
  GENERATE player_id..;
vals_then_nulls = FOREACH (ORDER nulls_sort_demo BY
  has_older_epoch DESC, OPS_all DESC, player_id)
  GENERATE player_id..;
------

==== Floating Values to the Top or Bottom of the Sort Order

Use the dummy field trick any time you want to float records to the top or
bottom of the sort order based on a criterion. This moves all players whose
careers start in 1985 or later to the top, but otherwise sorts on number of
older seasons:

.Floating Values to the Top of the Sort Order
------
post1985_vs_earlier = FOREACH career_epochs
  GENERATE (beg_year >= 1985 ? 1 : 0) AS is_1985, player_id..;
post1985_vs_earlier = FOREACH (ORDER post1985_vs_earlier BY is_1985 DESC, n_older DESC, player_id)
  GENERATE player_id..;
------

// TODO: ??Change this to use the parks table earlier, floating the modern ones to the top?

=== Sorting Records within a Group

This operation is straightforward enough and so useful we've been applying it
all this chapter, but it's time to be properly introduced and clarify a
couple points.

Sort records within a group using ORDER BY within a nested FOREACH. Here's a
snippet to list the top four players for each team-season, in decreasing
order by plate appearances.

------
players_PA = FOREACH bat_seasons GENERATE team_id, year_id, player_id, name_first, name_last, PA;
team_playerslist_by_PA = FOREACH (GROUP players_PA BY (team_id, year_id)) {
  players_o_1 = ORDER players_PA BY PA DESC, player_id;
  players_o = LIMIT players_o_1 4;
  GENERATE group.team_id, group.year_id,
    players_o.(player_id, name_first, name_last, PA) AS players_o;
};
------

Ordering a group in the nested block immediately following a structural
operation does not require extra operations, since Pig is able to simply
specify those fields as secondary sort keys. Basically, as long as it happens
first in the reduce operation it's free (though if you're nervous, look for
the line "Secondary sort: true" in the output of EXPLAIN). Messing with a bag
before the `ORDER..BY` causes Pig to instead sort it in-memory using
quicksort, but will not cause another map-reduce job. That's good news unless
some bags are so huge they challenge available RAM or CPU, which won't be
subtle.

If you depend on having a certain sorting, specify it explicitly, even when
you notice that a `GROUP..BY` or some other operation seems to leave it in
that desired order. It gives a valuable signal to anyone reading your code,
and a necessary defense against some future optimization deranging that order
footnote:[That's not too hypothetical: there are cases where you could more
efficiently group by binning the items directly in a Map rather than sorting]

Once sorted, the bag's order is preserved by projections, by most functions
that iterate over a bag, and by the nested pipeline operations FILTER,
FOREACH, and LIMIT. The return values of nested structural operations CROSS,
ORDER..BY and DISTINCT do not follow the same order as their input; neither
do structural functions such as CountEach (in-bag histogram) or the set
operations (REF) described at the end of the chapter. (Note that though their
outputs are dis-arranged these of course don't mess with the order of their
inputs: everything in Pig is immutable once created.)

------
team_playerslist_by_PA_2 = FOREACH team_playerslist_by_PA {
  -- will not have same order, even though contents will be identical
  disordered    = DISTINCT players_o;
  -- this ORDER BY does _not_ come for free, though it's not terribly costly
  alt_order     = ORDER players_o BY player_id;
  -- these are all iterative and so will share the same order of descending PA
  still_ordered = FILTER players_o BY PA > 10;
  pa_only       = players_o.PA;
  pretty        = FOREACH players_o GENERATE
    CONCAT((chararray)PA, ':', name_first, ' ', name_last);
  GENERATE team_id, year_id,
    disordered, alt_order,
    still_ordered, pa_only, BagToString(pretty, '|');
};
------

The lines 'Global sort: false // Secondary sort: true' in the explain output indicate that pig is indeed relying on the free secondary sort, rather than quicksorting the bag itself in the reducer.



==== Select Rows with the Top-K Values for a Field

On its own, `LIMIT` will return the first records it finds.  What if you want to _rank_ the records -- sort by some criteria -- so you don't just return the first ones, but the _top_ ones?

Use the `ORDER` operator before a `LIMIT` to guarantee this "top _K_" ordering.  This technique also applies a clever optimization (reservoir sampling, see TODO ref) that sharply limits the amount of data sent to the reducers.

Let's say you wanted to select the top 20 seasons by number of hits:

------
TODO: Pig code
------

In SQL, this would be:

------
SELECT H FROM bat_season WHERE PA > 60 AND year_id > 1900 ORDER BY H  DESC LIMIT 10
------

// TODO: not sure what is the second optimization here?
// TODO: remove the term "N" if it is not used elsewhere in this section.


There are two useful optimizations to make when the number of records you will keep (_K_) is much smaller than the number of records in the table (_N_). The first one, which Pig does for you, is to only retain the top K records at each Mapper; this is a great demonstration of where a Combiner is useful:  After each intermediate merge/sort on the Map side and the Reduce side, the Combiner discards all but the top K records.

NOTE: We've cheated on the theme of this chapter (pipeline-only operations) -- sharp eyes will note that `ORDER … LIMIT` will in fact trigger a reduce operation.  We still feel that top-_K_ belongs with the other data elimination pattern, though, so we've included it here.

==== Top K Within a Group

There is a situation where the heap-based top K algorithm is appropriate:  finding the top K elements for a group. Pig's 'top' function accepts a bag and returns a bag with its top K elements.

TODO: needs code example. (Old example used World Cup data; let's find one that fits the baseball dataset)


==== Numbering Records by Sorted Rank

* ORDER by multiple fields: sort on OPS to three places then use games then playerid
* note value of stabilizing list
* (how do `null`s sort?)
* ASC / DESC: fewest strikeouts per plate appearance

==== Rank records in a group using Stitch/Over


### ???

* Over / Stitch
  - Calculating Successive-Record Differences
  - Generating a Running Total (over and stitch)
  - Finding Cumulative Sums and Running Averages
  - age vs y-o-y performance change


==== Finding Records Associated with Maximum Values

For each player, find their best significant season by OPS:

------
  -- For each season by a player, select the team they played the most games for.
  -- In SQL, this is fairly clumsy (involving a self-join and then elimination of
  -- ties) In Pig, we can ORDER BY within a foreach and then pluck the first
  -- element of the bag.

SELECT bat.player_id, bat.year_id, bat.team_id, MAX(batmax.Gmax), MAX(batmax.stints), MAX(team_ids), MAX(Gs)
  FROM       batting bat
  INNER JOIN (SELECT player_id, year_id, COUNT(*) AS stints, MAX(G) AS Gmax, GROUP_CONCAT(team_id) AS team_ids, GROUP_CONCAT(G) AS Gs FROM batting bat GROUP BY player_id, year_id) batmax
  ON bat.player_id = batmax.player_id AND bat.year_id = batmax.year_id AND bat.G = batmax.Gmax
  GROUP BY player_id, year_id
  -- WHERE stints > 1
  ;

  -- About 7% of seasons have more than one stint; only about 2% of seasons have
  -- more than one stint and more than a half-season's worth of games
SELECT COUNT(*), SUM(mt1stint), SUM(mt1stint)/COUNT(*) FROM (SELECT player_id, year_id, IF(COUNT(*) > 1 AND SUM(G) > 77, 1, 0) AS mt1stint FROM batting GROUP BY player_id, year_id) bat
------

TOP(topN, sort_column_idx, bag_of_tuples)
must have an explicit field -- can't use an expression

Leaderboard By Season-and-league

GROUP BY year_id, lg_id

There is no good way to find the tuples associated with the minimum value.
EXERCISE: make a "BTM" UDF, having the same signature as the "TOP" operation,
to return the lowest-n tuples from a bag.

==== Top K Records within a table using ORDER..LIMIT

Most hr in a season
Describe pigs optimization of order..limit

* Pulling a Section from the Middle of a Result Set: rank and filter? Modify the quantile/median code?

* Hard in SQL but easy in Pig: Finding Rows Containing Per-Group Minimum or Maximum Value, Displaying One Set of Values While Sorting by Another: - can only ORDER BY an explicit field. In SQL you can omit the sort expression from the table (use expression to sort by)
* Sorting a Result Set (when can you count on reducer order?)


==== Select Rows using a Limit and Offset

A common practice is to express percentiles of your data, that is, to order it and see which records are in the top _K_ percent.  That's another way of saying that those records are better than the remaining 100-_K_ percent, or that they are in the (100-_K_)th percentile.

The first step to calculating percentiles is to determine the number of records in your dataset.  Multiplying that number by 0.01 (that is, 1/100) will show how many records are in one percent of the data.  Multiplying the total by 0.05 (5/100) will show the number of records in five percent of the data, and so on.

For example, thanks to a quick inspection and some `wc -l` action, our baseball dataset holds 41,040 records footnote:[Remember what we said earlier, about "know your data"?] footnote:[In the next chapter, we provide some details on how to inject global values into your Pig scripts, so you needn't hardcode such a value.  For now, please bear with us.].   Five percent of 41,040 is 2,052.  To fetch the top five percent of records -- that is, those records in the 95th percentile -- we would sort the records and extract the top 2,052.

----
TODO: Pig code
----

----
SELECT H FROM bat_season WHERE PA > 60 AND year_id > 1900 ORDER BY H  DESC LIMIT 2052
----

Instead of fetching all of the records in a given percentile, we sometimes just want to know which is the lowest-ranked record of that percentile.  This tells us which record is the boundary between the ranges above and below the percentile marking.  Calculating this requires an additional step, that both Pig and SQL call `OFFSET`.  To find the 95th percentile values for our topline stats -- assuming a post-1900 game, and players with more than 60 plate appearances -- then, we would run:

----
TODO: Pig code
----

----
SELECT H FROM bat_season WHERE PA > 60 AND year_id > 1900 ORDER BY H  DESC LIMIT 1 OFFSET 2052
----

If you repeat those steps for the 75th and 50th percentiles, Pig should return the following:


----
-- %ile	  Row	H	 BB	HBP	h2B	h3B	HR	 G	 PA	OBP	SLG	OPS
-- 95th	 2052	175	75	7	34	9	25	155	669	0.394	0.519	0.895
-- 75th	10260	124	41	3	21	4	9	132	520	0.347	0.422	0.765
-- 50th	20521	66	22	1	11	1	3	93	294	0.313	0.359	0.676
----

WARNING: Be really careful doing this.  As opposed to the `ORDER BY .. LIMIT` pattern, Pig must do a total sort on the full table to calculate percentiles this way.



====  Shuffle a set of records

See notes on random numbers.

You might also enjoy the random number table, holding 350 million 64-bit numbers directly from random.org (7 GB of 20-digit decimal numbers)
* 160-bit numbers in hexadecimal form
* 32 64-bit numbers (2048-bits per row)

cogroup events by team_id
... there's a way to do this in one less reduce in M/R -- can you in Pig?
